{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callhome es-en word level configuration\n",
      "translating es to en\n",
      "vocab size, en=51, fr=51\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%aimport basics\n",
    "%aimport nn_config\n",
    "%aimport enc_dec\n",
    "\n",
    "\n",
    "from basics import *\n",
    "from nn_config import *\n",
    "from enc_dec import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp = cuda.cupy if gpuid >= 0 else np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = pickle.load(open(text_data_dict, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SpeechEncoderDecoder(SPEECH_DIM, vocab_size_en, num_layers_enc, num_layers_dec,\n",
    "                               hidden_units, gpuid, attn=use_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('es_speech_to_en_char_model/train_17394sen_5-2layers_128units_es_en_speech2text_callhome_es_en_1.log',\n",
       " {'en': '../../corpora/callhome/uttr_fa_vad_wavs/train.en',\n",
       "  'fr': '../../corpora/callhome/uttr_fa_vad_wavs/speech_train.es'},\n",
       " {'en': '../../corpora/callhome/uttr_fa_vad_wavs/dev.en',\n",
       "  'fr': '../../corpora/callhome/uttr_fa_vad_wavs/speech_dev.es'},\n",
       " {'en': '../../corpora/callhome/uttr_fa_vad_wavs/test.en',\n",
       "  'fr': '../../corpora/callhome/uttr_fa_vad_wavs/speech_test.es'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_train_fil_name, text_fname, dev_fname, test_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_sentence(line_num, line_fr, line_en=None, display=True, plot_name=None, p_filt=0, r_filt=0):\n",
    "    if not CHAR_LEVEL:\n",
    "        fr_sent = line_fr.strip().split()\n",
    "    else:\n",
    "        fr_sent = [c.encode() for c in list(line_fr.strip().decode())]\n",
    "    fr_ids = [w2i[\"fr\"].get(w, UNK_ID) for w in fr_sent]\n",
    "\n",
    "    # english reference is optional. If provided, compute precision/recall\n",
    "    if line_en:\n",
    "        en_sent = line_en.strip().split()\n",
    "        en_ids = [w2i[\"en\"].get(w, UNK_ID) for w in en_sent]\n",
    "\n",
    "    pred_ids, alpha_arr = model.encode_decode_predict(fr_ids)\n",
    "    pred_words = [i2w[\"en\"][w].decode() if w != EOS_ID else \" _EOS\" for w in pred_ids]\n",
    "    # print(pred_ids)\n",
    "    # print(pred_words)\n",
    "\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    filter_match = False\n",
    "\n",
    "    matches = count_match(en_ids, pred_ids)\n",
    "    if EOS_ID in pred_ids:\n",
    "        pred_len = len(pred_ids)-1\n",
    "    else:\n",
    "        pred_len = len(pred_ids)\n",
    "    # subtract 1 from length for EOS id\n",
    "    prec = (matches/pred_len) if pred_len > 0 else 0\n",
    "    rec = matches/len(en_ids)\n",
    "\n",
    "    if display and (prec >= p_filt and rec >= r_filt):\n",
    "        filter_match = True\n",
    "        # convert raw binary into string\n",
    "        # fr_words = [w.decode() for w in fr_sent]\n",
    "\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "        print(\"sentence: {0:d}\".format(line_num))\n",
    "        print(\"{0:s} | {1:80s}\".format(\"Src\", line_fr.strip().decode()))\n",
    "        print(\"{0:s} | {1:80s}\".format(\"Ref\", line_en.strip().decode()))\n",
    "        \n",
    "        if not CHAR_LEVEL:\n",
    "            print(\"{0:s} | {1:80s}\".format(\"Hyp\", \" \".join(pred_words)))\n",
    "        else:\n",
    "            print(\"{0:s} | {1:80s}\".format(\"Hyp\", \"\".join(pred_words)))\n",
    "\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "        print(\"{0:s} | {1:0.4f}\".format(\"precision\", prec))\n",
    "        print(\"{0:s} | {1:0.4f}\".format(\"recall\", rec))\n",
    "\n",
    "        # if plot_name and use_attn:\n",
    "        #     plot_attention(alpha_arr, fr_words, pred_words, plot_name)\n",
    "\n",
    "    return matches, len(pred_ids), len(en_ids), filter_match\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def predict(s=NUM_TRAINING_SENTENCES, num=NUM_DEV_SENTENCES, display=True, plot=False, p_filt=0, r_filt=0, fil_name=text_fname):\n",
    "    print(\"English predictions, s={0:d}, num={1:d}:\".format(s, num))\n",
    "\n",
    "    metrics = {\"cp\":[], \"tp\":[], \"t\":[]}\n",
    "\n",
    "    filter_count = 0\n",
    "\n",
    "    with open(fil_name[\"fr\"], \"rb\") as fr_file, open(fil_name[\"en\"], \"rb\") as en_file:\n",
    "        for i, (line_fr, line_en) in enumerate(zip(fr_file, en_file), start=0):\n",
    "            if i >= s and i < (s+num):\n",
    "                if plot:\n",
    "                    plot_name = os.path.join(model_dir, \"sample_{0:d}_plot.png\".format(i+1))\n",
    "                else:\n",
    "                    plot_name=None\n",
    "\n",
    "                # make prediction\n",
    "                cp, tp, t, f = predict_sentence(i, line_fr,\n",
    "                                             line_en,\n",
    "                                             display=display,\n",
    "                                             plot_name=plot_name,\n",
    "                                             p_filt=p_filt, r_filt=r_filt)\n",
    "                metrics[\"cp\"].append(cp)\n",
    "                metrics[\"tp\"].append(tp)\n",
    "                metrics[\"t\"].append(t)\n",
    "                filter_count += (1 if f else 0)\n",
    "\n",
    "    print(\"sentences matching filter = {0:d}\".format(filter_count))\n",
    "    return metrics\n",
    "\n",
    "def count_match(list1, list2):\n",
    "    # each list can have repeated elements. The count should account for this.\n",
    "    count1 = Counter(list1)\n",
    "    count2 = Counter(list2)\n",
    "    count2_keys = count2.keys()-set([UNK_ID, EOS_ID])\n",
    "    common_w = set(count1.keys()) & set(count2_keys)\n",
    "    #all_w = set(count1.keys()) + set(count2.keys())\n",
    "    matches = sum([min(count1[w], count2[w]) for w in common_w])\n",
    "    #matches = sum([max(0, count2[v]-count1[v]) for v in (count2-count1).values()])\n",
    "    #matches = sum([max(0, count2[v]-count1[v]) for v in common_w])\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_pplx(src_fname, tar_fname, num_sent):\n",
    "    loss = 0\n",
    "    num_words = 0\n",
    "    # with open(test_fname[\"fr\"], \"rb\") as fr_file, open(test_fname[\"en\"], \"rb\") as en_file:\n",
    "    with open(src_fname, \"rb\") as fr_file, open(tar_fname, \"rb\") as en_file:\n",
    "        with tqdm(total=num_sent) as pbar:\n",
    "            sys.stderr.flush()\n",
    "            out_str = \"loss={0:.6f}\".format(0)\n",
    "            pbar.set_description(out_str)\n",
    "            for i, (line_fr, line_en) in enumerate(zip(fr_file, en_file), start=1):\n",
    "\n",
    "                if i > num_sent:\n",
    "                    break\n",
    "\n",
    "                if not CHAR_LEVEL:\n",
    "                    fr_sent = line_fr.strip().split()\n",
    "                    en_sent = line_en.strip().split()\n",
    "                else:\n",
    "                    fr_sent = [c.encode() for c in list(line_fr.strip().decode())]\n",
    "                    en_sent = [c.encode() for c in list(line_en.strip().decode())]\n",
    "\n",
    "                fr_ids = [w2i[\"fr\"].get(w, UNK_ID) for w in fr_sent]\n",
    "                en_ids = [w2i[\"en\"].get(w, UNK_ID) for w in en_sent]\n",
    "\n",
    "                if len(fr_ids) > 0 and len(en_ids) > 0:\n",
    "                    # compute loss\n",
    "                    curr_loss = float(model.encode_decode_train(fr_ids, en_ids, train=False).data)\n",
    "                    loss += curr_loss\n",
    "                    num_words += len(en_ids)\n",
    "\n",
    "                    out_str = \"loss={0:.6f}\".format(curr_loss)\n",
    "                    pbar.set_description(out_str)\n",
    "                pbar.update(1)\n",
    "\n",
    "            # end of for\n",
    "        # end of pbar\n",
    "    # end of with open file\n",
    "    loss_per_word = loss / num_words\n",
    "    pplx = 2 ** loss_per_word\n",
    "    random_pplx = vocab_size_en\n",
    "\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "    print(\"{0:s} | {1:0.6f}\".format(\"dev perplexity\", pplx))\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "    return pplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ids(align_list, char_level=CHAR_LEVEL):\n",
    "    words = [a.word for a in align_list]\n",
    "    text_line = \" \".join(words)\n",
    "    \n",
    "    if not char_level:\n",
    "        symbols = [w.encode() for w in text_line.strip()]\n",
    "    else:\n",
    "        symbols = [c.encode() for c in list(text_line.strip())]\n",
    "    \n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'T O   S E E   H O W   T H E   D O C U M E N T S   A R E   A N D   E L S E   W H A T   H A P P E N S   H A P P E N S   I S   A S   I   H A V E   C H A N G E D   H O U S E   D O N   D O N'\n"
     ]
    }
   ],
   "source": [
    "print(b\" \".join(get_ids(text_data[\"train\"][\"041.004\"][\"en\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_loop(num_training, num_epochs, log_mode=\"a\"):\n",
    "    # Set up log file for loss\n",
    "    log_dev_fil = open(log_dev_fil_name, mode=log_mode)\n",
    "    log_dev_csv = csv.writer(log_dev_fil, lineterminator=\"\\n\")\n",
    "\n",
    "    # initialize perplexity on dev set\n",
    "    # save model when new epoch value is lower than previous\n",
    "    pplx = float(\"inf\")\n",
    "\n",
    "    sys.stderr.flush()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, sp_fil in enumerate(sorted(list(text_data[\"train\"].keys()))[:num_training]):\n",
    "            with tqdm(total=num_training) as pbar:\n",
    "                sys.stderr.flush()\n",
    "                print(sp_fil)\n",
    "                loss_per_epoch = 0\n",
    "                out_str = \"epoch={0:d}, loss={1:.6f}, mean loss={2:.6f}\".format(epoch+1, 0, 0)\n",
    "                pbar.set_description(out_str)\n",
    "                \n",
    "                # get the word/character ids\n",
    "                fr_sent = get_ids(text_data[\"train\"][sp_fil][\"es\"])\n",
    "                en_sent = get_ids(text_data[\"train\"][sp_fil][\"en\"])\n",
    "\n",
    "                fr_ids = [w2i[\"fr\"].get(w, UNK_ID) for w in fr_sent]\n",
    "                en_ids = [w2i[\"en\"].get(w, UNK_ID) for w in en_sent]\n",
    "                \n",
    "                speech_feat = xp.load(os.path.join(speech_dir, sp_fil+speech_extn))\n",
    "                print(speech_feat.shape)\n",
    "\n",
    "                it = (epoch * num_training) + i\n",
    "\n",
    "                # compute loss\n",
    "                loss = model.encode_decode_train(speech_feat, en_ids)\n",
    "\n",
    "                # set up for backprop\n",
    "                model.cleargrads()\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                optimizer.update()\n",
    "                # store loss value for display\n",
    "                loss_val = float(loss.data)\n",
    "                loss_per_epoch += loss_val\n",
    "\n",
    "                out_str = \"epoch={0:d}, loss={1:.6f}, mean loss={2:.6f}\".format(\n",
    "                           epoch+1, it, loss_val, (loss_per_epoch / i))\n",
    "                pbar.set_description(out_str)\n",
    "                pbar.update(1)\n",
    "            # end with pbar\n",
    "        # end for num_training\n",
    "\n",
    "        print(\"finished training on {0:d} sentences\".format(num_training))\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "        print(\"computing perplexity\")\n",
    "        pplx_new = compute_pplx(dev_fname[\"fr\"], dev_fname[\"en\"], NUM_MINI_DEV_SENTENCES)\n",
    "\n",
    "        if pplx_new > pplx:\n",
    "            print(\"perplexity went up during training, breaking out of loop\")\n",
    "            break\n",
    "        \n",
    "        pplx = pplx_new\n",
    "        print(log_dev_fil_name)\n",
    "        print(model_fil.replace(\".model\", \"_{0:d}.model\".format(epoch+1)))\n",
    "\n",
    "        if (epoch+1) % ITERS_TO_SAVE == 0:\n",
    "            bleu_score = compute_bleu(dev_fname[\"fr\"], dev_fname[\"en\"], NUM_MINI_DEV_SENTENCES)\n",
    "            print(\"Saving model\")\n",
    "            serializers.save_npz(model_fil.replace(\".model\", \"_{0:d}.model\".format(last_epoch_id+epoch+1)), \n",
    "                                 model)\n",
    "            print(\"Finished saving model\")\n",
    "\n",
    "        # log pplx and bleu score\n",
    "        log_dev_csv.writerow([(last_epoch_id+epoch+1), pplx_new, bleu_score])\n",
    "        log_dev_fil.flush()\n",
    "    \n",
    "    print(\"Simple predictions (╯°□°）╯︵ ┻━┻\")\n",
    "    print(\"training set predictions\")\n",
    "    _ = predict(s=0, num=2, plot=False)\n",
    "    print(\"Simple predictions (╯°□°）╯︵ ┻━┻\")\n",
    "    print(\"dev set predictions\")\n",
    "    _ = predict(s=NUM_TRAINING_SENTENCES, num=3, plot=False)\n",
    "    # print(\"{0:s}\".format(\"-\"*50))\n",
    "    # compute_bleu(dev_fname[\"fr\"], dev_fname[\"en\"], NUM_MINI_DEV_SENTENCES)\n",
    "    # print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "    print(\"Final saving model\")\n",
    "    serializers.save_npz(model_fil, model)\n",
    "    print(\"Finished saving model\")\n",
    "\n",
    "    # close log file\n",
    "    log_train_fil.close()\n",
    "    log_dev_fil.close()\n",
    "    print(log_train_fil_name)\n",
    "    print(log_dev_fil_name)\n",
    "    print(model_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward_states = model[model.lstm_enc[-1]].h\n",
    "# backward_states = model[model.lstm_rev_enc[-1]].h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.enc_states = F.concat((forward_states, backward_states), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "041.001\n",
      "(1168, 120)\n",
      "speech (1168, 1, 120)\n",
      "L0_enc before (1168, 1, 120)\n",
      "L0_enc out (584, 1, 256)\n",
      "L1_enc before (584, 1, 256)\n",
      "L1_enc out (292, 1, 256)\n",
      "L2_enc before (292, 1, 256)\n",
      "L2_enc out (146, 1, 256)\n",
      "L3_enc before (146, 1, 256)\n",
      "L3_enc out (73, 1, 256)\n",
      "L4_enc before (73, 1, 256)\n",
      "L4_enc out (73, 1, 128)\n",
      "speech (1168, 1, 120)\n",
      "L0_rev_enc before (1168, 1, 120)\n",
      "L0_rev_enc out (584, 1, 256)\n",
      "L1_rev_enc before (584, 1, 256)\n",
      "L1_rev_enc out (292, 1, 256)\n",
      "L2_rev_enc before (292, 1, 256)\n",
      "L2_rev_enc out (146, 1, 256)\n",
      "L3_rev_enc before (146, 1, 256)\n",
      "L3_rev_enc out (73, 1, 256)\n",
      "L4_rev_enc before (73, 1, 256)\n",
      "L4_rev_enc out (73, 1, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-44d0a5ac9590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-c84a3ac6d84a>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(num_training, num_epochs, log_mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# store loss value for display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "train_loop(num_training=1, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
