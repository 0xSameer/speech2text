{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%aimport basics\n",
    "%aimport nn_config\n",
    "%aimport enc_dec\n",
    "\n",
    "\n",
    "from basics import *\n",
    "from nn_config import *\n",
    "from enc_dec import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp = cuda.cupy if gpuid >= 0 else np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = pickle.load(open(text_data_dict, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SpeechEncoderDecoder(SPEECH_DIM, vocab_size_en, num_layers_enc, num_layers_dec,\n",
    "                               hidden_units, gpuid, attn=use_attn)\n",
    "if gpuid >= 0:\n",
    "    cuda.get_device(gpuid).use()\n",
    "    model.to_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam()\n",
    "# optimizer = optimizers.Adam(alpha=0.0005, beta1=0.9, beta2=0.999, eps=1e-08)\n",
    "# optimizer = optimizers.SGD(lr=0.0005)\n",
    "optimizer.setup(model)\n",
    "# gradient clipping\n",
    "optimizer.add_hook(chainer.optimizer.GradientClipping(threshold=5))\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_train_fil_name, text_fname, dev_fname, test_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_prediction(fr_line, en_line, pred_words, prec=0, rec=0):\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "    print(\"{0:s} | {1:80s}\".format(\"Src\", fr_line.strip()))\n",
    "    print(\"{0:s} | {1:80s}\".format(\"Ref\", en_line.strip()))\n",
    "\n",
    "    if not CHAR_LEVEL:\n",
    "        print(\"{0:s} | {1:80s}\".format(\"Hyp\", \" \".join(pred_words)))\n",
    "    else:\n",
    "        print(\"{0:s} | {1:80s}\".format(\"Hyp\", \"\".join(pred_words)))\n",
    "\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "    print(\"{0:s} | {1:0.4f}\".format(\"precision\", prec))\n",
    "    print(\"{0:s} | {1:0.4f}\".format(\"recall\", rec))\n",
    "\n",
    "    # if plot_name and use_attn:\n",
    "    #     plot_attention(alpha_arr, fr_words, pred_words, plot_name)\n",
    "    \n",
    "def predict_sentence(speech_feat, en_ids, p_filt=0, r_filt=0):\n",
    "    # get prediction\n",
    "    pred_ids, alpha_arr = model.encode_decode_predict(speech_feat)\n",
    "    \n",
    "    pred_words = [i2w[\"en\"][w].decode() if w != EOS_ID else \" _EOS\" for w in pred_ids]\n",
    "    \n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    filter_match = False\n",
    "\n",
    "    matches = count_match(en_ids, pred_ids)\n",
    "    if EOS_ID in pred_ids:\n",
    "        pred_len = len(pred_ids)-1\n",
    "    else:\n",
    "        pred_len = len(pred_ids)\n",
    "    \n",
    "    # subtract 1 from length for EOS id\n",
    "    prec = (matches/pred_len) if pred_len > 0 else 0\n",
    "    rec = matches/len(en_ids)\n",
    "\n",
    "    filter_match = (prec >= p_filt and rec >= r_filt)\n",
    "\n",
    "    return pred_words, matches, len(pred_ids), len(en_ids), filter_match\n",
    "\n",
    "\n",
    "def predict(s=0, num=1, cat=\"train\", display=True, plot=False, p_filt=0, r_filt=0):\n",
    "    print(\"English predictions, s={0:d}, num={1:d}:\".format(s, num))\n",
    "\n",
    "    metrics = {\"cp\":[], \"tp\":[], \"t\":[]}\n",
    "\n",
    "    filter_count = 0\n",
    "    \n",
    "    for i, sp_fil in enumerate(sorted(list(text_data[cat].keys()))[s:s+num]):\n",
    "        if plot:\n",
    "            plot_name = os.path.join(model_dir, \"{0:s}_plot.png\".format(sp_fil))\n",
    "        else:\n",
    "            plot_name=None\n",
    "            \n",
    "        fr_ids, en_ids, speech_feat = get_data_item(sp_fil, cat=cat)\n",
    "\n",
    "        # make prediction\n",
    "        pred_words, cp, tp, t, f = predict_sentence(speech_feat, en_ids, \n",
    "                                                    p_filt=p_filt, r_filt=r_filt)\n",
    "        metrics[\"cp\"].append(cp)\n",
    "        metrics[\"tp\"].append(tp)\n",
    "        metrics[\"t\"].append(t)\n",
    "        filter_count += (1 if f else 0)\n",
    "        \n",
    "        if display:\n",
    "            fr_line, en_line = get_text_lines(sp_fil, cat=cat)\n",
    "            print(\"-\"*80)\n",
    "            print(\"prediction for: {0:s}\".format(sp_fil))\n",
    "            display_prediction(fr_line, en_line, pred_words, prec=0, rec=0)\n",
    "\n",
    "    print(\"sentences matching filter = {0:d}\".format(filter_count))\n",
    "    return metrics\n",
    "\n",
    "def count_match(list1, list2):\n",
    "    # each list can have repeated elements. The count should account for this.\n",
    "    count1 = Counter(list1)\n",
    "    count2 = Counter(list2)\n",
    "    count2_keys = count2.keys()-set([UNK_ID, EOS_ID])\n",
    "    common_w = set(count1.keys()) & set(count2_keys)\n",
    "    matches = sum([min(count1[w], count2[w]) for w in common_w])\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_pplx(cat=\"dev\", num_sent=NUM_MINI_DEV_SENTENCES):\n",
    "    loss = 0\n",
    "    num_words = 0\n",
    "    for i, sp_fil in enumerate(sorted(list(text_data[cat].keys()))[:num_sent]):\n",
    "        with tqdm(total=num_sent) as pbar:\n",
    "            sys.stderr.flush()\n",
    "            out_str = \"loss={0:.6f}\".format(0)\n",
    "            pbar.set_description(out_str)\n",
    "            fr_ids, en_ids, speech_feat = get_data_item(sp_fil, cat=cat)\n",
    "\n",
    "            if len(fr_ids) > 0 and len(en_ids) > 0:\n",
    "                # compute loss\n",
    "                curr_loss = float(model.encode_decode_train(speech_feat, en_ids, train=False).data)\n",
    "                loss += curr_loss\n",
    "                num_words += len(en_ids)\n",
    "\n",
    "                out_str = \"loss={0:.6f}\".format(curr_loss)\n",
    "                pbar.set_description(out_str)\n",
    "            pbar.update(1)\n",
    "        # end of pbar\n",
    "    # end of for num_sent\n",
    "    \n",
    "    loss_per_word = loss / num_words\n",
    "    pplx = 2 ** loss_per_word\n",
    "    random_pplx = vocab_size_en\n",
    "\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "    print(\"{0:s} | {1:0.6f}\".format(\"dev perplexity\", pplx))\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "    return pplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_stats(hypothesis, reference):\n",
    "    yield len(hypothesis)\n",
    "    yield len(reference)\n",
    "    for n in range(1,5):\n",
    "        s_ngrams = Counter([tuple(hypothesis[i:i+n]) for i in range(len(hypothesis)+1-n)])\n",
    "        r_ngrams = Counter([tuple(reference[i:i+n]) for i in range(len(reference)+1-n)])\n",
    "        yield max([sum((s_ngrams & r_ngrams).values()), 0])\n",
    "        yield max([len(hypothesis)+1-n, 0])\n",
    "\n",
    "\n",
    "# Compute BLEU from collected statistics obtained by call(s) to bleu_stats\n",
    "def bleu(stats):\n",
    "    if len(list(filter(lambda x: x==0, stats))) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum([math.log(float(x)/y) for x,y in zip(stats[2::2],stats[3::2])]) / 4.\n",
    "    return math.exp(min([0, 1-float(r)/c]) + log_bleu_prec)\n",
    "\n",
    "\n",
    "def compute_bleu(cat=\"dev\", num_sent=NUM_MINI_DEV_SENTENCES):\n",
    "    list_of_references = []\n",
    "    list_of_hypotheses = []\n",
    "    for i, sp_fil in enumerate(sorted(list(text_data[cat].keys()))[:num_sent]):\n",
    "        with tqdm(total=num_sent) as pbar:\n",
    "            sys.stderr.flush()\n",
    "            out_str = \"predicting sentence={0:d}\".format(i)\n",
    "            pbar.update(1)\n",
    "\n",
    "            fr_ids, en_ids, speech_feat = get_data_item(sp_fil, cat=cat)\n",
    "            fr_line, en_line = get_text_lines(sp_fil, cat=cat)\n",
    "\n",
    "            # add reference translation\n",
    "            reference_words = en_line.strip().split()\n",
    "            list_of_references.append(reference_words)\n",
    "            \n",
    "            if len(fr_ids) > 0 and len(en_sent) > 0:\n",
    "                pred_sent, _ = model.encode_decode_predict(fr_ids)\n",
    "                pred_words = [i2w[\"en\"][w].decode() if w != EOS_ID else \"\" for w in pred_ids]\n",
    "                if CHAR_LEVEL:\n",
    "                    pred_words = \"\".join(pred_words)\n",
    "                    pred_words = pred_words.split()\n",
    "\n",
    "            else:\n",
    "                pred_words = []\n",
    "            list_of_hypotheses.append(pred_words)\n",
    "\n",
    "    stats = [0 for i in range(10)]\n",
    "    for (r,h) in zip(list_of_references, list_of_hypotheses):\n",
    "        stats = [sum(scores) for scores in zip(stats, bleu_stats(h,r))]\n",
    "    print(\"BLEU: %0.2f\" % (100 * bleu(stats)))\n",
    "\n",
    "    return (100 * bleu(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_lines(sp_fil, cat=\"train\"):\n",
    "    _, fr_line = get_ids(text_data[cat][sp_fil][\"es\"])\n",
    "    _, en_line = get_ids(text_data[cat][sp_fil][\"en\"])\n",
    "    \n",
    "    return fr_line, en_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ids(align_list, char_level=CHAR_LEVEL):\n",
    "    words = [a.word for a in align_list]\n",
    "    text_line = \" \".join(words)\n",
    "    \n",
    "    if not char_level:\n",
    "        symbols = [w.encode() for w in text_line.strip()]\n",
    "    else:\n",
    "        symbols = [c.encode() for c in list(text_line.strip())]\n",
    "    \n",
    "    return symbols, text_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_item(sp_fil, cat=\"train\"):    \n",
    "    fr_sent, _ = get_ids(text_data[cat][sp_fil][\"es\"])\n",
    "    en_sent, _ = get_ids(text_data[cat][sp_fil][\"en\"])\n",
    "\n",
    "    fr_ids = [w2i[\"fr\"].get(w, UNK_ID) for w in fr_sent]\n",
    "    en_ids = [w2i[\"en\"].get(w, UNK_ID) for w in en_sent]\n",
    "\n",
    "    speech_feat = xp.load(os.path.join(speech_dir, sp_fil+speech_extn))\n",
    "    return fr_ids, en_ids, speech_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict(s=0, num=1, cat=\"train\", display=True, plot=False, p_filt=0, r_filt=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(b\" \".join(get_ids(text_data[\"train\"][\"041.004\"][\"en\"])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_loop(num_training, num_epochs, log_mode=\"a\", last_epoch_id=0):\n",
    "    # Set up log file for loss\n",
    "    log_dev_fil = open(log_dev_fil_name, mode=log_mode)\n",
    "    log_dev_csv = csv.writer(log_dev_fil, lineterminator=\"\\n\")\n",
    "    bleu_score = 0\n",
    "\n",
    "    # initialize perplexity on dev set\n",
    "    # save model when new epoch value is lower than previous\n",
    "    pplx = float(\"inf\")\n",
    "\n",
    "    sys.stderr.flush()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with tqdm(total=num_training) as pbar:\n",
    "            for i, sp_fil in enumerate(sorted(list(text_data[\"train\"].keys()))[:num_training], start=1):\n",
    "                sys.stderr.flush()\n",
    "                loss_per_epoch = 0\n",
    "                out_str = \"epoch={0:d}, loss={1:.4f}, mean loss={2:.4f}\".format(epoch+1, 0, 0)\n",
    "                pbar.set_description(out_str)\n",
    "                \n",
    "                # get the word/character ids\n",
    "                fr_ids, en_ids, speech_feat = get_data_item(sp_fil, cat=\"train\")\n",
    "\n",
    "                it = (epoch * num_training) + i\n",
    "\n",
    "                # compute loss\n",
    "                loss = model.encode_decode_train(speech_feat, en_ids, train=True)\n",
    "\n",
    "                # set up for backprop\n",
    "                model.cleargrads()\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                optimizer.update()\n",
    "                # store loss value for display\n",
    "                loss_val = float(loss.data)\n",
    "                loss_per_epoch += loss_val\n",
    "\n",
    "                out_str = \"epoch={0:d}, loss={1:.4f}, mean loss={2:.4f}\".format(\n",
    "                           epoch+1, loss_val, (loss_per_epoch / i))\n",
    "                pbar.set_description(out_str)\n",
    "                pbar.update(1)\n",
    "            # end with pbar\n",
    "        # end for num_training\n",
    "\n",
    "        print(\"finished training on {0:d} sentences\".format(num_training))\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "        print(\"computing perplexity\")\n",
    "        pplx_new = compute_pplx(cat=\"dev\", num_sent=NUM_MINI_DEV_SENTENCES)\n",
    "\n",
    "        if pplx_new > pplx:\n",
    "            print(\"perplexity went up during training, breaking out of loop\")\n",
    "            break\n",
    "        \n",
    "        pplx = pplx_new\n",
    "        print(log_dev_fil_name)\n",
    "        print(model_fil.replace(\".model\", \"_{0:d}.model\".format(epoch+1)))\n",
    "\n",
    "        if (epoch+1) % ITERS_TO_SAVE == 0:\n",
    "            bleu_score = compute_bleu(cat=\"dev\", num_sent=NUM_MINI_DEV_SENTENCES)\n",
    "            print(\"Saving model\")\n",
    "            serializers.save_npz(model_fil.replace(\".model\", \"_{0:d}.model\".format(last_epoch_id+epoch+1)), \n",
    "                                 model)\n",
    "            print(\"Finished saving model\")\n",
    "\n",
    "        # log pplx and bleu score\n",
    "        log_dev_csv.writerow([(last_epoch_id+epoch+1), pplx_new, bleu_score])\n",
    "        log_dev_fil.flush()\n",
    "    \n",
    "    print(\"Simple predictions (╯°□°）╯︵ ┻━┻\")\n",
    "    print(\"training set predictions\")\n",
    "    _ = predict(s=0, num=2, cat=\"train\", display=True, plot=False, p_filt=0, r_filt=0)\n",
    "    print(\"Simple predictions (╯°□°）╯︵ ┻━┻\")\n",
    "    print(\"dev set predictions\")\n",
    "    _ = predict(s=0, num=2, cat=\"dev\", display=True, plot=False, p_filt=0, r_filt=0)\n",
    "\n",
    "    print(\"Final saving model\")\n",
    "    serializers.save_npz(model_fil, model)\n",
    "    print(\"Finished saving model\")\n",
    "\n",
    "    # close log file\n",
    "    log_dev_fil.close()\n",
    "\n",
    "    print(log_dev_fil_name)\n",
    "    print(model_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward_states = model[model.lstm_enc[-1]].h\n",
    "# backward_states = model[model.lstm_rev_enc[-1]].h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.enc_states = F.concat((forward_states, backward_states), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loop(num_training=1000, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
