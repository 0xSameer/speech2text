{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callhome es-en word level configuration\n",
      "translating es to en\n",
      "vocab size, en=51, fr=51\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%aimport basics\n",
    "%aimport nn_config\n",
    "%aimport enc_dec\n",
    "\n",
    "\n",
    "from basics import *\n",
    "from nn_config import *\n",
    "from enc_dec import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp = cuda.cupy if gpuid >= 0 else np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = pickle.load(open(text_data_dict, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SpeechEncoderDecoder(SPEECH_DIM, vocab_size_en, num_layers_enc, num_layers_dec,\n",
    "                               hidden_units, gpuid, attn=use_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam()\n",
    "# optimizer = optimizers.Adam(alpha=0.0005, beta1=0.9, beta2=0.999, eps=1e-08)\n",
    "# optimizer = optimizers.SGD(lr=0.0005)\n",
    "optimizer.setup(model)\n",
    "# gradient clipping\n",
    "optimizer.add_hook(chainer.optimizer.GradientClipping(threshold=5))\n",
    "optimizer.add_hook(chainer.optimizer.WeightDecay(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('es_speech_to_en_char_model/train_17394sen_5-2layers_128units_es_en_speech2text_callhome_es_en_1.log',\n",
       " {'en': '../../corpora/callhome/uttr_fa_vad_wavs/train.en',\n",
       "  'fr': '../../corpora/callhome/uttr_fa_vad_wavs/speech_train.es'},\n",
       " {'en': '../../corpora/callhome/uttr_fa_vad_wavs/dev.en',\n",
       "  'fr': '../../corpora/callhome/uttr_fa_vad_wavs/speech_dev.es'},\n",
       " {'en': '../../corpora/callhome/uttr_fa_vad_wavs/test.en',\n",
       "  'fr': '../../corpora/callhome/uttr_fa_vad_wavs/speech_test.es'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_train_fil_name, text_fname, dev_fname, test_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_prediction(fr_line, en_line, pred_words, prec=0, rec=0):\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "    print(\"{0:s} | {1:80s}\".format(\"Src\", fr_line.strip()))\n",
    "    print(\"{0:s} | {1:80s}\".format(\"Ref\", en_line.strip()))\n",
    "\n",
    "    if not CHAR_LEVEL:\n",
    "        print(\"{0:s} | {1:80s}\".format(\"Hyp\", \" \".join(pred_words)))\n",
    "    else:\n",
    "        print(\"{0:s} | {1:80s}\".format(\"Hyp\", \"\".join(pred_words)))\n",
    "\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "    print(\"{0:s} | {1:0.4f}\".format(\"precision\", prec))\n",
    "    print(\"{0:s} | {1:0.4f}\".format(\"recall\", rec))\n",
    "\n",
    "    # if plot_name and use_attn:\n",
    "    #     plot_attention(alpha_arr, fr_words, pred_words, plot_name)\n",
    "    \n",
    "def predict_sentence(speech_feat, en_ids, p_filt=0, r_filt=0):\n",
    "    # get prediction\n",
    "    pred_ids, alpha_arr = model.encode_decode_predict(speech_feat)\n",
    "    \n",
    "    pred_words = [i2w[\"en\"][w].decode() if w != EOS_ID else \" _EOS\" for w in pred_ids]\n",
    "    \n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    filter_match = False\n",
    "\n",
    "    matches = count_match(en_ids, pred_ids)\n",
    "    if EOS_ID in pred_ids:\n",
    "        pred_len = len(pred_ids)-1\n",
    "    else:\n",
    "        pred_len = len(pred_ids)\n",
    "    \n",
    "    # subtract 1 from length for EOS id\n",
    "    prec = (matches/pred_len) if pred_len > 0 else 0\n",
    "    rec = matches/len(en_ids)\n",
    "\n",
    "    filter_match = (prec >= p_filt and rec >= r_filt)\n",
    "\n",
    "    return pred_words, matches, len(pred_ids), len(en_ids), filter_match\n",
    "\n",
    "\n",
    "def predict(s=0, num=1, cat=\"train\", display=True, plot=False, p_filt=0, r_filt=0):\n",
    "    print(\"English predictions, s={0:d}, num={1:d}:\".format(s, num))\n",
    "\n",
    "    metrics = {\"cp\":[], \"tp\":[], \"t\":[]}\n",
    "\n",
    "    filter_count = 0\n",
    "    \n",
    "    for i, sp_fil in enumerate(sorted(list(text_data[cat].keys()))[s:s+num]):\n",
    "        if plot:\n",
    "            plot_name = os.path.join(model_dir, \"{0:s}_plot.png\".format(sp_fil))\n",
    "        else:\n",
    "            plot_name=None\n",
    "            \n",
    "        fr_ids, en_ids, speech_feat = get_data_item(sp_fil, cat=cat)\n",
    "\n",
    "        # make prediction\n",
    "        pred_words, cp, tp, t, f = predict_sentence(speech_feat, en_ids, \n",
    "                                                    p_filt=p_filt, r_filt=r_filt)\n",
    "        metrics[\"cp\"].append(cp)\n",
    "        metrics[\"tp\"].append(tp)\n",
    "        metrics[\"t\"].append(t)\n",
    "        filter_count += (1 if f else 0)\n",
    "        \n",
    "        if display:\n",
    "            fr_line, en_line = get_text_lines(sp_fil, cat=cat)\n",
    "            print(\"-\"*80)\n",
    "            print(\"prediction for: {0:s}\".format(sp_fil))\n",
    "            display_prediction(fr_line, en_line, pred_words, prec=0, rec=0)\n",
    "\n",
    "    print(\"sentences matching filter = {0:d}\".format(filter_count))\n",
    "    return metrics\n",
    "\n",
    "def count_match(list1, list2):\n",
    "    # each list can have repeated elements. The count should account for this.\n",
    "    count1 = Counter(list1)\n",
    "    count2 = Counter(list2)\n",
    "    count2_keys = count2.keys()-set([UNK_ID, EOS_ID])\n",
    "    common_w = set(count1.keys()) & set(count2_keys)\n",
    "    matches = sum([min(count1[w], count2[w]) for w in common_w])\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_pplx(cat=\"dev\", num_sent=NUM_MINI_DEV_SENTENCES):\n",
    "    loss = 0\n",
    "    num_words = 0\n",
    "    for i, sp_fil in enumerate(sorted(list(text_data[cat].keys()))[:num_sent]):\n",
    "        with tqdm(total=num_sent) as pbar:\n",
    "            sys.stderr.flush()\n",
    "            out_str = \"loss={0:.6f}\".format(0)\n",
    "            pbar.set_description(out_str)\n",
    "            fr_ids, en_ids, speech_feat = get_data_item(sp_fil, cat=cat)\n",
    "\n",
    "            if len(fr_ids) > 0 and len(en_ids) > 0:\n",
    "                # compute loss\n",
    "                curr_loss = float(model.encode_decode_train(speech_feat, en_ids, train=False).data)\n",
    "                loss += curr_loss\n",
    "                num_words += len(en_ids)\n",
    "\n",
    "                out_str = \"loss={0:.6f}\".format(curr_loss)\n",
    "                pbar.set_description(out_str)\n",
    "            pbar.update(1)\n",
    "        # end of pbar\n",
    "    # end of for num_sent\n",
    "    \n",
    "    loss_per_word = loss / num_words\n",
    "    pplx = 2 ** loss_per_word\n",
    "    random_pplx = vocab_size_en\n",
    "\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "    print(\"{0:s} | {1:0.6f}\".format(\"dev perplexity\", pplx))\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "    return pplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_stats(hypothesis, reference):\n",
    "    yield len(hypothesis)\n",
    "    yield len(reference)\n",
    "    for n in range(1,5):\n",
    "        s_ngrams = Counter([tuple(hypothesis[i:i+n]) for i in range(len(hypothesis)+1-n)])\n",
    "        r_ngrams = Counter([tuple(reference[i:i+n]) for i in range(len(reference)+1-n)])\n",
    "        yield max([sum((s_ngrams & r_ngrams).values()), 0])\n",
    "        yield max([len(hypothesis)+1-n, 0])\n",
    "\n",
    "\n",
    "# Compute BLEU from collected statistics obtained by call(s) to bleu_stats\n",
    "def bleu(stats):\n",
    "    if len(list(filter(lambda x: x==0, stats))) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum([math.log(float(x)/y) for x,y in zip(stats[2::2],stats[3::2])]) / 4.\n",
    "    return math.exp(min([0, 1-float(r)/c]) + log_bleu_prec)\n",
    "\n",
    "\n",
    "def compute_bleu(cat=\"dev\", num_sent=NUM_MINI_DEV_SENTENCES):\n",
    "    list_of_references = []\n",
    "    list_of_hypotheses = []\n",
    "    for i, sp_fil in enumerate(sorted(list(text_data[cat].keys()))[:num_sent]):\n",
    "        with tqdm(total=num_sent) as pbar:\n",
    "            sys.stderr.flush()\n",
    "            out_str = \"predicting sentence={0:d}\".format(i)\n",
    "            pbar.update(1)\n",
    "\n",
    "            fr_ids, en_ids, speech_feat = get_data_item(sp_fil, cat=cat)\n",
    "            fr_line, en_line = get_text_lines(sp_fil, cat=cat)\n",
    "\n",
    "            # add reference translation\n",
    "            reference_words = en_line.strip().split()\n",
    "            list_of_references.append(reference_words)\n",
    "            \n",
    "            if len(fr_ids) > 0 and len(en_sent) > 0:\n",
    "                pred_sent, _ = model.encode_decode_predict(fr_ids)\n",
    "                pred_words = [i2w[\"en\"][w].decode() if w != EOS_ID else \"\" for w in pred_ids]\n",
    "                if CHAR_LEVEL:\n",
    "                    pred_words = \"\".join(pred_words)\n",
    "                    pred_words = pred_words.split()\n",
    "\n",
    "            else:\n",
    "                pred_words = []\n",
    "            list_of_hypotheses.append(pred_words)\n",
    "\n",
    "    stats = [0 for i in range(10)]\n",
    "    for (r,h) in zip(list_of_references, list_of_hypotheses):\n",
    "        stats = [sum(scores) for scores in zip(stats, bleu_stats(h,r))]\n",
    "    print(\"BLEU: %0.2f\" % (100 * bleu(stats)))\n",
    "\n",
    "    return (100 * bleu(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_lines(sp_fil, cat=\"train\"):\n",
    "    _, fr_line = get_ids(text_data[cat][sp_fil][\"es\"])\n",
    "    _, en_line = get_ids(text_data[cat][sp_fil][\"en\"])\n",
    "    \n",
    "    return fr_line, en_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ids(align_list, char_level=CHAR_LEVEL):\n",
    "    words = [a.word for a in align_list]\n",
    "    text_line = \" \".join(words)\n",
    "    \n",
    "    if not char_level:\n",
    "        symbols = [w.encode() for w in text_line.strip()]\n",
    "    else:\n",
    "        symbols = [c.encode() for c in list(text_line.strip())]\n",
    "    \n",
    "    return symbols, text_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_item(sp_fil, cat=\"train\"):    \n",
    "    fr_sent, _ = get_ids(text_data[cat][sp_fil][\"es\"])\n",
    "    en_sent, _ = get_ids(text_data[cat][sp_fil][\"en\"])\n",
    "\n",
    "    fr_ids = [w2i[\"fr\"].get(w, UNK_ID) for w in fr_sent]\n",
    "    en_ids = [w2i[\"en\"].get(w, UNK_ID) for w in en_sent]\n",
    "\n",
    "    speech_feat = xp.load(os.path.join(speech_dir, sp_fil+speech_extn))\n",
    "    return fr_ids, en_ids, speech_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English predictions, s=0, num=1:\n",
      "speech (1168, 1, 120)\n",
      "L0_enc before (1168, 1, 120)\n",
      "L0_enc out (584, 1, 256)\n",
      "L1_enc before (584, 1, 256)\n",
      "L1_enc out (292, 1, 256)\n",
      "L2_enc before (292, 1, 256)\n",
      "L2_enc out (146, 1, 256)\n",
      "L3_enc before (146, 1, 256)\n",
      "L3_enc out (73, 1, 256)\n",
      "L4_enc before (73, 1, 256)\n",
      "L4_enc out (73, 1, 128)\n",
      "speech (1168, 1, 120)\n",
      "L0_rev_enc before (1168, 1, 120)\n",
      "L0_rev_enc out (584, 1, 256)\n",
      "L1_rev_enc before (584, 1, 256)\n",
      "L1_rev_enc out (292, 1, 256)\n",
      "L2_rev_enc before (292, 1, 256)\n",
      "L2_rev_enc out (146, 1, 256)\n",
      "L3_rev_enc before (146, 1, 256)\n",
      "L3_rev_enc out (73, 1, 256)\n",
      "L4_rev_enc before (73, 1, 256)\n",
      "L4_rev_enc out (73, 1, 128)\n",
      "--------------------------------------------------------------------------------\n",
      "prediction for: 041.001\n",
      "--------------------------------------------------\n",
      "Src | NO PORQUE YA NO ESTOY TOMANDO CLASES AHORA ES ESO LO QUE ESTABA ESPERANDO ESTABA PRIMERO TOMANDO CLASES EN EL VERANO PERO MI AYUDA ECONóMICA PARA EL VERANO NO FUE MUY BUENA ENTONCES LAS TUVE QUE DEJAR LAS CLASES PORQUE SI NO NO VOY A TENER DINERO SUFICIENTE PARA PAGAR GRECIA\n",
      "Ref | NO BECAUSE I AM NOT TAKING CLASSES NOW THAT IS WHAT I WAS WAITING FOR AT FIRST WAS TAKING CLASSES IN SUMMER BUT MY ECONOMIC ECONOMIC AID AID IN SUMMER WAS NOT VERY GOOD THEN HAD TO LEAVE THE CLASSES BECAUSE IF DIDN\n",
      "Hyp | ¡¡_GO¡¡_GO¡¨H_GOI_GO_GO¡_GO¡_GO¡_GO¡_GOS_GOSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS\n",
      "--------------------------------------------------\n",
      "precision | 0.0000\n",
      "recall | 0.0000\n",
      "sentences matching filter = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cp': [20], 't': [214], 'tp': [90]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict(s=0, num=1, cat=\"train\", display=True, plot=False, p_filt=0, r_filt=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'T O   S E E   H O W   T H E   D O C U M E N T S   A R E   A N D   E L S E   W H A T   H A P P E N S   H A P P E N S   I S   A S   I   H A V E   C H A N G E D   H O U S E   D O N   D O N'\n"
     ]
    }
   ],
   "source": [
    "# print(b\" \".join(get_ids(text_data[\"train\"][\"041.004\"][\"en\"])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_loop(num_training, num_epochs, log_mode=\"a\", last_epoch_id=0):\n",
    "    # Set up log file for loss\n",
    "    log_dev_fil = open(log_dev_fil_name, mode=log_mode)\n",
    "    log_dev_csv = csv.writer(log_dev_fil, lineterminator=\"\\n\")\n",
    "\n",
    "    # initialize perplexity on dev set\n",
    "    # save model when new epoch value is lower than previous\n",
    "    pplx = float(\"inf\")\n",
    "\n",
    "    sys.stderr.flush()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, sp_fil in enumerate(sorted(list(text_data[\"train\"].keys()))[:num_training], start=1):\n",
    "            with tqdm(total=num_training) as pbar:\n",
    "                sys.stderr.flush()\n",
    "                print(sp_fil)\n",
    "                loss_per_epoch = 0\n",
    "                out_str = \"epoch={0:d}, loss={1:.6f}, mean loss={2:.6f}\".format(epoch+1, 0, 0)\n",
    "                pbar.set_description(out_str)\n",
    "                \n",
    "                # get the word/character ids\n",
    "                fr_ids, en_ids, speech_feat = get_data_item(sp_fil, cat=\"train\")\n",
    "                print(speech_feat.shape)\n",
    "\n",
    "                it = (epoch * num_training) + i\n",
    "\n",
    "                # compute loss\n",
    "                loss = model.encode_decode_train(speech_feat, en_ids, train=True)\n",
    "\n",
    "                # set up for backprop\n",
    "                model.cleargrads()\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                optimizer.update()\n",
    "                # store loss value for display\n",
    "                loss_val = float(loss.data)\n",
    "                loss_per_epoch += loss_val\n",
    "\n",
    "                out_str = \"epoch={0:d}, loss={1:.6f}, mean loss={2:.6f}\".format(\n",
    "                           epoch+1, it, loss_val, (loss_per_epoch / i))\n",
    "                pbar.set_description(out_str)\n",
    "                pbar.update(1)\n",
    "            # end with pbar\n",
    "        # end for num_training\n",
    "\n",
    "        print(\"finished training on {0:d} sentences\".format(num_training))\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "        print(\"computing perplexity\")\n",
    "        pplx_new = compute_pplx(cat=\"dev\", num_sent=NUM_MINI_DEV_SENTENCES)\n",
    "\n",
    "        if pplx_new > pplx:\n",
    "            print(\"perplexity went up during training, breaking out of loop\")\n",
    "            break\n",
    "        \n",
    "        pplx = pplx_new\n",
    "        print(log_dev_fil_name)\n",
    "        print(model_fil.replace(\".model\", \"_{0:d}.model\".format(epoch+1)))\n",
    "\n",
    "        if (epoch+1) % ITERS_TO_SAVE == 0:\n",
    "            bleu_score = compute_bleu(cat=\"dev\", num_sent=NUM_MINI_DEV_SENTENCES)\n",
    "            print(\"Saving model\")\n",
    "            serializers.save_npz(model_fil.replace(\".model\", \"_{0:d}.model\".format(last_epoch_id+epoch+1)), \n",
    "                                 model)\n",
    "            print(\"Finished saving model\")\n",
    "\n",
    "        # log pplx and bleu score\n",
    "        log_dev_csv.writerow([(last_epoch_id+epoch+1), pplx_new, bleu_score])\n",
    "        log_dev_fil.flush()\n",
    "    \n",
    "    print(\"Simple predictions (╯°□°）╯︵ ┻━┻\")\n",
    "    print(\"training set predictions\")\n",
    "    _ = predict(s=0, num=2, cat=\"train\", display=True, plot=False, p_filt=0, r_filt=0)\n",
    "    print(\"Simple predictions (╯°□°）╯︵ ┻━┻\")\n",
    "    print(\"dev set predictions\")\n",
    "    _ = predict(s=0, num=2, cat=\"dev\", display=True, plot=False, p_filt=0, r_filt=0)\n",
    "\n",
    "    print(\"Final saving model\")\n",
    "    serializers.save_npz(model_fil, model)\n",
    "    print(\"Finished saving model\")\n",
    "\n",
    "    # close log file\n",
    "    log_dev_fil.close()\n",
    "\n",
    "    print(log_dev_fil_name)\n",
    "    print(model_fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward_states = model[model.lstm_enc[-1]].h\n",
    "# backward_states = model[model.lstm_rev_enc[-1]].h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.enc_states = F.concat((forward_states, backward_states), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "041.001\n",
      "(1168, 120)\n",
      "speech (1168, 1, 120)\n",
      "L0_enc before (1168, 1, 120)\n",
      "L0_enc out (584, 1, 256)\n",
      "L1_enc before (584, 1, 256)\n",
      "L1_enc out (292, 1, 256)\n",
      "L2_enc before (292, 1, 256)\n",
      "L2_enc out (146, 1, 256)\n",
      "L3_enc before (146, 1, 256)\n",
      "L3_enc out (73, 1, 256)\n",
      "L4_enc before (73, 1, 256)\n",
      "L4_enc out (73, 1, 128)\n",
      "speech (1168, 1, 120)\n",
      "L0_rev_enc before (1168, 1, 120)\n",
      "L0_rev_enc out (584, 1, 256)\n",
      "L1_rev_enc before (584, 1, 256)\n",
      "L1_rev_enc out (292, 1, 256)\n",
      "L2_rev_enc before (292, 1, 256)\n",
      "L2_rev_enc out (146, 1, 256)\n",
      "L3_rev_enc before (146, 1, 256)\n",
      "L3_rev_enc out (73, 1, 256)\n",
      "L4_rev_enc before (73, 1, 256)\n",
      "L4_rev_enc out (73, 1, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch=1, loss=1.000000, mean loss=846.411926: 100%|██████████| 1/1 [00:21<00:00, 21.76s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training on 1 sentences\n",
      "--------------------------------------------------\n",
      "computing perplexity\n",
      "speech (176, 1, 120)\n",
      "L0_enc before (176, 1, 120)\n",
      "L0_enc out (88, 1, 256)\n",
      "L1_enc before (88, 1, 256)\n",
      "L1_enc out (44, 1, 256)\n",
      "L2_enc before (44, 1, 256)\n",
      "L2_enc out (22, 1, 256)\n",
      "L3_enc before (22, 1, 256)\n",
      "L3_enc out (11, 1, 256)\n",
      "L4_enc before (11, 1, 256)\n",
      "L4_enc out (11, 1, 128)\n",
      "speech (176, 1, 120)\n",
      "L0_rev_enc before (176, 1, 120)\n",
      "L0_rev_enc out (88, 1, 256)\n",
      "L1_rev_enc before (88, 1, 256)\n",
      "L1_rev_enc out (44, 1, 256)\n",
      "L2_rev_enc before (44, 1, 256)\n",
      "L2_rev_enc out (22, 1, 256)\n",
      "L3_rev_enc before (22, 1, 256)\n",
      "L3_rev_enc out (11, 1, 256)\n",
      "L4_rev_enc before (11, 1, 256)\n",
      "L4_rev_enc out (11, 1, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=145.748795: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "dev perplexity | 15.339256\n",
      "--------------------------------------------------\n",
      "es_speech_to_en_char_model/dev_17394sen_5-2layers_128units_es_en_speech2text_callhome_es_en_1.log\n",
      "es_speech_to_en_char_model/seq2seq_17394sen_5-2layers_128units_es_en_speech2text_callhome_es_en_1_1.model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'last_epoch_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-44d0a5ac9590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-70855a046350>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(num_training, num_epochs, log_mode)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# log pplx and bleu score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mlog_dev_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_epoch_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpplx_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mlog_dev_fil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'last_epoch_id' is not defined"
     ]
    }
   ],
   "source": [
    "train_loop(num_training=1, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
