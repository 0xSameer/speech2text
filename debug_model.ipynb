{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #coding: utf-8\n",
    "\n",
    "from basics import *\n",
    "import prep_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SpeechEncoderDecoder(Chain):\n",
    "    def __init__(self, m_cfg, gpuid):\n",
    "        self.m_cfg = m_cfg\n",
    "        self.gpuid = gpuid\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "        if self.gpuid >= 0:\n",
    "            cuda.get_device(self.gpuid).use()\n",
    "\n",
    "        super(SpeechEncoderDecoder, self).__init__()\n",
    "        self.init_model()\n",
    "\n",
    "    def init_params(self):\n",
    "        #----------------------------------------------------------------------\n",
    "        # determine rnn type\n",
    "        #----------------------------------------------------------------------\n",
    "        if self.m_cfg['rnn_unit'] == RNN_GRU:\n",
    "            self.RNN = L.GRU\n",
    "        else:\n",
    "            self.RNN = L.LSTM\n",
    "        #----------------------------------------------------------------------\n",
    "        # get vocab size\n",
    "        #----------------------------------------------------------------------\n",
    "        if 'fisher' in self.m_cfg['train_set']:\n",
    "            if self.m_cfg['stemmify'] == False:\n",
    "                v_path = os.path.join(self.m_cfg['data_path'],\n",
    "                                                'train_vocab.dict')\n",
    "            else:\n",
    "                v_path = os.path.join(self.m_cfg['data_path'],\n",
    "                                                'train_stemmed_vocab.dict')\n",
    "        else:\n",
    "            v_path = os.path.join(self.m_cfg['data_path'],\n",
    "                                            'ch_train_vocab.dict')\n",
    "        vocab_dict = pickle.load(open(v_path, \"rb\"))\n",
    "        if self.m_cfg['enc_key'] != 'sp':\n",
    "            self.v_size_es = len(vocab_dict[self.m_cfg['enc_key']]['w2i'])\n",
    "        else:\n",
    "            self.v_size_es = 0\n",
    "        self.v_size_en = len(vocab_dict[self.m_cfg['dec_key']]['w2i'])\n",
    "        #----------------------------------------------------------------------\n",
    "        # sim dict\n",
    "        #----------------------------------------------------------------------\n",
    "        if \"sample_out\" in self.m_cfg and self.m_cfg[\"sample_out\"] == True:\n",
    "            sim_dict_path = os.path.join(self.m_cfg['data_path'], self.m_cfg['sim_dict'])\n",
    "            if os.path.exists(sim_dict_path):\n",
    "                self.sim_dict = pickle.load(open(sim_dict_path, \"rb\"))\n",
    "            else:\n",
    "                print(\"{0:s} -- sim dict not found!\".format(sim_dict_path))\n",
    "        #----------------------------------------------------------------------\n",
    "        # bag-of-words dict\n",
    "        #----------------------------------------------------------------------\n",
    "        bow_dict_path = os.path.join(self.m_cfg['data_path'],\n",
    "                                     'train_top_K_enw.dict')\n",
    "        if os.path.exists(bow_dict_path):\n",
    "            self.bow_dict = pickle.load(open(bow_dict_path, \"rb\"))\n",
    "            self.bag_size_en = len(self.bow_dict['w2i'])\n",
    "        else:\n",
    "            print(\"bag-of-words data not found\")\n",
    "        #----------------------------------------------------------------------\n",
    "\n",
    "    def add_rnn_layers(self, layer_names, in_units, out_units):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        for i, rnn_name in enumerate(layer_names):\n",
    "            #------------------------------------------------------------------\n",
    "            # for first layer, use in_units\n",
    "            #------------------------------------------------------------------\n",
    "            curr_in = in_units if i == 0 else out_units\n",
    "            #------------------------------------------------------------------\n",
    "            # add rnn layer\n",
    "            #------------------------------------------------------------------\n",
    "            self.add_link(rnn_name, self.RNN(curr_in, out_units))\n",
    "            #------------------------------------------------------------------\n",
    "            # add layer normalization\n",
    "            #------------------------------------------------------------------\n",
    "            if self.m_cfg['ln']:\n",
    "                self.add_link(\"{0:s}_ln\".format(rnn_name), L.LayerNormalization(out_units))\n",
    "            #------------------------------------------------------------------\n",
    "\n",
    "    def init_rnn_model(self, in_dim):\n",
    "        h_units = self.m_cfg['hidden_units']\n",
    "        #----------------------------------------------------------------------\n",
    "        # add encoder layers\n",
    "        #----------------------------------------------------------------------\n",
    "        self.rnn_enc = [\"L{0:d}_enc\".format(i)\n",
    "                         for i in range(self.m_cfg['enc_layers'])]\n",
    "        self.add_rnn_layers(self.rnn_enc, in_dim, h_units)\n",
    "\n",
    "        if self.m_cfg['bi_rnn']:\n",
    "            #------------------------------------------------------------------\n",
    "            # if bi rnn, add rev rnn layer\n",
    "            #------------------------------------------------------------------\n",
    "            self.rnn_rev_enc = [\"L{0:d}_rev_enc\".format(i) for i in range(self.m_cfg['enc_layers'])]\n",
    "            self.add_rnn_layers(self.rnn_rev_enc, in_dim, h_units)\n",
    "\n",
    "        if \"bagofwords\" not in self.m_cfg or self.m_cfg['bagofwords'] == False:\n",
    "            #------------------------------------------------------------------\n",
    "            # add attention layers\n",
    "            #------------------------------------------------------------------\n",
    "            a_units = self.m_cfg['attn_units']\n",
    "            if self.m_cfg['bi_rnn']:\n",
    "                self.add_link(\"attn_Wa\", L.Linear(2*h_units, 2*h_units))\n",
    "                #--------------------------------------------------------------\n",
    "                # context layer = 2*h_units from enc + 2*h_units from dec\n",
    "                #--------------------------------------------------------------\n",
    "                self.add_link(\"context\", L.Linear(4*h_units, a_units))\n",
    "            else:\n",
    "                self.add_link(\"attn_Wa\", L.Linear(h_units, h_units))\n",
    "                #--------------------------------------------------------------\n",
    "                # context layer = 1*h_units from enc + 1*h_units from dec\n",
    "                #--------------------------------------------------------------\n",
    "                self.add_link(\"context\", L.Linear(2*h_units, a_units))\n",
    "            #------------------------------------------------------------------\n",
    "            # add decoder layers\n",
    "            #------------------------------------------------------------------\n",
    "            e_units = self.m_cfg['embedding_units']\n",
    "            # first layer appends previous ht, and therefore,\n",
    "            # in_units = embed units + hidden units\n",
    "            self.rnn_dec = [\"L{0:d}_dec\".format(i)\n",
    "                            for i in range(self.m_cfg['dec_layers'])]\n",
    "            #------------------------------------------------------------------\n",
    "            # decoder rnn input = emb + prev. context vector\n",
    "            #------------------------------------------------------------------\n",
    "            if self.m_cfg['bi_rnn']:\n",
    "                self.add_rnn_layers(self.rnn_dec, e_units+a_units, 2*h_units)\n",
    "            else:\n",
    "                self.add_rnn_layers(self.rnn_dec, e_units+a_units, h_units)\n",
    "            #------------------------------------------------------------------\n",
    "\n",
    "    def init_deep_cnn_model(self):\n",
    "        CNN_IN_DIM = (self.m_cfg['sp_dim'] if self.m_cfg['enc_key'] == 'sp'\n",
    "                             else self.m_cfg['embedding_units'])\n",
    "        # ---------------------------------------------------------------------\n",
    "        # initialize list of cnn layers\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.cnns = []\n",
    "        if len(self.m_cfg['cnn_layers']) > 0:\n",
    "            # -----------------------------------------------------------------\n",
    "            # using He initializer\n",
    "            # -----------------------------------------------------------------\n",
    "            w = chainer.initializers.HeNormal()\n",
    "            # add CNN layers\n",
    "            cnn_out_dim = 0\n",
    "            self.reduce_dim_len = 1\n",
    "            reduce_dim = CNN_IN_DIM\n",
    "            for i, l in enumerate(self.m_cfg['cnn_layers']):\n",
    "                lname = \"CNN_{0:d}\".format(i)\n",
    "                cnn_out_dim += l[\"out_channels\"]\n",
    "                self.cnns.append(lname)\n",
    "                self.add_link(lname, L.Convolution2D(**l, initialW=w))\n",
    "                reduce_dim = math.ceil(reduce_dim / l[\"stride\"][1])\n",
    "                self.reduce_dim_len *= l[\"stride\"][0]\n",
    "                if self.m_cfg['bn']:\n",
    "                    # ---------------------------------------------------------\n",
    "                    # add batch normalization\n",
    "                    # ---------------------------------------------------------\n",
    "                    self.add_link('{0:s}_bn'.format(lname), L.BatchNormalization((l[\"out_channels\"])))\n",
    "                    # ---------------------------------------------------------\n",
    "            self.cnn_out_dim = self.m_cfg['cnn_layers'][-1][\"out_channels\"]\n",
    "            # -----------------------------------------------------------------\n",
    "            # cnn output has reduced dimensions based on strides\n",
    "            # -----------------------------------------------------------------\n",
    "            self.cnn_out_dim *= reduce_dim\n",
    "            # -----------------------------------------------------------------\n",
    "        else:\n",
    "            # -----------------------------------------------------------------\n",
    "            # no cnns added\n",
    "            # -----------------------------------------------------------------\n",
    "            self.cnn_out_dim = CNN_IN_DIM\n",
    "            # -----------------------------------------------------------------\n",
    "    # end init_deep_cnn_model()\n",
    "\n",
    "    def init_model(self):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        # ---------------------------------------------------------------------\n",
    "        # add enc embedding layer if text model\n",
    "        # ---------------------------------------------------------------------\n",
    "        if self.m_cfg['enc_key'] != 'sp':\n",
    "            self.add_link(\"embed_enc\", L.EmbedID(self.v_size_es,\n",
    "                                                self.m_cfg['embedding_units']))\n",
    "        # ---------------------------------------------------------------------\n",
    "        # add cnn layer\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.init_deep_cnn_model()\n",
    "        rnn_in_units = self.cnn_out_dim\n",
    "        # ---------------------------------------------------------------------\n",
    "        # add rnn layers\n",
    "        # ---------------------------------------------------------------------\n",
    "        print(\"cnn_out_dim = rnn_in_units = \", rnn_in_units)\n",
    "        self.init_rnn_model(rnn_in_units)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # add decoder/bag-of-words\n",
    "        # ---------------------------------------------------------------------\n",
    "        if \"bagofwords\" not in self.m_cfg or self.m_cfg['bagofwords'] == False:\n",
    "            # -----------------------------------------------------------------\n",
    "            # add dec embedding layer\n",
    "            # -----------------------------------------------------------------\n",
    "            self.add_link(\"embed_dec\", L.EmbedID(self.v_size_en,\n",
    "                                                 self.m_cfg['embedding_units']))\n",
    "            # -----------------------------------------------------------------\n",
    "            # add output layers\n",
    "            # -----------------------------------------------------------------\n",
    "            self.add_link(\"out\", L.Linear(self.m_cfg['attn_units'],\n",
    "                                          self.v_size_en))\n",
    "            # -----------------------------------------------------------------\n",
    "            # create masking array for pad id\n",
    "            # -----------------------------------------------------------------\n",
    "            with cupy.cuda.Device(self.gpuid):\n",
    "                self.mask_pad_id = xp.ones(self.v_size_en, dtype=xp.float32)\n",
    "            # make the class weight for pad id equal to 0\n",
    "            # this way loss will not be computed for this predicted loss\n",
    "            self.mask_pad_id[0] = 0\n",
    "            # -----------------------------------------------------------------\n",
    "        else:\n",
    "            # -----------------------------------------------------------------\n",
    "            # add bag-of-words output layer\n",
    "            # -----------------------------------------------------------------\n",
    "            if self.m_cfg['bi_rnn']:\n",
    "                h_units = self.m_cfg['hidden_units'] * 2\n",
    "            else:\n",
    "                h_units = self.m_cfg['hidden_units']\n",
    "\n",
    "            # Add highway layers for classification\n",
    "            self.highway = []\n",
    "            for i in range(self.m_cfg[\"highway_layers\"]):\n",
    "                lname = \"HIGHWAY_{0:d}\".format(i)\n",
    "                self.highway.append(lname)\n",
    "                self.add_link(lname, L.Highway(h_units))\n",
    "            # Add final prediction layer\n",
    "            self.add_link(\"out\", L.Linear(h_units, self.bag_size_en))\n",
    "            # -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def reset_state(self):\n",
    "        # ---------------------------------------------------------------------\n",
    "        # reset the state of LSTM layers\n",
    "        # ---------------------------------------------------------------------\n",
    "        if self.m_cfg['bi_rnn']:\n",
    "            for rnn_name in self.rnn_enc + self.rnn_rev_enc:\n",
    "                self[rnn_name].reset_state()\n",
    "        else:\n",
    "            for rnn_name in self.rnn_enc:\n",
    "                self[rnn_name].reset_state()\n",
    "\n",
    "        if \"bagofwords\" not in self.m_cfg or self.m_cfg['bagofwords'] == False:\n",
    "            for rnn_name in self.rnn_dec:\n",
    "                self[rnn_name].reset_state()\n",
    "\n",
    "        self.loss = 0\n",
    "\n",
    "    def set_decoder_state(self):\n",
    "        # ---------------------------------------------------------------------\n",
    "        # set the hidden and cell state (LSTM) of the first RNN in the decoder\n",
    "        # ---------------------------------------------------------------------\n",
    "        if self.m_cfg['bi_rnn']:\n",
    "            for enc, rev_enc, dec in zip(self.rnn_enc,\n",
    "                                         self.rnn_rev_enc,\n",
    "                                         self.rnn_dec):\n",
    "                h_state = F.concat((self[enc].h, self[rev_enc].h))\n",
    "                if self.m_cfg['rnn_unit'] == RNN_LSTM:\n",
    "                    c_state = F.concat((self[enc].c, self[rev_enc].c))\n",
    "                    self[dec].set_state(c_state, h_state)\n",
    "                else:\n",
    "                    self[dec].set_state(h_state)\n",
    "        else:\n",
    "            for enc, dec in zip(self.rnn_enc, self.rnn_dec):\n",
    "                if self.m_cfg['rnn_unit'] == RNN_LSTM:\n",
    "                    self[dec].set_state(self[enc].c, self[enc].h)\n",
    "                else:\n",
    "                    self[dec].set_state(self[enc].h)\n",
    "        # ---------------------------------------------------------------------\n",
    "\n",
    "    def compute_context_vector(self, dec_h):\n",
    "        batch_size, n_units = dec_h.shape\n",
    "        # attention weights for the hidden states of each word in the input list\n",
    "        # ---------------------------------------------------------------------\n",
    "        # compute weights\n",
    "        ht = self.attn_Wa(dec_h)\n",
    "        weights = F.batch_matmul(self.enc_states, ht)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # '''\n",
    "        # this line is valid when no max pooling or sequence length manipulation is performed\n",
    "        # weights = F.where(self.mask, weights, self.minf)\n",
    "            # '''\n",
    "        # ---------------------------------------------------------------------\n",
    "        # softmax to compute alphas\n",
    "        # ---------------------------------------------------------------------\n",
    "        alphas = F.softmax(weights)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # compute context vector\n",
    "        # ---------------------------------------------------------------------\n",
    "        cv = F.squeeze(F.batch_matmul(F.swapaxes(self.enc_states, 2, 1), alphas), axis=2)\n",
    "        # ---------------------------------------------------------------------\n",
    "        return cv, alphas\n",
    "        # ---------------------------------------------------------------------\n",
    "\n",
    "    def feed_rnn(self, rnn_in, rnn_layers, highway_layers=None):\n",
    "        hs = rnn_in\n",
    "        for rnn_layer in rnn_layers:\n",
    "            # -----------------------------------------------------------------\n",
    "            # apply rnn\n",
    "            # -----------------------------------------------------------------\n",
    "            if self.m_cfg['rnn_dropout'] > 0:\n",
    "                hs = F.dropout(self[rnn_layer](hs),\n",
    "                               ratio=self.m_cfg['rnn_dropout'])\n",
    "            else:\n",
    "                hs = self[rnn_layer](hs)\n",
    "            # -----------------------------------------------------------------\n",
    "            # layer normalization\n",
    "            # -----------------------------------------------------------------\n",
    "            if self.m_cfg['ln']:\n",
    "                ln_name = \"{0:s}_ln\".format(rnn_layer)\n",
    "                hs = self[ln_name](hs)\n",
    "            # -----------------------------------------------------------------\n",
    "            # RELU activation\n",
    "            # -----------------------------------------------------------------\n",
    "            if 'rnn_relu' in self.m_cfg and self.m_cfg['rnn_relu'] == True:\n",
    "                hs = F.relu(hs)\n",
    "            # -----------------------------------------------------------------\n",
    "        return hs\n",
    "\n",
    "    def encode(self, data_in, rnn_layers):\n",
    "        h = self.feed_rnn(data_in, rnn_layers)\n",
    "        return h\n",
    "\n",
    "    def decode(self, word, ht):\n",
    "        # ---------------------------------------------------------------------\n",
    "        # get embedding\n",
    "        # ---------------------------------------------------------------------\n",
    "        if 'embed_dropout' in self.m_cfg:\n",
    "            embed_id = F.dropout(self.embed_dec(word),\n",
    "                                 ratio=self.m_cfg['rnn_dropout'])\n",
    "        else:\n",
    "            embed_id = self.embed_dec(word)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # apply rnn - input feeding, use previous ht\n",
    "        # ---------------------------------------------------------------------\n",
    "        rnn_in = F.concat((embed_id, ht), axis=1)\n",
    "        h = self.feed_rnn(rnn_in, self.rnn_dec)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # compute context vector\n",
    "        # ---------------------------------------------------------------------\n",
    "        cv, _ = self.compute_context_vector(h)\n",
    "        cv_hdec = F.concat((cv, h), axis=1)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # compute attentional hidden state\n",
    "        # ---------------------------------------------------------------------\n",
    "        ht = F.tanh(self.context(cv_hdec))\n",
    "        # ---------------------------------------------------------------------\n",
    "        # make prediction\n",
    "        # ---------------------------------------------------------------------\n",
    "        if self.m_cfg['out_dropout'] > 0:\n",
    "            predicted_out = F.dropout(self.out(ht),\n",
    "                                      ratio=self.m_cfg['out_dropout'])\n",
    "        else:\n",
    "            predicted_out = self.out(ht)\n",
    "        # ---------------------------------------------------------------------\n",
    "        return predicted_out, ht\n",
    "\n",
    "    def decode_batch(self, decoder_batch, teacher_ratio):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        batch_size = decoder_batch.shape[1]\n",
    "        loss = 0\n",
    "        # ---------------------------------------------------------------------\n",
    "        # initialize hidden states as a zero vector\n",
    "        # ---------------------------------------------------------------------\n",
    "        a_units = self.m_cfg['attn_units']\n",
    "        ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "        # ---------------------------------------------------------------------\n",
    "        decoder_input = decoder_batch[0]\n",
    "        # for all sequences in the batch, feed the characters one by one\n",
    "        for curr_word, next_word in zip(decoder_batch, decoder_batch[1:]):\n",
    "            # -----------------------------------------------------------------\n",
    "            # teacher forcing logic\n",
    "            # -----------------------------------------------------------------\n",
    "            use_label = True if random.random() < teacher_ratio else False\n",
    "            if use_label:\n",
    "                decoder_input = curr_word\n",
    "            # -----------------------------------------------------------------\n",
    "            # encode tokens\n",
    "            # -----------------------------------------------------------------\n",
    "            predicted_out, ht = self.decode(decoder_input, ht)\n",
    "            decoder_input = F.argmax(predicted_out, axis=1)\n",
    "            # -----------------------------------------------------------------\n",
    "            # compute loss\n",
    "            # -----------------------------------------------------------------\n",
    "            if \"smooth_out\" in self.m_cfg and self.m_cfg[\"smooth_out\"] == True:\n",
    "                t = xp.zeros(shape=predicted_out.shape, dtype='i')\n",
    "                for i, w in enumerate(next_word.data.tolist()):\n",
    "                    if w == PAD_ID:\n",
    "                        t[i,:] = -1\n",
    "                    else:\n",
    "                        t[i,self.sim_dict['i'][w]] = 1\n",
    "                loss_arr = F.sigmoid_cross_entropy(predicted_out, t, normalize=True)\n",
    "            elif \"sample_out\" in self.m_cfg and self.m_cfg[\"sample_out\"] == True:\n",
    "                t_alt = xp.copy(next_word.data)\n",
    "                # sample and replace each element in the batch\n",
    "                for i in range(len(t_alt)):\n",
    "                    # use_sample = True if random.random() > self.m_cfg[\"sample_out_prob\"] else False\n",
    "                    if random.random() > self.m_cfg[\"sample_out_prob\"]:\n",
    "                        t_alt[i] = xp.random.choice(self.sim_dict['i'][int(t_alt[i])],1)\n",
    "\n",
    "                loss_arr = F.softmax_cross_entropy(predicted_out, t_alt,\n",
    "                                               class_weight=self.mask_pad_id)\n",
    "            else:\n",
    "                loss_arr = F.softmax_cross_entropy(predicted_out, next_word,\n",
    "                                               class_weight=self.mask_pad_id)\n",
    "            loss += loss_arr\n",
    "            # -----------------------------------------------------------------\n",
    "        return loss\n",
    "\n",
    "    def predict_batch(self, batch_size, pred_limit, y=None, display=False):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        # max number of predictions to make\n",
    "        # if labels are provided, this variable is not used\n",
    "        stop_limit = pred_limit\n",
    "        # to track number of predictions made\n",
    "        npred = 0\n",
    "        # to store loss\n",
    "        loss = 0\n",
    "        # if labels are provided, use them for computing loss\n",
    "        compute_loss = True if y is not None else False\n",
    "        # ---------------------------------------------------------------------\n",
    "        if compute_loss:\n",
    "            stop_limit = len(y)-1\n",
    "            # get starting word to initialize decoder\n",
    "            curr_word = y[0]\n",
    "        else:\n",
    "            # intialize starting word to GO_ID symbol\n",
    "            curr_word = Variable(xp.full((batch_size,), GO_ID, dtype=xp.int32))\n",
    "        # ---------------------------------------------------------------------\n",
    "        # flag to track if all sentences in batch have predicted EOS\n",
    "        # ---------------------------------------------------------------------\n",
    "        with cupy.cuda.Device(self.gpuid):\n",
    "            check_if_all_eos = xp.full((batch_size,), False, dtype=xp.bool_)\n",
    "        # ---------------------------------------------------------------------\n",
    "        a_units = self.m_cfg['attn_units']\n",
    "        ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "        # ---------------------------------------------------------------------\n",
    "        while npred < (stop_limit):\n",
    "            # -----------------------------------------------------------------\n",
    "            # decode and predict\n",
    "            pred_out, ht = self.decode(curr_word, ht)\n",
    "            pred_word = F.argmax(pred_out, axis=1)\n",
    "            # -----------------------------------------------------------------\n",
    "            # save prediction at this time step\n",
    "            # -----------------------------------------------------------------\n",
    "            if npred == 0:\n",
    "                pred_sents = pred_word.data\n",
    "            else:\n",
    "                pred_sents = xp.vstack((pred_sents, pred_word.data))\n",
    "            # -----------------------------------------------------------------\n",
    "            if compute_loss:\n",
    "                # compute loss\n",
    "                loss += F.softmax_cross_entropy(pred_out, y[npred+1],\n",
    "                                                   class_weight=self.mask_pad_id)\n",
    "            # -----------------------------------------------------------------\n",
    "            curr_word = pred_word\n",
    "            # check if EOS is predicted for all sentences\n",
    "            # -----------------------------------------------------------------\n",
    "            check_if_all_eos[pred_word.data == EOS_ID] = True\n",
    "            # if xp.all(check_if_all_eos == EOS_ID):\n",
    "            if xp.all(check_if_all_eos):\n",
    "                break\n",
    "            # -----------------------------------------------------------------\n",
    "            # increment number of predictions made\n",
    "            npred += 1\n",
    "            # -----------------------------------------------------------------\n",
    "        return pred_sents.T, loss\n",
    "\n",
    "    def decode_bow_batch(self, y):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        # use final rnn state for both fwd and rev (if configured) rnns\n",
    "\n",
    "        if self.m_cfg['highway_layers'] > 0:\n",
    "            highway_h = self.forward_highway(self.h_final_rnn)\n",
    "\n",
    "        predicted_out = self.out(highway_h)\n",
    "\n",
    "        loss = F.sigmoid_cross_entropy(predicted_out, y, reduce=\"no\")\n",
    "\n",
    "        loss_weights = xp.ones(shape=y.data.shape, dtype=\"f\")\n",
    "        loss_weights[y.data < 0] = 0\n",
    "        loss_weights[y.data == 0] = self.m_cfg[\"negative_weight\"]\n",
    "        loss_weights[y.data > 0] = self.m_cfg[\"positive_weight\"]\n",
    "        #loss_avg = F.average(F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce='no'), weights=loss_weights)\n",
    "        loss_avg = F.mean(loss_weights * loss)\n",
    "        # ---------------------------------------------------------------------\n",
    "        pred_words = []\n",
    "        pred_probs = []\n",
    "        pred_limit = self.m_cfg['max_en_pred']\n",
    "        for row in predicted_out.data:\n",
    "            pred_inds = xp.where(row >= self.m_cfg[\"pred_thresh\"])[0]\n",
    "            if len(pred_inds) > pred_limit:\n",
    "                pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "            #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "            pred_words.append([i for i in pred_inds.tolist() if i > 3])\n",
    "            pred_probs.append(row)\n",
    "\n",
    "        return pred_words, loss_avg, pred_probs\n",
    "\n",
    "    def predict_bow_batch(self, batch_size, pred_limit, y=None, display=False):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        # to store loss\n",
    "        loss = 0\n",
    "        loss_avg = 0\n",
    "        # if labels are provided, use them for computing loss\n",
    "        compute_loss = True if y is not None else False\n",
    "        pred_words = []\n",
    "        pred_probs = []\n",
    "        # ---------------------------------------------------------------------\n",
    "        # decode and predict\n",
    "        if self.m_cfg['highway_layers'] > 0:\n",
    "            highway_h = self.forward_highway(self.h_final_rnn)\n",
    "\n",
    "        predicted_out = self.out(highway_h)\n",
    "\n",
    "        for row in predicted_out.data:\n",
    "            pred_inds = xp.where(row >= self.m_cfg[\"pred_thresh\"])[0]\n",
    "            if len(pred_inds) > pred_limit:\n",
    "                pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "            #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "            pred_words.append([i for i in pred_inds.tolist() if i > 3])\n",
    "            pred_probs.append(row)\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        if compute_loss:\n",
    "            # compute loss\n",
    "            loss = F.sigmoid_cross_entropy(predicted_out, y, reduce=\"no\")\n",
    "\n",
    "            loss_weights = xp.ones(shape=y.data.shape, dtype=\"f\")\n",
    "            loss_weights[y.data < 0] = 0\n",
    "            loss_weights[y.data == 0] = self.m_cfg[\"negative_weight\"]\n",
    "            loss_weights[y.data > 0] = self.m_cfg[\"positive_weight\"]\n",
    "            #loss_avg = F.average(F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce='no'), weights=loss_weights)\n",
    "            loss_avg = F.mean(loss_weights * loss)\n",
    "        # -----------------------------------------------------------------\n",
    "        return pred_words, loss_avg, pred_probs\n",
    "\n",
    "    def forward_deep_cnn(self, h):\n",
    "        # ---------------------------------------------------------------------\n",
    "        # check and prepare for 2d convolutions\n",
    "        # ---------------------------------------------------------------------\n",
    "        h = F.expand_dims(h, 2)\n",
    "        h = F.swapaxes(h,1,2)\n",
    "        # ---------------------------------------------------------------------\n",
    "        for i, cnn_layer in enumerate(self.cnns):\n",
    "            # -----------------------------------------------------------------\n",
    "            # apply cnn\n",
    "            # -----------------------------------------------------------------\n",
    "            h = self[cnn_layer](h)\n",
    "            # -----------------------------------------------------------------\n",
    "            # batch normalization before non-linearity\n",
    "            # -----------------------------------------------------------------\n",
    "            if self.m_cfg['bn']:\n",
    "                bn_lname = '{0:s}_bn'.format(cnn_layer)\n",
    "                h = self[bn_lname](h)\n",
    "            # -----------------------------------------------------------------\n",
    "            h = F.relu(h)\n",
    "            # -----------------------------------------------------------------\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # prepare return\n",
    "        # batch size * num time frames after pooling * cnn out dim\n",
    "        # ---------------------------------------------------------------------\n",
    "        h = F.swapaxes(h,1,2)\n",
    "        h = F.reshape(h, h.shape[:2] + tuple([-1]))\n",
    "        h = F.rollaxis(h,1)\n",
    "        # ---------------------------------------------------------------------\n",
    "        return h\n",
    "\n",
    "    def forward_highway(self, X):\n",
    "        for i in range(len(self.highway)):\n",
    "            if self.m_cfg['highway_dropout'] > 0:\n",
    "                h = F.dropout(self[self.highway[i]](X), ratio=self.m_cfg['highway_dropout'])\n",
    "            else:\n",
    "                h = self[self.highway[i]](X)\n",
    "        return h\n",
    "\n",
    "    def forward_rnn(self, X):\n",
    "        # ---------------------------------------------------------------------\n",
    "        # reset rnn state\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.reset_state()\n",
    "        # ---------------------------------------------------------------------\n",
    "        in_size, batch_size, in_dim = X.shape\n",
    "        for i in range(in_size):\n",
    "            if i > 0:\n",
    "                h_fwd = F.concat((h_fwd,\n",
    "                                  F.expand_dims(self.encode(X[i],\n",
    "                                    self.rnn_enc), 0)),\n",
    "                                  axis=0)\n",
    "                if self.m_cfg['bi_rnn']:\n",
    "                    h_rev = F.concat((h_rev,\n",
    "                                      F.expand_dims(self.encode(X[-i],\n",
    "                                        self.rnn_rev_enc), 0)),\n",
    "                                      axis=0)\n",
    "            else:\n",
    "                h_fwd = F.expand_dims(self.encode(X[i], self.rnn_enc), 0)\n",
    "                if self.m_cfg['bi_rnn']:\n",
    "                    h_rev = F.expand_dims(self.encode(X[-i], self.rnn_rev_enc), 0)\n",
    "        # ---------------------------------------------------------------------\n",
    "        if self.m_cfg['bi_rnn']:\n",
    "            h_rev = F.flipud(h_rev)\n",
    "            self.enc_states = F.concat((h_fwd, h_rev), axis=2)\n",
    "        else:\n",
    "            self.enc_states = h_fwd\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.enc_states = F.swapaxes(self.enc_states, 0, 1)\n",
    "        # ---------------------------------------------------------------------\n",
    "\n",
    "    def forward_bow_rnn(self, X, l):\n",
    "        # ---------------------------------------------------------------------\n",
    "        # reset rnn state\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.reset_state()\n",
    "        # ---------------------------------------------------------------------\n",
    "        in_size, batch_size, in_dim = X.shape\n",
    "        len_check = xp.floor(xp.array(l, dtype='i') / self.reduce_dim_len)\n",
    "        #print(X.shape)\n",
    "        #print(len_check)\n",
    "        for i in range(in_size):\n",
    "            curr_fwd_h = self.encode(X[i], self.rnn_enc)\n",
    "            if i == 0:\n",
    "                h_fwd = curr_fwd_h\n",
    "            else:\n",
    "                h_fwd = (h_fwd * (i >= len_check)[:, xp.newaxis]) + (curr_fwd_h * (i < len_check)[:, xp.newaxis])\n",
    "            #print(h_fwd.shape)\n",
    "            #print(\"h_fwd\", h_fwd[:2, :5])\n",
    "            #print(\"curr_fwd_h\", curr_fwd_h[:2, :5])\n",
    "            \n",
    "            if self.m_cfg['bi_rnn']:\n",
    "                curr_rev_h = self.encode(X[-i], self.rnn_rev_enc)\n",
    "                if i == 0:\n",
    "                    h_rev = curr_rev_h\n",
    "                else:\n",
    "                    h_rev = (h_rev * (i >= len_check)[:, xp.newaxis]) + (curr_rev_h * (i < len_check)[:, xp.newaxis])\n",
    "                #print(h_rev.shape)\n",
    "        # ---------------------------------------------------------------------\n",
    "#         self.h_final_rnn = self[self.rnn_enc[-1]].h.data\n",
    "#         if self.m_cfg['bi_rnn']:\n",
    "#             h_rev = self[self.rnn_rev_enc[-1]].h.data\n",
    "#             self.h_final_rnn = F.concat((self.h_final_rnn, h_rev), axis=1)\n",
    "        self.h_final_rnn = h_fwd\n",
    "        if self.m_cfg['bi_rnn']:\n",
    "            self.h_final_rnn = F.concat((self.h_final_rnn, h_rev), axis=1)\n",
    "\n",
    "    def forward_enc(self, X, l=None):\n",
    "        if self.m_cfg['enc_key'] != 'sp':\n",
    "            # -----------------------------------------------------------------\n",
    "            # get encoder embedding for text input\n",
    "            # -----------------------------------------------------------------\n",
    "            h = self.embed_enc(X)\n",
    "            # -----------------------------------------------------------------\n",
    "        else:\n",
    "            h = X\n",
    "        # ---------------------------------------------------------------------\n",
    "        # call cnn logic\n",
    "        # ---------------------------------------------------------------------\n",
    "        # if len(self.cnns) > 0:\n",
    "        h = self.forward_deep_cnn(h)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # call rnn logic\n",
    "        # ---------------------------------------------------------------------\n",
    "        if \"bagofwords\" not in self.m_cfg or self.m_cfg['bagofwords'] == False:\n",
    "            self.forward_rnn(h)\n",
    "        else:\n",
    "            self.forward_bow_rnn(h, l)\n",
    "        # ---------------------------------------------------------------------\n",
    "\n",
    "    def forward(self, X, add_noise=0, teacher_ratio=0, y=None):\n",
    "        # get shape\n",
    "        batch_size = X.shape[0]\n",
    "        # check whether to add noi, start=1se\n",
    "        # ---------------------------------------------------------------------\n",
    "        # check whether to add noise to speech input\n",
    "        # ---------------------------------------------------------------------\n",
    "        if add_noise > 0 and chainer.config.train:\n",
    "            # due to CUDA issues with random number generator\n",
    "            # creating a numpy array and moving to GPU\n",
    "            noise = Variable(np.random.normal(1.0,\n",
    "                                              add_noise,\n",
    "                                              size=X.shape).astype(np.float32))\n",
    "            if self.gpuid >= 0:\n",
    "                noise.to_gpu(self.gpuid)\n",
    "            X = X * noise\n",
    "        # ---------------------------------------------------------------------\n",
    "        # encode input\n",
    "        self.forward_enc(X)\n",
    "        # -----------------------------------------------------------------\n",
    "        # initialize decoder LSTM to final encoder state\n",
    "        # -----------------------------------------------------------------\n",
    "        self.set_decoder_state()\n",
    "        # -----------------------------------------------------------------\n",
    "        # swap axes of the decoder batch\n",
    "        if y is not None:\n",
    "            y = F.swapaxes(y, 0, 1)\n",
    "        # -----------------------------------------------------------------\n",
    "        # check if train or test\n",
    "        # -----------------------------------------------------------------\n",
    "        if chainer.config.train:\n",
    "            # -------------------------------------------------------------\n",
    "            # decode\n",
    "            # -------------------------------------------------------------\n",
    "            self.loss = self.decode_batch(y, teacher_ratio)\n",
    "            # -------------------------------------------------------------\n",
    "            # make return statements consistent\n",
    "            return [], self.loss\n",
    "        else:\n",
    "            # -------------------------------------------------------------\n",
    "            # predict\n",
    "            # -------------------------------------------------------------\n",
    "            # make return statements consistent\n",
    "            return(self.predict_batch(batch_size=batch_size,\n",
    "                                      pred_limit=self.m_cfg['max_en_pred'],\n",
    "                                      y=y))\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "    def forward_bow(self, X, add_noise=0, y=None, l=None):\n",
    "        # get shape\n",
    "        batch_size = X.shape[0]\n",
    "        # check whether to add noi, start=1se\n",
    "        # ---------------------------------------------------------------------\n",
    "        # check whether to add noise to speech input\n",
    "        # ---------------------------------------------------------------------\n",
    "        if add_noise > 0 and chainer.config.train:\n",
    "            # due to CUDA issues with random number generator\n",
    "            # creating a numpy array and moving to GPU\n",
    "            noise = Variable(np.random.normal(1.0,\n",
    "                                              add_noise,\n",
    "                                              size=X.shape).astype(np.float32))\n",
    "            if self.gpuid >= 0:\n",
    "                noise.to_gpu(self.gpuid)\n",
    "            X = X * noise\n",
    "        # ---------------------------------------------------------------------\n",
    "        # encode input\n",
    "        self.forward_enc(X, l)\n",
    "        # -----------------------------------------------------------------\n",
    "        # check if train or test\n",
    "        # -----------------------------------------------------------------\n",
    "        if chainer.config.train:\n",
    "            # -------------------------------------------------------------\n",
    "            # decode\n",
    "            # -------------------------------------------------------------\n",
    "            # self.loss = self.decode_bow_batch(y)\n",
    "            # -------------------------------------------------------------\n",
    "            # make return statements consistent\n",
    "            # return [], self.loss\n",
    "            return(self.decode_bow_batch(y))\n",
    "        else:\n",
    "            # -------------------------------------------------------------\n",
    "            # predict\n",
    "            # -------------------------------------------------------------\n",
    "            # make return statements consistent\n",
    "            return(self.predict_bow_batch(batch_size=batch_size,\n",
    "                                      pred_limit=self.m_cfg['max_en_pred'],\n",
    "                                      y=y))\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "    def add_gru_weight_noise(self, rnn_layer, mu, sigma):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        # W_shape = self[rnn_layer].W.W.shape\n",
    "        # b_shape = self[rnn_layer].W.b.shape\n",
    "\n",
    "        rnn_params = [\"W\", \"W_r\", \"W_z\", \"U\", \"U_r\", \"U_z\"]\n",
    "        for p in rnn_params:\n",
    "            # add noise to W\n",
    "            s_w = xp.random.normal(mu,\n",
    "                                   sigma,\n",
    "                                   self[rnn_layer][p].W.shape,\n",
    "                                   dtype=xp.float32)\n",
    "            s_b = xp.random.normal(mu,\n",
    "                                   sigma,\n",
    "                                   self[rnn_layer][p].b.shape,\n",
    "                                   dtype=xp.float32)\n",
    "            self[rnn_layer][p].W.data = self[rnn_layer][p].W.data + s_w\n",
    "            self[rnn_layer][p].b.data = self[rnn_layer][p].b.data + s_b\n",
    "\n",
    "\n",
    "    def add_lstm_weight_noise(self, rnn_layer, mu, sigma):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        # W_shape = self[rnn_layer].W.W.shape\n",
    "        # b_shape = self[rnn_layer].W.b.shape\n",
    "        rnn_params = [\"upward\", \"lateral\"]\n",
    "        for p in rnn_params:\n",
    "            # add noise to W\n",
    "            s_w = xp.random.normal(mu,\n",
    "                                   sigma,\n",
    "                                   self[rnn_layer][p].W.shape,\n",
    "                                   dtype=xp.float32)\n",
    "\n",
    "            self[rnn_layer][p].W.data = self[rnn_layer][p].W.data + s_w\n",
    "\n",
    "            if p == \"upward\":\n",
    "                s_b = xp.random.normal(mu,\n",
    "                                       sigma,\n",
    "                                       self[rnn_layer][p].b.shape,\n",
    "                                       dtype=xp.float32)\n",
    "                self[rnn_layer][p].b.data = self[rnn_layer][p].b.data + s_b\n",
    "\n",
    "    def add_weight_noise(self, mu, sigma):\n",
    "        xp = cuda.cupy if self.gpuid >= 0 else np\n",
    "        # add noise to rnn weights\n",
    "        if self.m_cfg['bi_rnn']:\n",
    "            rnn_layers = self.rnn_enc + self.rnn_rev_enc + self.rnn_dec\n",
    "        else:\n",
    "            rnn_layers = self.rnn_enc + self.rnn_dec\n",
    "\n",
    "        for rnn_layer in rnn_layers:\n",
    "            if self.m_cfg['rnn_unit'] == RNN_GRU:\n",
    "                self.add_gru_weight_noise(rnn_layer, mu, sigma)\n",
    "            else:\n",
    "                self.add_lstm_weight_noise(rnn_layer, mu, sigma)\n",
    "\n",
    "        # add noise to decoder embeddings\n",
    "        self.embed_dec.W.data = (self.embed_dec.W.data +\n",
    "                                   xp.random.normal(mu,\n",
    "                                                    sigma,\n",
    "                                                    self.embed_dec.W.shape,\n",
    "                                                    dtype=xp.float32))\n",
    "\n",
    "# In[ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bow_batch(m_dict, x_key, y_key, utt_list, vocab_dict, bow_dict,\n",
    "                  max_enc, max_dec, input_path=''):\n",
    "    batch_data = {'X':[], 't':[], 'y':[], 'r':[], 'l': []}\n",
    "    # -------------------------------------------------------------------------\n",
    "    # loop through each utterance in utt list\n",
    "    # -------------------------------------------------------------------------\n",
    "    for i, u in enumerate(utt_list):\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add X data\n",
    "        # ---------------------------------------------------------------------\n",
    "        if x_key == 'sp':\n",
    "            # -----------------------------------------------------------------\n",
    "            # for speech data\n",
    "            # -----------------------------------------------------------------\n",
    "            # get path to speech file\n",
    "            utt_sp_path = os.path.join(input_path, \"{0:s}.npy\".format(u))\n",
    "            if not os.path.exists(utt_sp_path):\n",
    "                # for training data, there are sub-folders\n",
    "                utt_sp_path = os.path.join(input_path,\n",
    "                                           u.split('_',1)[0],\n",
    "                                           \"{0:s}.npy\".format(u))\n",
    "            if os.path.exists(utt_sp_path):\n",
    "                x_data = xp.load(utt_sp_path)[:max_enc]\n",
    "            else:\n",
    "                # -------------------------------------------------------------\n",
    "                # exception if file not found\n",
    "                # -------------------------------------------------------------\n",
    "                raise FileNotFoundError(\"ERROR!! file not found: {0:s}\".format(utt_sp_path))\n",
    "                # -------------------------------------------------------------\n",
    "        else:\n",
    "            # -----------------------------------------------------------------\n",
    "            # for text data\n",
    "            # -----------------------------------------------------------------\n",
    "            x_ids = [vocab_dict[x_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][x_key]]\n",
    "            x_data = xp.asarray(x_ids, dtype=xp.int32)[:max_enc]\n",
    "            # -----------------------------------------------------------------\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add labels\n",
    "        # ---------------------------------------------------------------------\n",
    "        if type(m_dict[u][y_key]) == list:\n",
    "            en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key]])-set(range(4)))\n",
    "            r_data = [en_ids[:max_dec]]\n",
    "\n",
    "        else:\n",
    "            # dev and test data have multiple translations\n",
    "            # choose the first one for computing perplexity\n",
    "            en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key][0]])-set(range(4)))\n",
    "            r_data = []\n",
    "            for r in m_dict[u][y_key]:\n",
    "                r_list = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in r])-set(range(4)))\n",
    "                r_data.append(r_list[:max_dec])\n",
    "\n",
    "        y_ids = en_ids[:max_dec]\n",
    "        # ---------------------------------------------------------------------\n",
    "        if len(x_data) > 0:\n",
    "            #  and len(y_ids) > 0\n",
    "            batch_data['X'].append(x_data)\n",
    "            batch_data['t'].append([y_ids])\n",
    "            y_data = xp.zeros(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "            y_data[y_ids] = 1\n",
    "            y_data[list(range(4))] = -1\n",
    "            batch_data['y'].append(y_data)\n",
    "            batch_data['r'].append(r_data)\n",
    "            batch_data['l'].append(len(x_data))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # end for all utterances in batch\n",
    "    # -------------------------------------------------------------------------\n",
    "    if len(batch_data['X']) > 0 and len(batch_data['y']) > 0:\n",
    "        batch_data['X'] = F.pad_sequence(batch_data['X'], padding=PAD_ID)\n",
    "        batch_data['y'] = F.pad_sequence(batch_data['y'], padding=PAD_ID)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_model(cfg_path):\n",
    "    # -------------------------------------------------------------------------\n",
    "    # read config files model\n",
    "    # -------------------------------------------------------------------------\n",
    "    with open(os.path.join(cfg_path, \"model_cfg.json\"), \"r\") as model_f:\n",
    "        m_cfg = json.load(model_f)\n",
    "    # -------------------------------------------------------------------------\n",
    "    with open(os.path.join(cfg_path, \"train_cfg.json\"), \"r\") as train_f:\n",
    "        t_cfg = json.load(train_f)\n",
    "    xp = cuda.cupy if t_cfg['gpuid'] >= 0 else np\n",
    "    # -------------------------------------------------------------------------\n",
    "    # check model path\n",
    "    # -------------------------------------------------------------------------\n",
    "    if not os.path.exists(m_cfg['data_path']):\n",
    "        raise FileNotFoundError(\"ERROR!! file not found: {0:s}\".format(m_cfg['data_path']))\n",
    "    # end if\n",
    "    # -------------------------------------------------------------------------\n",
    "    # initialize new model\n",
    "    # -------------------------------------------------------------------------\n",
    "    model = SpeechEncoderDecoder(m_cfg, t_cfg['gpuid'])\n",
    "    model.to_gpu(t_cfg['gpuid'])\n",
    "    # -------------------------------------------------------------------------\n",
    "    # set up optimizer\n",
    "    # -------------------------------------------------------------------------\n",
    "    if t_cfg['optimizer'] == OPT_ADAM:\n",
    "        print(\"using ADAM optimizer\")\n",
    "        optimizer = optimizers.Adam(alpha=t_cfg['lr'],\n",
    "                                    beta1=0.9,\n",
    "                                    beta2=0.999,\n",
    "                                    eps=1e-08)\n",
    "    else:\n",
    "        print(\"using SGD optimizer\")\n",
    "        optimizer = optimizers.SGD(lr=t_cfg['lr'])\n",
    "\n",
    "    # attach optimizer\n",
    "    optimizer.setup(model)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # optimizer settings\n",
    "    # -------------------------------------------------------------------------\n",
    "    if m_cfg['l2'] > 0:\n",
    "        optimizer.add_hook(chainer.optimizer.WeightDecay(m_cfg['l2']))\n",
    "\n",
    "    # gradient clipping\n",
    "    optimizer.add_hook(chainer.optimizer.GradientClipping(threshold=m_cfg['grad_clip']))\n",
    "\n",
    "    # gradient noise\n",
    "    if t_cfg['grad_noise_eta'] > 0:\n",
    "        print(\"------ Adding gradient noise\")\n",
    "        optimizer.add_hook(chainer.optimizer.GradientNoise(eta=t_cfg['grad_noise_eta']))\n",
    "        print(\"Finished adding gradient noise\")\n",
    "    # -------------------------------------------------------------------------\n",
    "    # check last saved model\n",
    "    # -------------------------------------------------------------------------\n",
    "    max_epoch = 0\n",
    "    # -------------------------------------------------------------------------\n",
    "    # add debug info\n",
    "    # -------------------------------------------------------------------------\n",
    "    m_cfg['model_dir'] = cfg_path\n",
    "    m_cfg['train_log'] = os.path.join(m_cfg['model_dir'], \"train.log\")\n",
    "    m_cfg['dev_log'] = os.path.join(m_cfg['model_dir'], \"dev.log\")\n",
    "    m_cfg['model_fname'] = os.path.join(m_cfg['model_dir'], \"seq2seq.model\")\n",
    "    m_cfg['opt_fname'] = os.path.join(m_cfg['model_dir'], \"train.opt\")\n",
    "    # -------------------------------------------------------------------------\n",
    "    model_fil = m_cfg['model_fname']\n",
    "    model_files = [f for f in os.listdir(os.path.dirname(model_fil))\n",
    "                   if os.path.basename(model_fil).replace('.model','') in f]\n",
    "    if len(model_files) > 0:\n",
    "        print(\"-\"*80)\n",
    "        max_model_fil = max(model_files, key=lambda s: int(s.split('_')[-1].split('.')[0]))\n",
    "        max_model_fil = os.path.join(os.path.dirname(model_fil),\n",
    "                                     max_model_fil)\n",
    "        print('model found = \\n{0:s}'.format(max_model_fil))\n",
    "        serializers.load_npz(max_model_fil, model)\n",
    "        print(\"finished loading ..\")\n",
    "        max_epoch = int(max_model_fil.split('_')[-1].split('.')[0])\n",
    "        # load optimizer\n",
    "        if os.path.exists(m_cfg['opt_fname']):\n",
    "            print(\"optimizer found = {0:s}\".format(m_cfg['opt_fname']))\n",
    "            serializers.load_npz(m_cfg['opt_fname'], optimizer)\n",
    "            print(\"finished loading optimizer ...\")\n",
    "        else:\n",
    "            print(\"optimizer not found\")\n",
    "    else:\n",
    "        print(\"-\"*80)\n",
    "        print('model not found')\n",
    "    # end if model found\n",
    "    # -------------------------------------------------------------------------\n",
    "    return max_epoch, model, optimizer, m_cfg, t_cfg\n",
    "# end check_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_dicts(m_cfg):\n",
    "    print(\"-\"*50)\n",
    "    # load dictionaries\n",
    "    # -------------------------------------------------------------------------\n",
    "    # MAP dict\n",
    "    # -------------------------------------------------------------------------\n",
    "    map_dict_path = os.path.join(m_cfg['data_path'],'map.dict')\n",
    "    print(\"loading dict: {0:s}\".format(map_dict_path))\n",
    "    map_dict = pickle.load(open(map_dict_path, \"rb\"))\n",
    "    # -------------------------------------------------------------------------\n",
    "    # VOCAB\n",
    "    # -------------------------------------------------------------------------\n",
    "    if 'fisher' in m_cfg['train_set']:\n",
    "        if m_cfg['stemmify'] == False:\n",
    "            vocab_path = os.path.join(m_cfg['data_path'], 'train_vocab.dict')\n",
    "        else:\n",
    "            vocab_path = os.path.join(m_cfg['data_path'], 'train_stemmed_vocab.dict')\n",
    "    else:\n",
    "        vocab_path = os.path.join(m_cfg['data_path'], 'ch_train_vocab.dict')\n",
    "    print(\"loading dict: {0:s}\".format(vocab_path))\n",
    "    vocab_dict = pickle.load(open(vocab_path, \"rb\"))\n",
    "    print(\"-\"*50)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # BUCKETS\n",
    "    # -------------------------------------------------------------------------\n",
    "    prep_buckets.buckets_main(m_cfg['data_path'],\n",
    "                              m_cfg['buckets_num'],\n",
    "                              m_cfg['buckets_width'],\n",
    "                              m_cfg['enc_key'],\n",
    "                              scale=m_cfg['train_scale'],\n",
    "                              seed=m_cfg['seed'])\n",
    "\n",
    "    buckets_path = os.path.join(m_cfg['data_path'],\n",
    "                                'buckets_{0:s}.dict'.format(m_cfg['enc_key']))\n",
    "    print(\"loading dict: {0:s}\".format(buckets_path))\n",
    "    bucket_dict = pickle.load(open(buckets_path, \"rb\"))\n",
    "    print(\"-\"*50)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # bag-of-words\n",
    "    # -------------------------------------------------------------------------\n",
    "    bow_dict_path = os.path.join(m_cfg['data_path'],\n",
    "                                     'train_top_K_enw.dict')\n",
    "    print(\"loading dict: {0:s}\".format(bow_dict_path))\n",
    "    bow_dict = pickle.load(open(bow_dict_path, \"rb\"))\n",
    "    print(\"-\"*50)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # INFORMATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    for cat in map_dict:\n",
    "        print('utterances in {0:s} = {1:d}'.format(cat, len(map_dict[cat])))\n",
    "\n",
    "    if m_cfg['enc_key'] != 'sp':\n",
    "        vocab_size_es = len(vocab_dict[m_cfg['enc_key']]['w2i'])\n",
    "    else:\n",
    "        vocab_size_es = 0\n",
    "    vocab_size_en = len(vocab_dict[m_cfg['dec_key']]['w2i'])\n",
    "    print('vocab size for {0:s} = {1:d}'.format(m_cfg['enc_key'],\n",
    "                                                vocab_size_es))\n",
    "    print('vocab size for {0:s} = {1:d}'.format(m_cfg['dec_key'],\n",
    "                                                vocab_size_en))\n",
    "    # -------------------------------------------------------------------------\n",
    "    return map_dict, vocab_dict, bucket_dict, bow_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_model(model, optimizer, m_dict, b_dict,\n",
    "               batch_size, vocab_dict, bow_dict, x_key, y_key,\n",
    "               train, input_path, max_dec, t_cfg, use_y=True):\n",
    "    # number of buckets\n",
    "    num_b = b_dict['num_b']\n",
    "    width_b = b_dict['width_b']\n",
    "    utts = {\"ids\": [], \"preds\": [], \"probs\": [], \"refs\": []}\n",
    "\n",
    "    total_loss = 0\n",
    "    loss_per_epoch = 0\n",
    "    total_loss_updates= 0\n",
    "\n",
    "    sys.stderr.flush()\n",
    "    # -------------------------------------------------------------------------\n",
    "    # create batches of utterances - shuffled\n",
    "    # -------------------------------------------------------------------------\n",
    "    utt_list_batches, total_utts = create_batches(b_dict, batch_size)\n",
    "    # -------------------------------------------------------------------------\n",
    "    with tqdm(total=total_utts, ncols=80) as pbar:\n",
    "        for i, (utt_list, b) in enumerate(utt_list_batches):\n",
    "            # -----------------------------------------------------------------\n",
    "            # get batch_data\n",
    "            # -----------------------------------------------------------------\n",
    "            batch_data = get_bow_batch(m_dict,\n",
    "                                   x_key, y_key,\n",
    "                                   utt_list,\n",
    "                                   vocab_dict,\n",
    "                                   bow_dict,\n",
    "                                   ((b+1) * width_b),\n",
    "                                   max_dec,\n",
    "                                   input_path=input_path)\n",
    "            # -----------------------------------------------------------------\n",
    "            if (len(batch_data['X']) > 0 and len(batch_data['y']) > 0):\n",
    "                if use_y:\n",
    "                    # ---------------------------------------------------------\n",
    "                    # using labels, computing loss\n",
    "                    # also used for dev set\n",
    "                    # ---------------------------------------------------------\n",
    "                    with chainer.using_config('train', train):\n",
    "                        cuda.get_device(t_cfg['gpuid']).use()\n",
    "                        p_words, loss, p_probs = model.forward_bow(X=batch_data['X'],\n",
    "                                                                   y=batch_data['y'],\n",
    "                                                                   add_noise=t_cfg['speech_noise'],\n",
    "                                                                   l=l)\n",
    "                        loss_val = float(loss.data)\n",
    "                else:\n",
    "                    # ---------------------------------------------------------\n",
    "                    # prediction only\n",
    "                    # ---------------------------------------------------------\n",
    "                    with chainer.using_config('train', False):\n",
    "                        cuda.get_device(t_cfg['gpuid']).use()\n",
    "                        p_words, _, p_probs = model.forward_bow(X=batch_data['X'], l=l)\n",
    "                        loss_val = 0.0\n",
    "                # -------------------------------------------------------------\n",
    "                # add list of utterances used\n",
    "                # -------------------------------------------------------------\n",
    "                for u, pred, prob, ref in zip(utt_list, p_words, p_probs, batch_data['r']):\n",
    "                    utts['ids'].append(u)\n",
    "                    utts[\"preds\"].append(pred)\n",
    "                    utts[\"probs\"].append(prob)\n",
    "                    utts[\"refs\"].append(ref)\n",
    "                # utts.extend(utt_list)\n",
    "                # -------------------------------------------------------------\n",
    "                # if len(p) > 0:\n",
    "                #     pred_sents.extend(p)\n",
    "                #     refs.extend(batch_data['t'])\n",
    "\n",
    "                total_loss += loss_val\n",
    "                total_loss_updates += 1\n",
    "                loss_per_epoch = (total_loss / total_loss_updates)\n",
    "\n",
    "                out_str = \"b={0:d},l={1:.2f},avg={2:.2f}\".format((b+1),loss_val,loss_per_epoch)\n",
    "                # -------------------------------------------------------------\n",
    "                # train mode logic\n",
    "                # -------------------------------------------------------------\n",
    "                if train:\n",
    "                    # ---------------------------------------------------------\n",
    "                    model.cleargrads()\n",
    "                    loss.backward()\n",
    "                    optimizer.update()\n",
    "                    # ---------------------------------------------------------\n",
    "                pbar.set_description('{0:s}'.format(out_str))\n",
    "            else:\n",
    "                print(\"no data in batch\")\n",
    "                print(utt_list)\n",
    "            # update progress bar\n",
    "            pbar.update(len(utt_list))\n",
    "        # end for batches\n",
    "    # end tqdm\n",
    "    # return pred_sents, utts, refs, loss_per_epoch\n",
    "    utts[\"probs\"] = F.pad_sequence(utts[\"probs\"]).data\n",
    "    return utts, loss_per_epoch\n",
    "# end feed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg_path = \"./sp2bagwords/sp_1.0_h-256_e-128_rnn-2_hwy-2_cnn-32-2-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_out_dim = rnn_in_units =  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sameer/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/utils/experimental.py:104: FutureWarning: chainer.links.normalization.layer_normalization.py is experimental. The interface can change in the future.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ADAM optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "model not found\n"
     ]
    }
   ],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp = cuda.cupy if t_cfg['gpuid'] >= 0 else np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict, bow_dict = get_data_dicts(m_cfg)\n",
    "batch_size = {'max': 96, 'med': 128, 'min': 256, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "print(\"-\"*50)\n",
    "prep_buckets.display_buckets(bucket_dict, \"fisher_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bucket_dict['fisher_train']['buckets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "curr_bucket = 8\n",
    "num_utts = 50\n",
    "utt_list = bucket_dict['fisher_train']['buckets'][curr_bucket][:num_utts]\n",
    "curr_set='fisher_train'\n",
    "print(len(utt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6*10 / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if \"train\" in curr_set:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "    play_audio = False\n",
    "    width_b = bucket_dict[train_key][\"width_b\"]\n",
    "    num_b = bucket_dict[dev_key][\"num_b\"]\n",
    "else:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "    play_audio = True\n",
    "    width_b = bucket_dict[train_key][\"width_b\"]\n",
    "    num_b = bucket_dict[dev_key][\"num_b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width_b, num_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_bow_batch(map_dict[curr_set], \n",
    "                                enc_key,\n",
    "                                dec_key,\n",
    "                                utt_list,\n",
    "                                vocab_dict,\n",
    "                                bow_dict,\n",
    "                                (curr_bucket+1) * width_b,\n",
    "                                200,\n",
    "                                input_path=local_input_path)\n",
    "\n",
    "X, y, t, l = batch_data['X'], batch_data['y'], batch_data['t'], batch_data['l']\n",
    "\n",
    "batch_size = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1634, 80), [1623, 1634, 1616])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l[0] = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reduce_dim_len, np.floor(np.array(l, dtype='i') / model.reduce_dim_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_check = np.floor(np.array(l, dtype='i') / model.reduce_dim_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.ones(10, dtype='f')) * (i < np.array(l, dtype='i')), (np.array(l, dtype='i') >= i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.ones(10, dtype='f')) * (i < np.floor(np.array(l, dtype='i') / model.reduce_dim_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i >= len_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.ones(10, dtype='f')) * (i < np.floor(np.array(l, dtype='i') / model.reduce_dim_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.ones((10,2), dtype='f') * (i < len_check)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward_enc(X, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.h_final_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.h_final_rnn[:2, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    pred_words = []\n",
    "    batch_data = get_bow_batch(map_dict[curr_set], \n",
    "                                enc_key,\n",
    "                                dec_key,\n",
    "                                utt_list,\n",
    "                                vocab_dict,\n",
    "                                bow_dict,\n",
    "                                (curr_bucket+1) * m_cfg[\"buckets_width\"],\n",
    "                                200,\n",
    "                                input_path=local_input_path)\n",
    "\n",
    "    X, y, t, l = batch_data['X'], batch_data['y'], batch_data['t'], batch_data['l']\n",
    "\n",
    "    batch_size = X.shape[0]\n",
    "    # encode input\n",
    "    model.forward_enc(X, l)\n",
    "    \n",
    "    if m_cfg['highway_layers'] > 0:\n",
    "        highway_h = model.forward_highway(model.h_final_rnn)\n",
    "\n",
    "    predicted_out = model.out(highway_h)\n",
    "    \n",
    "    simple_loss = F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"mean\")\n",
    "    loss = F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"no\")\n",
    "    \n",
    "    loss_weights = xp.ones(shape=y.data.shape, dtype=\"f\")\n",
    "    loss_weights[y.data < 0] = 0\n",
    "    loss_weights[y.data == 0] = 1\n",
    "    loss_weights[y.data > 0] = 10\n",
    "    #loss_avg = F.average(F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce='no'), weights=loss_weights)\n",
    "    loss_avg = F.mean(loss_weights * loss)\n",
    "    print(i, \"---\".join([\"{0:.3f}\".format(float(val)) for val in (loss_avg.data, xp.mean(loss.data), simple_loss.data)]))\n",
    "    model.cleargrads()\n",
    "    loss_avg.backward()\n",
    "    optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10755518078804016\n",
      "0.09179939329624176\n",
      "0.10357362776994705\n",
      "0.08566057682037354\n",
      "0.07195165008306503\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    p_words, loss, p_probs = model.forward_bow(X=batch_data['X'],\n",
    "                                                y=batch_data['y'],\n",
    "                                                add_noise=t_cfg['speech_noise'],\n",
    "                                              l=batch_data['l'])\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    loss_val = float(loss.data)\n",
    "    print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
