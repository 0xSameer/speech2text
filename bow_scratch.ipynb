{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bow_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unpadded_batch(m_dict, x_key, y_key, utt_list, vocab_dict, bow_dict, max_enc, max_dec, input_path='', set_zero_num=500):\n",
    "    batch_data = {'X':[], 't':[], 'y':[]}\n",
    "    # -------------------------------------------------------------------------\n",
    "    # loop through each utterance in utt list\n",
    "    # -------------------------------------------------------------------------\n",
    "    for i, u in enumerate(utt_list):\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add X data\n",
    "        # ---------------------------------------------------------------------\n",
    "        if x_key == 'sp':\n",
    "            # -----------------------------------------------------------------\n",
    "            # for speech data\n",
    "            # -----------------------------------------------------------------\n",
    "            # get path to speech file\n",
    "            utt_sp_path = os.path.join(input_path, \"{0:s}.npy\".format(u))\n",
    "            if not os.path.exists(utt_sp_path):\n",
    "                # for training data, there are sub-folders\n",
    "                utt_sp_path = os.path.join(input_path,\n",
    "                                           u.split('_',1)[0],\n",
    "                                           \"{0:s}.npy\".format(u))\n",
    "            if os.path.exists(utt_sp_path):\n",
    "                x_data = Variable(xp.load(utt_sp_path)[:max_enc])\n",
    "            else:\n",
    "                # -------------------------------------------------------------\n",
    "                # exception if file not found\n",
    "                # -------------------------------------------------------------\n",
    "                raise FileNotFoundError(\"ERROR!! file not found: {0:s}\".format(utt_sp_path))\n",
    "                # -------------------------------------------------------------\n",
    "        else:\n",
    "            # -----------------------------------------------------------------\n",
    "            # for text data\n",
    "            # -----------------------------------------------------------------\n",
    "            x_ids = [vocab_dict[x_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][x_key]]\n",
    "            x_ids = xp.asarray(x_ids, dtype=xp.int32)\n",
    "            batch_data['X'].append(x_ids[:max_enc])\n",
    "            # -----------------------------------------------------------------\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add labels\n",
    "        # ---------------------------------------------------------------------\n",
    "        if type(m_dict[u][y_key]) == list:\n",
    "            en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key]])-set(range(4)))\n",
    "        else:\n",
    "            # dev and test data have multiple translations\n",
    "            # choose the first one for computing perplexity\n",
    "            en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key][0]])-set(range(4)))\n",
    "        y_ids = en_ids[:max_dec]\n",
    "        # ---------------------------------------------------------------------\n",
    "        if len(x_data) > 0 and len(y_ids) > 0:\n",
    "            batch_data['X'].append(x_data)\n",
    "            batch_data['t'].append([y_ids])\n",
    "            y_data = xp.zeros(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "            #y_data = -1 * xp.ones(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "            #set_some_to_0 = np.random.choice(np.array(range(len(bow_dict['w2i'])), dtype=\"i\"), \n",
    "            #                                 size=set_zero_num, \n",
    "            #                                 replace=False)\n",
    "            #y_data[set_some_to_0] = 0\n",
    "            y_data[y_ids] = 1\n",
    "            y_data[list(range(4))] = -1\n",
    "            batch_data['y'].append(y_data)\n",
    "            \n",
    "    # -------------------------------------------------------------------------\n",
    "    # end for all utterances in batch\n",
    "    # -------------------------------------------------------------------------\n",
    "#     if len(batch_data['X']) > 0 and len(batch_data['y']) > 0:\n",
    "#         batch_data['X'] = F.pad_sequence(batch_data['X'], padding=PAD_ID)\n",
    "#         batch_data['y'] = F.pad_sequence(batch_data['y'], padding=PAD_ID)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hmm_get_curr_bow_batch(m_dict, x_key, y_key, utt_list, vocab_dict, bow_dict, max_enc, max_dec, input_path='', set_zero_num=500):\n",
    "#     batch_data = {'X':[], 't':[], 'y':[]}\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     # loop through each utterance in utt list\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     for i, u in enumerate(utt_list):\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         #  add X data\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         if x_key == 'sp':\n",
    "#             # -----------------------------------------------------------------\n",
    "#             # for speech data\n",
    "#             # -----------------------------------------------------------------\n",
    "#             # get path to speech file\n",
    "#             utt_sp_path = os.path.join(input_path, \"{0:s}.npy\".format(u))\n",
    "#             if not os.path.exists(utt_sp_path):\n",
    "#                 # for training data, there are sub-folders\n",
    "#                 utt_sp_path = os.path.join(input_path,\n",
    "#                                            u.split('_',1)[0],\n",
    "#                                            \"{0:s}.npy\".format(u))\n",
    "#             if os.path.exists(utt_sp_path):\n",
    "#                 x_data = xp.load(utt_sp_path)[:max_enc]\n",
    "#             else:\n",
    "#                 # -------------------------------------------------------------\n",
    "#                 # exception if file not found\n",
    "#                 # -------------------------------------------------------------\n",
    "#                 raise FileNotFoundError(\"ERROR!! file not found: {0:s}\".format(utt_sp_path))\n",
    "#                 # -------------------------------------------------------------\n",
    "#         else:\n",
    "#             # -----------------------------------------------------------------\n",
    "#             # for text data\n",
    "#             # -----------------------------------------------------------------\n",
    "#             x_ids = [vocab_dict[x_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][x_key]]\n",
    "#             x_ids = xp.asarray(x_ids, dtype=xp.int32)\n",
    "#             batch_data['X'].append(x_ids[:max_enc])\n",
    "#             # -----------------------------------------------------------------\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         #  add labels\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         if type(m_dict[u][y_key]) == list:\n",
    "#             en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key]])-set(range(4)))\n",
    "#         else:\n",
    "#             # dev and test data have multiple translations\n",
    "#             # choose the first one for computing perplexity\n",
    "#             en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key][0]])-set(range(4)))\n",
    "#         y_ids = en_ids[:max_dec]\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         if len(x_data) > 0 and len(y_ids) > 0:\n",
    "#             batch_data['X'].append(x_data)\n",
    "#             batch_data['t'].append([y_ids])\n",
    "#             y_data = xp.zeros(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "#             #y_data = -1 * xp.ones(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "#             #set_some_to_0 = np.random.choice(np.array(range(len(bow_dict['w2i'])), dtype=\"i\"), \n",
    "#             #                                 size=set_zero_num, \n",
    "#             #                                 replace=False)\n",
    "#             #y_data[set_some_to_0] = 0\n",
    "#             y_data[y_ids] = 1\n",
    "#             y_data[list(range(4))] = -1\n",
    "#             batch_data['y'].append(y_data)\n",
    "            \n",
    "#     # -------------------------------------------------------------------------\n",
    "#     # end for all utterances in batch\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     if len(batch_data['X']) > 0 and len(batch_data['y']) > 0:\n",
    "#         batch_data['X'] = F.pad_sequence(batch_data['X'], padding=PAD_ID)\n",
    "#         batch_data['y'] = F.pad_sequence(batch_data['y'], padding=PAD_ID)\n",
    "#     return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"./sp2bagwords/sp_1.0_h-256_e-128_rnn-2_hwy-2_cnn-32-2-5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_out_dim = rnn_in_units =  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/project/lowres/work/miniconda3/envs/chainer3/lib/python3.6/site-packages/chainer/utils/experimental.py:104: FutureWarning: chainer.links.normalization.layer_normalization.py is experimental. The interface can change in the future.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ADAM optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "model found = \n",
      "./sp2bagwords/sp_1.0_h-256_e-128_rnn-2_hwy-2_cnn-32-2-5/seq2seq_20.model\n",
      "finished loading ..\n",
      "optimizer found = ./sp2bagwords/sp_1.0_h-256_e-128_rnn-2_hwy-2_cnn-32-2-5/train.opt\n",
      "finished loading optimizer ...\n"
     ]
    }
   ],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_dict_path = os.path.join(m_cfg['data_path'],\n",
    "#                                      'train_top_K_enw.dict')\n",
    "# if os.path.exists(bow_dict_path):\n",
    "#     bow_dict = pickle.load(open(bow_dict_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict, bow_dict = get_data_dicts(m_cfg)\n",
    "batch_size = {'max': 96, 'med': 128, 'min': 256, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27715, 1497356, 0.018509292379367364)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bow_dict['freq'].values()), sum(vocab_dict['en_w']['freq'].values()), sum(bow_dict['freq'].values()) / sum(vocab_dict['en_w']['freq'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bow_dict['i2w'].keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, m_cfg['max_en_pred']\n",
    "# min_len, max_len = 0, 10\n",
    "displayN = 50\n",
    "m_dict=map_dict[dev_key]\n",
    "# wavs_path = os.path.join(m_cfg['data_path'], \"wavs\")\n",
    "wavs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"wavs\")\n",
    "v_dict = vocab_dict['en_w']\n",
    "key = m_cfg['dev_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20051009_182032_217_fsp-B-1']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_dict['fisher_dev']['buckets'][0][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_dict['fisher_dev']['num_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_bucket = 3\n",
    "num_utts = 30\n",
    "utt_list = bucket_dict['fisher_train']['buckets'][curr_bucket][:num_utts]\n",
    "curr_set='fisher_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"train\" in curr_set:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "    play_audio = False\n",
    "else:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "    play_audio = True\n",
    "\n",
    "width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "\n",
    "batch_data = get_bow_batch(map_dict[curr_set], \n",
    "                                enc_key,\n",
    "                                dec_key,\n",
    "                                utt_list,\n",
    "                                vocab_dict,\n",
    "                                bow_dict,\n",
    "                                (curr_bucket+1) * width_b,\n",
    "                                200,\n",
    "                                input_path=local_input_path)\n",
    "\n",
    "X, y, t = batch_data['X'], batch_data['y'], batch_data['t']\n",
    "\n",
    "batch_size = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[17]],\n",
       " [[17]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[93]],\n",
       " [[25]],\n",
       " [[93]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[11]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[51]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[]],\n",
       " [[58]],\n",
       " [[]],\n",
       " [[12]],\n",
       " [[40, 4]],\n",
       " [[]]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-4.809871196746826, dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.min(batch_data['X'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"train\" in curr_set:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "    play_audio = False\n",
    "else:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "    play_audio = True\n",
    "\n",
    "width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "\n",
    "batch_data = get_bow_batch(map_dict[curr_set], \n",
    "                                enc_key,\n",
    "                                dec_key,\n",
    "                                utt_list,\n",
    "                                vocab_dict,\n",
    "                                bow_dict,\n",
    "                                (curr_bucket+1) * width_b,\n",
    "                                200,\n",
    "                                input_path=local_input_path)\n",
    "\n",
    "X, y, t, l = batch_data['X'], batch_data['y'], batch_data['t'], batch_data['l']\n",
    "\n",
    "# batch_size = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 799, 80) (30, 104)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.hyperparam.alpha = 0.001\n",
    "optimizer.hyperparam.lr = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_out[0,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06361255049705505\n",
      "0.06104724854230881\n",
      "0.056551627814769745\n",
      "0.05156370624899864\n",
      "0.04425334557890892\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    p_words, loss, p_probs = model.forward_bow(X=batch_data['X'],\n",
    "                                                y=batch_data['y'],\n",
    "                                                add_noise=t_cfg['speech_noise'],\n",
    "                                                l=batch_data['l'])\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "    optimizer.update()\n",
    "    loss_val = float(loss.data)\n",
    "    print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    pred_words = []\n",
    "    batch_data = get_bow_batch(map_dict[curr_set], \n",
    "                                enc_key,\n",
    "                                dec_key,\n",
    "                                utt_list,\n",
    "                                vocab_dict,\n",
    "                                bow_dict,\n",
    "                                (curr_bucket+1) * width_b,\n",
    "                                200,\n",
    "                                input_path=local_input_path)\n",
    "\n",
    "    X, y, t, l  = batch_data['X'], batch_data['y'], batch_data['t'], batch_data['l']\n",
    "\n",
    "    batch_size = X.shape[0]\n",
    "    # encode input\n",
    "    model.forward_enc(X)\n",
    "    \n",
    "    if m_cfg['highway_layers'] > 0:\n",
    "        highway_h = model.forward_highway(model.h_final_rnn)\n",
    "\n",
    "    predicted_out = model.out(highway_h)\n",
    "    \n",
    "    simple_loss = F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"mean\")\n",
    "    loss = F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"no\")\n",
    "    \n",
    "    loss_weights = xp.ones(shape=y.data.shape, dtype=\"f\")\n",
    "    loss_weights[y.data < 0] = 0\n",
    "    loss_weights[y.data == 0] = 1\n",
    "    loss_weights[y.data > 0] = 10\n",
    "    #loss_avg = F.average(F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce='no'), weights=loss_weights)\n",
    "    loss_avg = F.mean(loss_weights * loss)\n",
    "    print(i, \"---\".join([\"{0:.3f}\".format(float(val)) for val in (loss_avg.data, xp.mean(loss.data), simple_loss.data)]))\n",
    "    model.cleargrads()\n",
    "    loss_avg.backward()\n",
    "    optimizer.update()\n",
    "#     for row in predicted_out.data[:5]:\n",
    "#         print(xp.where(row > 0.6)[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "for i, row in enumerate(predicted_out.data):\n",
    "    print(row[10:12])\n",
    "    pred_inds = xp.where(row >= PRED_THRESH)[0]\n",
    "    if len(pred_inds) > 20:\n",
    "        pred_inds = xp.argsort(row)[-20:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    curr_row = [i for i in pred_inds.tolist() if i > 3]\n",
    "#     if i < 5:\n",
    "#         print(curr_row)\n",
    "    pred_words.append(curr_row)\n",
    "    probs.append(row)\n",
    "# print(pred_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.pad_sequence(probs).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[:,10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "for row, pred, ttt in zip(y.data[:N], predicted_out.data[:N], batch_data['t']):\n",
    "    #print(xp.where(row == 1))\n",
    "    print(ttt[0])\n",
    "    print(xp.where(pred >= PRED_THRESH)[0].tolist())\n",
    "    print(set(xp.where(pred >= PRED_THRESH)[0].tolist()) & set(ttt[0]))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(batch_data['t'], pred_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_accuracy(predicted_out.data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data['y'][:,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weight = xp.ones(shape=y.data.shape, dtype=\"f\")\n",
    "loss_weight[y.data < 0] = 0\n",
    "loss_weight[y.data == 0] = 10\n",
    "loss_weight[y.data > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.data[0][:20], loss_weight[0,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_full = F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_full.shape, loss_weight.shape, loss_full[0,:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.average(loss_full, weights=loss_weight), F.mean(loss_full), F.mean((loss_full*loss_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.average(F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"no\").data), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"mean\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_h = model.out_1(model.h_final_rnn)\n",
    "predicted_out = model.out_2(F.sigmoid(out_h))\n",
    "loss = F.sigmoid_cross_entropy(predicted_out, y, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cleargrads()\n",
    "loss.backward()\n",
    "optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_out.data[0,9:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xp.where(predicted_out.data > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.where(y[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[vocab_dict['en_w']['i2w'][i] for i in t[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.h_final_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.h_final_rnn[-1,:10], model.h_final_rnn[-1,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.L1_enc.h[-1,:10], model.L1_rev_enc.h[-1][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fwd = model.L1_enc.h.data\n",
    "h_rev = model.L1_rev_enc.h.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_fwd[-1,:10], h_rev[-1,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.concat((h_fwd, h_rev), axis=1)[-1][:10], F.concat((h_fwd, h_rev), axis=1)[-1][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pata = model[model.rnn_enc[-1]].h.data\n",
    "if model.m_cfg['bi_rnn']:\n",
    "    h_rev = model[model.rnn_rev_enc[-1]].h.data\n",
    "    pata = F.concat((pata, h_rev), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pata[-1][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = F.sigmoid(model.out_1(model.h_final_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y = xp.ones(y.shape, dtype='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hu = model.out_2(haha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(hu, dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.sigmoid_cross_entropy(hu, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cleargrads()\n",
    "loss.backward()\n",
    "optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha = F.sigmoid(model.out_2(haha)).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[expit(i) for i in hu[0][:10].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha[0].data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.where(ha[0].data >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.sigmoid_cross_entropy(model.out_2(haha), y, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0][:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data['y'][0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0.1, 0.7, 0.2], # prediction label is 1\n",
    "              [8.0, 1.0, 2.0], # prediction label is 0\n",
    "              [-8.0, 1.0, 2.0], # prediction label is 2\n",
    "              [-8.0, -1.0, -2.0]]) # prediction label is 1\n",
    "t = np.array([1, 0, 2, 1], 'i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.accuracy(y, t).data # 100% accuracy because all samples are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([1, 0, 0, 0], 'i')\n",
    "F.accuracy(y, t).data # 100% accuracy because all samples are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha.shape, batch_data['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_accuracy(ha.data, batch_data['y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
