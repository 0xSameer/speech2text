{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bow_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_utt(utt, m_dict):\n",
    "    sr, y = scipy.io.wavfile.read(os.path.join(wavs_path, utt.rsplit(\"-\",1)[0]+'.wav'))\n",
    "    start_t = min(seg['start'] for seg in m_dict[utt]['seg'])\n",
    "    end_t = max(seg['end'] for seg in m_dict[utt]['seg'])\n",
    "    print(start_t, end_t)\n",
    "    start_t_samples, end_t_samples = int(start_t*sr), int(end_t*sr)\n",
    "    display(Audio(y[start_t_samples:end_t_samples], rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_words(m_dict, v_dict, preds, utts, dec_key, key, play_audio=False, displayN=-1):\n",
    "    if displayN == -1:\n",
    "        displayN = len(utts)\n",
    "    es_ref = []\n",
    "    en_ref = []\n",
    "    google_ref = []\n",
    "    google_pred = []\n",
    "    for u in utts:\n",
    "        es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "        if type(m_dict[u][dec_key]) == list:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "        google_pred.append(\" \".join(google_hyp_r0[u]))\n",
    "        google_ref.append(\" \".join(google_dev_ref_0[u]))\n",
    "\n",
    "    en_pred = []\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    for p in preds:\n",
    "        if type(p) == list:\n",
    "            t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "            t_str = t_str[:t_str.find('_EOS')]\n",
    "            en_pred.append(t_str)\n",
    "        else:\n",
    "            en_pred.append(\"\")\n",
    "        \n",
    "\n",
    "    for u, es, en, p, g, gr in sorted(list(zip(utts, es_ref, en_ref, en_pred, google_pred, google_ref)))[:displayN]:\n",
    "        # for reference, 1st word is GO_ID, no need to display\n",
    "        print(\"Utterance: {0:s}\".format(u))\n",
    "        display_pp = PrettyTable([\"cat\",\"sent\"], hrules=True)\n",
    "        display_pp.align = \"l\"\n",
    "        display_pp.header = False\n",
    "        display_pp.add_row([\"es ref\", textwrap.fill(es,50)])\n",
    "        display_pp.add_row([\"en ref\", textwrap.fill(en,50)])\n",
    "        display_pp.add_row([\"model pred\", textwrap.fill(p,50)])\n",
    "        display_pp.add_row([\"model bleu\", \"{0:.2f}\".format(sentence_bleu([en], p, smoothing_function=smooth_fun.method2))])\n",
    "        display_pp.add_row([\"google pred\", textwrap.fill(g,50)])\n",
    "        display_pp.add_row([\"google bleu\", \"{0:.2f}\".format(sentence_bleu([gr], g, smoothing_function=smooth_fun.method2))])\n",
    "    \n",
    "\n",
    "        print(display_pp)\n",
    "        if play_audio:\n",
    "            play_utt(u, m_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pred(utt, X, y=None, display_limit=10):\n",
    "    # get shape\n",
    "    batch_size = X.shape[0]\n",
    "    # encode input\n",
    "    model.forward_enc(X)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # initialize decoder LSTM to final encoder state\n",
    "    # ---------------------------------------------------------------------\n",
    "    model.set_decoder_state()\n",
    "    # ---------------------------------------------------------------------\n",
    "    # swap axes of the decoder batch\n",
    "    if y is not None:\n",
    "        y = F.swapaxes(y, 0, 1)\n",
    "    # -----------------------------------------------------------------\n",
    "    # predict\n",
    "    # -----------------------------------------------------------------\n",
    "    # make return statements consistent\n",
    "    return(decode_display(utt, batch_size=batch_size,\n",
    "                          pred_limit=model.m_cfg['max_en_pred'],\n",
    "#                           pred_limit=20,\n",
    "                          y=y, display_limit=display_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_display(utt, batch_size, pred_limit, y=None, display_limit=10):\n",
    "    xp = cuda.cupy if model.gpuid >= 0 else np\n",
    "    # max number of predictions to make\n",
    "    # if labels are provided, this variable is not used\n",
    "    stop_limit = pred_limit\n",
    "    # to track number of predictions made\n",
    "    npred = 0\n",
    "    # to store loss\n",
    "    loss = 0\n",
    "    # if labels are provided, use them for computing loss\n",
    "    compute_loss = True if y is not None else False\n",
    "    # ---------------------------------------------------------------------\n",
    "    if compute_loss:\n",
    "        stop_limit = len(y)-1\n",
    "        # get starting word to initialize decoder\n",
    "        curr_word = y[0]\n",
    "    else:\n",
    "        # intialize starting word to GO_ID symbol\n",
    "        curr_word = Variable(xp.full((batch_size,), GO_ID, dtype=xp.int32))\n",
    "    # ---------------------------------------------------------------------\n",
    "    # flag to track if all sentences in batch have predicted EOS\n",
    "    # ---------------------------------------------------------------------\n",
    "    with cupy.cuda.Device(model.gpuid):\n",
    "        check_if_all_eos = xp.full((batch_size,), False, dtype=xp.bool_)\n",
    "    # ---------------------------------------------------------------------\n",
    "    a_units = m_cfg['attn_units']\n",
    "    ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "    # ---------------------------------------------------------------------\n",
    "    prob_out = {}\n",
    "    prob_print_str = []\n",
    "    while npred < (stop_limit):\n",
    "        # -----------------------------------------------------------------\n",
    "        # decode and predict\n",
    "        #print(\"decoding with word: {0:s}\".format(vocab_dict['en_w']['i2w'][curr_word.data[0].tolist()].decode()))\n",
    "        pred_out, ht = model.decode(curr_word, ht)\n",
    "        pred_word = F.argmax(pred_out, axis=1)\n",
    "        # -----------------------------------------------------------------\n",
    "        # printing conditional probabilities\n",
    "        # -----------------------------------------------------------------\n",
    "        pred_probs = xp.asnumpy(F.softmax(pred_out).data[0])\n",
    "        top_n_probs = np.argsort(pred_probs)[-display_limit:]\n",
    "        #print(\"-\"*60)\n",
    "        #print(\"predicting word : {0:d}\".format(npred))\n",
    "        prob_print_str.append(\"-\" * 60)\n",
    "        prob_print_str.append(\"predicting word : {0:d}\".format(npred))\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "#         if npred == 0:\n",
    "#             sample_word = np.random.choice(range(len(pred_probs)), p=pred_probs)\n",
    "#             sample_word = np.argsort(pred_probs)[-2]\n",
    "#             print(np.argsort(pred_probs)[-2], np.argsort(pred_probs)[-1])\n",
    "#             pred_word = Variable(xp.asarray([sample_word], dtype=xp.int32))\n",
    "        # -----------------------------------------------------------------\n",
    "        \n",
    "        prob_out[npred] = {}\n",
    "        for pi in top_n_probs[::-1]:\n",
    "            prob_out[npred][v_dict['i2w'][pi].decode()] = \"{0:.3f}\".format(pred_probs[pi])\n",
    "            #print(\"{0:10s} = {1:5.3f}\".format(v_dict['i2w'][pi].decode(), pred_probs[pi]))\n",
    "            prob_print_str.append(\"{0:10s} = {1:5.3f}\".format(v_dict['i2w'][pi].decode(), pred_probs[pi]))\n",
    "            \n",
    "        # -----------------------------------------------------------------\n",
    "        # save prediction at this time step\n",
    "        # -----------------------------------------------------------------\n",
    "        if npred == 0:\n",
    "            pred_sents = pred_word.data\n",
    "        else:\n",
    "            pred_sents = xp.vstack((pred_sents, pred_word.data))\n",
    "        # -----------------------------------------------------------------\n",
    "        if compute_loss:\n",
    "            # compute loss\n",
    "            loss += F.softmax_cross_entropy(pred_out, y[npred+1],\n",
    "                                               class_weight=model.mask_pad_id)\n",
    "        # -----------------------------------------------------------------\n",
    "        curr_word = pred_word\n",
    "        # -----------------------------------------------------------------\n",
    "        # check if EOS is predicted for all sentences\n",
    "        # -----------------------------------------------------------------\n",
    "        check_if_all_eos[pred_word.data == EOS_ID] = True\n",
    "        if xp.all(check_if_all_eos):\n",
    "            break\n",
    "        # -----------------------------------------------------------------\n",
    "        # increment number of predictions made\n",
    "        npred += 1\n",
    "        # -----------------------------------------------------------------\n",
    "    \n",
    "    out_fname = os.path.join(m_cfg['model_dir'], \"probs\", \"{0:s}_probs.json\".format(utt))\n",
    "    with open(out_fname, \"w\") as out_f:\n",
    "        json.dump(prob_out, out_f, indent=4)\n",
    "    print(\"saved probs in : {0:s}\".format(out_fname))\n",
    "    return pred_sents.T, loss, \"\\n\".join(prob_print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_loss(eg_utt, curr_set='fisher_dev', teacher_ratio=1.0):\n",
    "    # get shape\n",
    "    if \"train\" in curr_set:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "        play_audio = False\n",
    "    else:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "        play_audio = True\n",
    "        \n",
    "    eg_utt_bucket = -1\n",
    "    for i, bucket in enumerate(bucket_dict[curr_set][\"buckets\"]):\n",
    "        if eg_utt in bucket:\n",
    "            eg_utt_bucket = i\n",
    "            #print(\"found\")\n",
    "        # end if\n",
    "    # end for\n",
    "    #print(\"found in bucket : {0:d}\".format(eg_utt_bucket))\n",
    "    width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "    utt_list = [eg_utt]\n",
    "    \n",
    "    batch_data = get_batch(map_dict[curr_set], \n",
    "                           enc_key,\n",
    "                           dec_key,\n",
    "                           utt_list,\n",
    "                           vocab_dict,\n",
    "                           (eg_utt_bucket+1) * width_b,\n",
    "                           200,\n",
    "                           input_path=local_input_path)\n",
    "    \n",
    "    X, y = batch_data['X'], batch_data['y']\n",
    "    \n",
    "    batch_size = X.shape[0]\n",
    "    # encode input\n",
    "    model.forward_enc(X)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # initialize decoder LSTM to final encoder state\n",
    "    # ---------------------------------------------------------------------\n",
    "    model.set_decoder_state()\n",
    "    # ---------------------------------------------------------------------\n",
    "    y = F.swapaxes(y, 0, 1)\n",
    "        \n",
    "    xp = cuda.cupy if model.gpuid >= 0 else np\n",
    "    \n",
    "    decoder_batch = y \n",
    "    batch_size = decoder_batch.shape[1]\n",
    "    loss = 0\n",
    "    # ---------------------------------------------------------------------\n",
    "    # initialize hidden states as a zero vector\n",
    "    # ---------------------------------------------------------------------\n",
    "    a_units = model.m_cfg['attn_units']\n",
    "    ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "    # ---------------------------------------------------------------------\n",
    "    decoder_input = decoder_batch[0]\n",
    "    # for all sequences in the batch, feed the characters one by one\n",
    "    for curr_word, next_word in zip(decoder_batch, decoder_batch[1:]):\n",
    "        #print(curr_word, next_word)\n",
    "        # -----------------------------------------------------------------\n",
    "        # teacher forcing logic\n",
    "        # -----------------------------------------------------------------\n",
    "        use_label = True if random.random() < teacher_ratio else False\n",
    "        if use_label:\n",
    "            decoder_input = curr_word\n",
    "        # -----------------------------------------------------------------\n",
    "        # encode tokens\n",
    "        # -----------------------------------------------------------------\n",
    "        predicted_out, ht = model.decode(decoder_input, ht)\n",
    "        decoder_input = F.argmax(predicted_out, axis=1)\n",
    "        #print(decoder_input)\n",
    "        # -----------------------------------------------------------------\n",
    "        # compute loss\n",
    "        # -----------------------------------------------------------------\n",
    "        loss_arr = F.softmax_cross_entropy(predicted_out, next_word,\n",
    "                                           class_weight=model.mask_pad_id)\n",
    "        #print(loss_arr.data.tolist())\n",
    "        loss += loss_arr\n",
    "        # -----------------------------------------------------------------\n",
    "    #print(loss, loss / (y.shape[0]-2), y.shape)\n",
    "    return loss.data.tolist(), (loss / (y.shape[0]-1)).data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_utt_data(eg_utt, curr_set='fisher_dev'):\n",
    "    # get shape\n",
    "    if \"train\" in curr_set:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "        play_audio = False\n",
    "    else:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "        play_audio = True\n",
    "        \n",
    "    eg_utt_bucket = -1\n",
    "    for i, bucket in enumerate(bucket_dict[curr_set][\"buckets\"]):\n",
    "        if eg_utt in bucket:\n",
    "            eg_utt_bucket = i\n",
    "            #print(\"found\")\n",
    "        # end if\n",
    "    # end for\n",
    "    #print(\"found in bucket : {0:d}\".format(eg_utt_bucket))\n",
    "    width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "    utt_list = [eg_utt]\n",
    "    \n",
    "    \n",
    "    batch_data = get_batch(map_dict[curr_set], \n",
    "                           enc_key,\n",
    "                           dec_key,\n",
    "                           utt_list,\n",
    "                           vocab_dict,\n",
    "                           (eg_utt_bucket+1) * width_b,\n",
    "                           200,\n",
    "                           input_path=local_input_path)\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unpadded_batch(m_dict, x_key, y_key, utt_list, vocab_dict, bow_dict, max_enc, max_dec, input_path='', set_zero_num=500):\n",
    "    batch_data = {'X':[], 't':[], 'y':[]}\n",
    "    # -------------------------------------------------------------------------\n",
    "    # loop through each utterance in utt list\n",
    "    # -------------------------------------------------------------------------\n",
    "    for i, u in enumerate(utt_list):\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add X data\n",
    "        # ---------------------------------------------------------------------\n",
    "        if x_key == 'sp':\n",
    "            # -----------------------------------------------------------------\n",
    "            # for speech data\n",
    "            # -----------------------------------------------------------------\n",
    "            # get path to speech file\n",
    "            utt_sp_path = os.path.join(input_path, \"{0:s}.npy\".format(u))\n",
    "            if not os.path.exists(utt_sp_path):\n",
    "                # for training data, there are sub-folders\n",
    "                utt_sp_path = os.path.join(input_path,\n",
    "                                           u.split('_',1)[0],\n",
    "                                           \"{0:s}.npy\".format(u))\n",
    "            if os.path.exists(utt_sp_path):\n",
    "                x_data = Variable(xp.load(utt_sp_path)[:max_enc])\n",
    "            else:\n",
    "                # -------------------------------------------------------------\n",
    "                # exception if file not found\n",
    "                # -------------------------------------------------------------\n",
    "                raise FileNotFoundError(\"ERROR!! file not found: {0:s}\".format(utt_sp_path))\n",
    "                # -------------------------------------------------------------\n",
    "        else:\n",
    "            # -----------------------------------------------------------------\n",
    "            # for text data\n",
    "            # -----------------------------------------------------------------\n",
    "            x_ids = [vocab_dict[x_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][x_key]]\n",
    "            x_ids = xp.asarray(x_ids, dtype=xp.int32)\n",
    "            batch_data['X'].append(x_ids[:max_enc])\n",
    "            # -----------------------------------------------------------------\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add labels\n",
    "        # ---------------------------------------------------------------------\n",
    "        if type(m_dict[u][y_key]) == list:\n",
    "            en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key]])-set(range(4)))\n",
    "        else:\n",
    "            # dev and test data have multiple translations\n",
    "            # choose the first one for computing perplexity\n",
    "            en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key][0]])-set(range(4)))\n",
    "        y_ids = en_ids[:max_dec]\n",
    "        # ---------------------------------------------------------------------\n",
    "        if len(x_data) > 0 and len(y_ids) > 0:\n",
    "            batch_data['X'].append(x_data)\n",
    "            batch_data['t'].append([y_ids])\n",
    "            y_data = xp.zeros(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "            #y_data = -1 * xp.ones(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "            #set_some_to_0 = np.random.choice(np.array(range(len(bow_dict['w2i'])), dtype=\"i\"), \n",
    "            #                                 size=set_zero_num, \n",
    "            #                                 replace=False)\n",
    "            #y_data[set_some_to_0] = 0\n",
    "            y_data[y_ids] = 1\n",
    "            y_data[list(range(4))] = -1\n",
    "            batch_data['y'].append(y_data)\n",
    "            \n",
    "    # -------------------------------------------------------------------------\n",
    "    # end for all utterances in batch\n",
    "    # -------------------------------------------------------------------------\n",
    "#     if len(batch_data['X']) > 0 and len(batch_data['y']) > 0:\n",
    "#         batch_data['X'] = F.pad_sequence(batch_data['X'], padding=PAD_ID)\n",
    "#         batch_data['y'] = F.pad_sequence(batch_data['y'], padding=PAD_ID)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hmm_get_curr_bow_batch(m_dict, x_key, y_key, utt_list, vocab_dict, bow_dict, max_enc, max_dec, input_path='', set_zero_num=500):\n",
    "    batch_data = {'X':[], 't':[], 'y':[]}\n",
    "    # -------------------------------------------------------------------------\n",
    "    # loop through each utterance in utt list\n",
    "    # -------------------------------------------------------------------------\n",
    "    for i, u in enumerate(utt_list):\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add X data\n",
    "        # ---------------------------------------------------------------------\n",
    "        if x_key == 'sp':\n",
    "            # -----------------------------------------------------------------\n",
    "            # for speech data\n",
    "            # -----------------------------------------------------------------\n",
    "            # get path to speech file\n",
    "            utt_sp_path = os.path.join(input_path, \"{0:s}.npy\".format(u))\n",
    "            if not os.path.exists(utt_sp_path):\n",
    "                # for training data, there are sub-folders\n",
    "                utt_sp_path = os.path.join(input_path,\n",
    "                                           u.split('_',1)[0],\n",
    "                                           \"{0:s}.npy\".format(u))\n",
    "            if os.path.exists(utt_sp_path):\n",
    "                x_data = xp.load(utt_sp_path)[:max_enc]\n",
    "            else:\n",
    "                # -------------------------------------------------------------\n",
    "                # exception if file not found\n",
    "                # -------------------------------------------------------------\n",
    "                raise FileNotFoundError(\"ERROR!! file not found: {0:s}\".format(utt_sp_path))\n",
    "                # -------------------------------------------------------------\n",
    "        else:\n",
    "            # -----------------------------------------------------------------\n",
    "            # for text data\n",
    "            # -----------------------------------------------------------------\n",
    "            x_ids = [vocab_dict[x_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][x_key]]\n",
    "            x_ids = xp.asarray(x_ids, dtype=xp.int32)\n",
    "            batch_data['X'].append(x_ids[:max_enc])\n",
    "            # -----------------------------------------------------------------\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add labels\n",
    "        # ---------------------------------------------------------------------\n",
    "        if type(m_dict[u][y_key]) == list:\n",
    "            en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key]])-set(range(4)))\n",
    "        else:\n",
    "            # dev and test data have multiple translations\n",
    "            # choose the first one for computing perplexity\n",
    "            en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key][0]])-set(range(4)))\n",
    "        y_ids = en_ids[:max_dec]\n",
    "        # ---------------------------------------------------------------------\n",
    "        if len(x_data) > 0 and len(y_ids) > 0:\n",
    "            batch_data['X'].append(x_data)\n",
    "            batch_data['t'].append([y_ids])\n",
    "            y_data = xp.zeros(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "            #y_data = -1 * xp.ones(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "            #set_some_to_0 = np.random.choice(np.array(range(len(bow_dict['w2i'])), dtype=\"i\"), \n",
    "            #                                 size=set_zero_num, \n",
    "            #                                 replace=False)\n",
    "            #y_data[set_some_to_0] = 0\n",
    "            y_data[y_ids] = 1\n",
    "            y_data[list(range(4))] = -1\n",
    "            batch_data['y'].append(y_data)\n",
    "            \n",
    "    # -------------------------------------------------------------------------\n",
    "    # end for all utterances in batch\n",
    "    # -------------------------------------------------------------------------\n",
    "    if len(batch_data['X']) > 0 and len(batch_data['y']) > 0:\n",
    "        batch_data['X'] = F.pad_sequence(batch_data['X'], padding=PAD_ID)\n",
    "        batch_data['y'] = F.pad_sequence(batch_data['y'], padding=PAD_ID)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg_path = \"./sp2bagwords/sp_1.0_h-128_e-128_rnn-2_hwy-1_cnn-128-5-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_out_dim = rnn_in_units =  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sameer/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/utils/experimental.py:104: FutureWarning: chainer.links.normalization.layer_normalization.py is experimental. The interface can change in the future.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using SGD optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "model not found\n"
     ]
    }
   ],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_dict_path = os.path.join(m_cfg['data_path'],\n",
    "                                     'train_top_K_enw.dict')\n",
    "if os.path.exists(bow_dict_path):\n",
    "    bow_dict = pickle.load(open(bow_dict_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict, bow_dict = get_data_dicts(m_cfg)\n",
    "batch_size = {'max': 96, 'med': 128, 'min': 256, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308663, 1497356, 0.2061386871258405)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bow_dict['freq'].values()), sum(vocab_dict['en_w']['freq'].values()), sum(bow_dict['freq'].values()) / sum(vocab_dict['en_w']['freq'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bow_dict['i2w'].keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, m_cfg['max_en_pred']\n",
    "# min_len, max_len = 0, 10\n",
    "displayN = 50\n",
    "m_dict=map_dict[dev_key]\n",
    "# wavs_path = os.path.join(m_cfg['data_path'], \"wavs\")\n",
    "wavs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"wavs\")\n",
    "v_dict = vocab_dict['en_w']\n",
    "key = m_cfg['dev_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20051009_182032_217_fsp-B-1']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_dict['fisher_dev']['buckets'][0][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curr_bucket = 0\n",
    "num_utts = 10\n",
    "utt_list = bucket_dict['fisher_train']['buckets'][curr_bucket][:num_utts]\n",
    "curr_set='fisher_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if \"train\" in curr_set:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "    play_audio = False\n",
    "else:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "    play_audio = True\n",
    "\n",
    "width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "\n",
    "batch_data = hmm_get_curr_bow_batch(map_dict[curr_set], \n",
    "                                enc_key,\n",
    "                                dec_key,\n",
    "                                utt_list,\n",
    "                                vocab_dict,\n",
    "                                bow_dict,\n",
    "                                (curr_bucket+1) * width_b,\n",
    "                                200,\n",
    "                                input_path=local_input_path)\n",
    "\n",
    "X, y, t = batch_data['X'], batch_data['y'], batch_data['t']\n",
    "\n",
    "batch_size = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[4]], [[8, 9, 85]], [[8]], [[8]], [[23]], [[37]]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-11.340629577636719, dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.min(batch_data['X'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"train\" in curr_set:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "    play_audio = False\n",
    "else:\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "    play_audio = True\n",
    "\n",
    "width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "\n",
    "batch_data = get_unpadded_batch(map_dict[curr_set], \n",
    "                                enc_key,\n",
    "                                dec_key,\n",
    "                                utt_list,\n",
    "                                vocab_dict,\n",
    "                                bow_dict,\n",
    "                                (curr_bucket+1) * width_b,\n",
    "                                200,\n",
    "                                input_path=local_input_path)\n",
    "\n",
    "X, y, t = batch_data['X'], batch_data['y'], batch_data['t']\n",
    "\n",
    "# batch_size = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_nlstm = L.NStepBiLSTM(2, 80, 256, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 6, 640)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy and cupy must not be used together\ntype(W): <class 'numpy.ndarray'>, type(x): <class 'cupy.core.core.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4dbed991aa67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_nlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/links/connection/n_step_lstm.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, hx, cx, xs, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         hy, cy, trans_y = self.rnn(\n\u001b[0;32m--> 135\u001b[0;31m             self.n_layers, self.dropout, hx, cx, ws, bs, trans_x)\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/functions/connection/n_step_lstm.py\u001b[0m in \u001b[0;36mn_step_bilstm\u001b[0;34m(n_layers, dropout_ratio, hx, cx, ws, bs, xs, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m     return n_step_lstm_base(n_layers, dropout_ratio, hx, cx, ws, bs, xs,\n\u001b[0;32m--> 246\u001b[0;31m                             use_bi_direction=True, **kwargs)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/functions/connection/n_step_lstm.py\u001b[0m in \u001b[0;36mn_step_lstm_base\u001b[0;34m(n_layers, dropout_ratio, hx, cx, ws, bs, xs, use_bi_direction, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_one_directional_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0mhy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mcy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/functions/connection/n_step_lstm.py\u001b[0m in \u001b[0;36m_one_directional_loop\u001b[0;34m(di)\u001b[0m\n\u001b[1;32m    381\u001b[0m                         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     lstm_in = linear.linear(x, xws[layer_idx],\n\u001b[0;32m--> 383\u001b[0;31m                                             xbs[layer_idx]) + \\\n\u001b[0m\u001b[1;32m    384\u001b[0m                         \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/functions/connection/linear.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(x, W, b)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_indexes_to_retain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_indexes_to_retain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/functions/connection/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             raise ValueError('numpy and cupy must not be used together\\n'\n\u001b[1;32m     36\u001b[0m                              \u001b[0;34m'type(W): {0}, type(x): {1}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                              .format(type(W), type(x)))\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# NumPy raises an error when the array is not contiguous.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy and cupy must not be used together\ntype(W): <class 'numpy.ndarray'>, type(x): <class 'cupy.core.core.ndarray'>"
     ]
    }
   ],
   "source": [
    "t_rnn = t_nlstm(hx=None, cx=None, xs=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 210, 80) (6, 104)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer.hyperparam.alpha = 0.001\n",
    "optimizer.hyperparam.lr = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_out[0,:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    pred_words = []\n",
    "    batch_data = hmm_get_curr_bow_batch(map_dict[curr_set], \n",
    "                                enc_key,\n",
    "                                dec_key,\n",
    "                                utt_list,\n",
    "                                vocab_dict,\n",
    "                                bow_dict,\n",
    "                                (curr_bucket+1) * width_b,\n",
    "                                200,\n",
    "                                input_path=local_input_path,\n",
    "                                set_zero_num=50)\n",
    "\n",
    "    X, y, t = batch_data['X'], batch_data['y'], batch_data['t']\n",
    "\n",
    "    batch_size = X.shape[0]\n",
    "    # encode input\n",
    "    model.forward_enc(X)\n",
    "    \n",
    "    if m_cfg['highway_layers'] > 0:\n",
    "        highway_h = model.forward_highway(model.h_final_rnn)\n",
    "\n",
    "    predicted_out = model.out(highway_h)\n",
    "    \n",
    "    simple_loss = F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"mean\")\n",
    "    loss = F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"no\")\n",
    "    \n",
    "    loss_weights = xp.ones(shape=y.data.shape, dtype=\"f\")\n",
    "    loss_weights[y.data < 0] = 0\n",
    "    loss_weights[y.data == 0] = 1\n",
    "    loss_weights[y.data > 0] = 10\n",
    "    #loss_avg = F.average(F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce='no'), weights=loss_weights)\n",
    "    loss_avg = F.mean(loss_weights * loss)\n",
    "    print(i, \"---\".join([\"{0:.3f}\".format(float(val)) for val in (loss_avg.data, xp.mean(loss.data), simple_loss.data)]))\n",
    "    model.cleargrads()\n",
    "    loss_avg.backward()\n",
    "    optimizer.update()\n",
    "#     for row in predicted_out.data[:5]:\n",
    "#         print(xp.where(row > 0.6)[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = []\n",
    "for i, row in enumerate(predicted_out.data):\n",
    "    print(row[10:12])\n",
    "    pred_inds = xp.where(row >= PRED_THRESH)[0]\n",
    "    if len(pred_inds) > 20:\n",
    "        pred_inds = xp.argsort(row)[-20:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    curr_row = [i for i in pred_inds.tolist() if i > 3]\n",
    "#     if i < 5:\n",
    "#         print(curr_row)\n",
    "    pred_words.append(curr_row)\n",
    "    probs.append(row)\n",
    "# print(pred_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = F.pad_sequence(probs).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[:,10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "for row, pred, ttt in zip(y.data[:N], predicted_out.data[:N], batch_data['t']):\n",
    "    #print(xp.where(row == 1))\n",
    "    print(ttt[0])\n",
    "    print(xp.where(pred >= PRED_THRESH)[0].tolist())\n",
    "    print(set(xp.where(pred >= PRED_THRESH)[0].tolist()) & set(ttt[0]))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(batch_data['t'], pred_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.binary_accuracy(predicted_out.data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data['y'][:,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_weight = xp.ones(shape=y.data.shape, dtype=\"f\")\n",
    "loss_weight[y.data < 0] = 0\n",
    "loss_weight[y.data == 0] = 10\n",
    "loss_weight[y.data > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.data[0][:20], loss_weight[0,:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_full = F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce='no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_full.shape, loss_weight.shape, loss_full[0,:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.average(loss_full, weights=loss_weight), F.mean(loss_full), F.mean((loss_full*loss_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.average(F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"no\").data), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(predicted_out, y, normalize=True, reduce=\"mean\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_h = model.out_1(model.h_final_rnn)\n",
    "predicted_out = model.out_2(F.sigmoid(out_h))\n",
    "loss = F.sigmoid_cross_entropy(predicted_out, y, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.cleargrads()\n",
    "loss.backward()\n",
    "optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_out.data[0,9:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(xp.where(predicted_out.data > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[0,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp.where(y[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[vocab_dict['en_w']['i2w'][i] for i in t[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.h_final_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.h_final_rnn[-1,:10], model.h_final_rnn[-1,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.L1_enc.h[-1,:10], model.L1_rev_enc.h[-1][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_fwd = model.L1_enc.h.data\n",
    "h_rev = model.L1_rev_enc.h.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_fwd[-1,:10], h_rev[-1,-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.concat((h_fwd, h_rev), axis=1)[-1][:10], F.concat((h_fwd, h_rev), axis=1)[-1][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pata = model[model.rnn_enc[-1]].h.data\n",
    "if model.m_cfg['bi_rnn']:\n",
    "    h_rev = model[model.rnn_rev_enc[-1]].h.data\n",
    "    pata = F.concat((pata, h_rev), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pata[-1][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.out_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "haha = F.sigmoid(model.out_1(model.h_final_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_y = xp.ones(y.shape, dtype='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hu = model.out_2(haha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(hu, dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = F.sigmoid_cross_entropy(hu, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.cleargrads()\n",
    "loss.backward()\n",
    "optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ha = F.sigmoid(model.out_2(haha)).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[expit(i) for i in hu[0][:10].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ha[0].data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp.where(ha[0].data >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = F.sigmoid_cross_entropy(model.out_2(haha), y, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[0][:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ha[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ha.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data['y'][0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[0.1, 0.7, 0.2], # prediction label is 1\n",
    "              [8.0, 1.0, 2.0], # prediction label is 0\n",
    "              [-8.0, 1.0, 2.0], # prediction label is 2\n",
    "              [-8.0, -1.0, -2.0]]) # prediction label is 1\n",
    "t = np.array([1, 0, 2, 1], 'i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.accuracy(y, t).data # 100% accuracy because all samples are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.array([1, 0, 0, 0], 'i')\n",
    "F.accuracy(y, t).data # 100% accuracy because all samples are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ha.shape, batch_data['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.binary_accuracy(ha.data, batch_data['y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
