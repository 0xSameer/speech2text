{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_fil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0fb065746136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_files = [f for f in os.listdir(os.path.dirname(model_fil))\n\u001b[0m\u001b[1;32m      2\u001b[0m                    if os.path.basename(model_fil).replace('.model','') in f]\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(model_files)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_fil' is not defined"
     ]
    }
   ],
   "source": [
    "model_files = [f for f in os.listdir(os.path.dirname(model_fil))\n",
    "                   if os.path.basename(model_fil).replace('.model','') in f]\n",
    "# print(model_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model_fil = max(model_files, key=lambda s: int(s.split('_')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key='fisher_train'\n",
    "train=False\n",
    "m_dict = map_dict[key]\n",
    "v_dict = vocab_dict[dec_key]\n",
    "n=len(map_dict[key])\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(m_dict.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words = []\n",
    "_ = [en_words.extend(w['en_w']) for w in m_dict.values()]\n",
    "\n",
    "es_words = []\n",
    "_ = [es_words.extend(w['es_w']) for w in m_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_words), len(set(en_words)), len(es_words), len(set(es_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(word_list, prune_freq=0):\n",
    "    vocab = {}\n",
    "    for w in word_list:\n",
    "        if w in vocab:\n",
    "            vocab[w] += 1\n",
    "        else:\n",
    "            vocab[w] = 1\n",
    "    # end for\n",
    "    \n",
    "    # prune \n",
    "    vocab = {k:v for k,v in vocab.items() if v >= prune_freq}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = create_vocab(en_words, prune_freq=1)\n",
    "es_vocab = create_vocab(es_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words_pruned = [w for w in en_words if w in en_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max([len(w) for w in en_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_range = (np.min([len(w) for w in en_vocab]), np.max([len(w) for w in en_vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([len(w) for w in en_words_pruned])\n",
    "len_counts = {i:0 for i in range(1,np.max(x)+1,1)}\n",
    "\n",
    "for i in x:\n",
    "    len_counts[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "_ = sns.barplot(x=list(len_counts.keys()), y = list(len_counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"..\"k,v)\n",
    "# os.chdir(\"chainer2/speech2text\")\n",
    "os.chdir(\"/afs/inf.ed.ac.uk/group/project/lowres/work/chainer2/speech2text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words_by_len = {i: set([w for w in en_vocab if len(w) ==  i]) for i in range(1,np.max(x)+1,1)}\n",
    "en_word_counts_by_len = {i: [w for w in en_words_pruned if len(w) ==  i] for i in range(1,np.max(x)+1,1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_len = 12\n",
    "total_words_above_len = 0\n",
    "total_word_counts_above_len = 0\n",
    "for i in range(start_len,np.max(x)+1,1):\n",
    "    total_words_above_len += len(en_words_by_len[i])\n",
    "    total_word_counts_above_len += len(en_word_counts_by_len[i])\n",
    "    print(\"{0:5d} | {1:5d} | {2:5d} | {3:5d}\".format(i, \n",
    "                                                     len(en_words_by_len[i]), \n",
    "                                                     total_words_above_len,\n",
    "                                                     total_word_counts_above_len))\n",
    "\n",
    "print(\"\\ntotal english words={0:d}\".format(len(en_words_pruned)))\n",
    "\n",
    "# start_len = 15\n",
    "# total_word_counts_above_len = 0\n",
    "# for i in range(start_len,np.max(x)+1,1):\n",
    "#     total_word_counts_above_len += len(en_word_counts_by_len[i])\n",
    "#     print(\"{0:5d} | {1:5d} | {2:5d}\".format(i, len(en_words_by_len[i]), total_word_counts_above_len))\n",
    "# print(total_word_counts_above_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words_above_len = {k:v for k,v in en_vocab.items() if len(k) >= start_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([(w,len(w)) for w in en_words_above_len], key=lambda t:t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import *\n",
    "# from nltk.stem.porter import *\n",
    "# from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# lemmer = nltk.wordnet.WordNetLemmatizer()\n",
    "from stemming.porter2 import stem\n",
    "import stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems_lems = [(w.decode(\"utf-8\"), stem(w.decode(\"utf-8\")), stem(w.decode(\"utf-8\"))) for w in en_words_above_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem('residencies'), stem('residencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_stems = [stem(w.decode()) for w in en_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(en_vocab.keys())), len(set(en_word_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_stem_freq = {}\n",
    "for w in en_vocab:\n",
    "    if stem(w.decode()) not in en_word_stem_freq:\n",
    "        en_word_stem_freq[stem(w.decode())] = [w.decode()]\n",
    "    else:\n",
    "        en_word_stem_freq[stem(w.decode())].append(w.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_stem_cats = sorted(en_word_stem_freq.items(), reverse=True, key=lambda t: len(t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pp = PrettyTable([\"stem\",\"words\"], hrules=True)\n",
    "display_pp.align = \"l\"\n",
    "for stem, word_list in en_word_stem_cats:\n",
    "    if len(word_list) > 1:\n",
    "        display_pp.add_row([stem, textwrap.fill(\", \".join(word_list),50)])\n",
    "\n",
    "print(display_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.array([len(v) for k,v in en_word_stem_cats if len(v)>1])\n",
    "len_counts = {i:0 for i in range(1,np.max(x)+1,1)}\n",
    "\n",
    "for i in x:\n",
    "    len_counts[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "_ = sns.barplot(x=list(len_counts.keys()), y = list(len_counts.values()), palette=\"Blues_d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(stems_lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict['20050908_182943_22_fsp-A-1']['en_w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_speech_path = os.path.join(out_path, key)\n",
    "wavs_path = os.path.join(out_path, \"wavs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*80)\n",
    "print(\"EPOCH = {0:d}\".format(last_epoch+1))\n",
    "fsh_pred_sents, fsh_utts, loss = feed_model(map_dict[key],\n",
    "                  b_dict=bucket_dict[key],\n",
    "                  vocab_dict=vocab_dict,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  x_key=enc_key,\n",
    "                  y_key=dec_key,\n",
    "                  train=train,\n",
    "                  cat_speech_path=cat_speech_path, use_y=True)\n",
    "\n",
    "print(\"{0:s} {1:s} mean loss={2:.4f}\".format(\"*\" * 10,\n",
    "                                    \"train\" if train else \"dev\",\n",
    "                                    loss))\n",
    "print(\"-\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, 200\n",
    "displayN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_words(m_dict, v_dict, fsh_pred_sents[:displayN], fsh_utts[:displayN], \n",
    "              dec_key, key, min_len=min_len, max_len=max_len, play_audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, _, h, r = calc_bleu(m_dict, \n",
    "                              v_dict, \n",
    "                              fsh_pred_sents, \n",
    "                              fsh_utts, \n",
    "                              dec_key, \n",
    "                              min_len=min_len, \n",
    "                              max_len=max_len, \n",
    "                              ref_index=ref_index)\n",
    "\n",
    "print(\"BLEU score on: {0:s} = {1:.2f}\".format(key, b * 100))\n",
    "print(\"-\"*60)\n",
    "\n",
    "# chrf = [0]*4\n",
    "\n",
    "# for ref_i in range(4):\n",
    "#     _, chrf[ref_i], _, _ = calc_bleu(m_dict, \n",
    "#                               v_dict, \n",
    "#                               fsh_pred_sents[:n], \n",
    "#                               fsh_utts[:n], \n",
    "#                               dec_key, \n",
    "#                               min_len=min_len, \n",
    "#                               max_len=max_len, \n",
    "#                               ref_index=ref_i)\n",
    "#     # end for\n",
    "\n",
    "# # print chrf score\n",
    "# for i in range(4):\n",
    "#     print(\"CHRF score on: {0:s} = {1:.2f}, using reference = {2:d}\".format(key, chrf[i] * 100, i))\n",
    "\n",
    "# print(\"-\"*60)\n",
    "\n",
    "all_weights=[(1.,0.,0.,0.),\n",
    "             (0.,1.,0.,0.),\n",
    "             (0.,0.,1.,0.),\n",
    "             (0.,0.,0.,1.),\n",
    "             (1./2,1./2,0.,0.),\n",
    "             (1./3,1./3,1./3,0.),\n",
    "             (.25,.25,.25,.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:>20s} | {1:20s}\".format(\"bleu score (0-100)\", \"uni-bi-tri-quad\"))\n",
    "for weights in all_weights:\n",
    "    b = corpus_bleu(r, h, weights=weights, smoothing_function=smooth_fun.method2)\n",
    "    print(\"{0:20.2f} | {1:20s}\".format(b * 100, \"-\".join(map(\"{0:.2f}\".format, weights))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = corpus_precision_recall(r, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = \"/afs/inf.ed.ac.uk/group/project/lowres/work/installs/fisher-callhome-corpus/corpus/ldc\"\n",
    "pred_fname = os.path.join(pred_path, \"fisher_dev.pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict[fsh_utts[2]]['en_w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ref, en_hyp = write_predictions_to_file(m_dict, v_dict, fsh_pred_sents, fsh_utts, dec_key, key, min_len=min_len, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_ref),len(en_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callhome dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key='callhome_devtest'\n",
    "train=False\n",
    "m_dict = map_dict[key]\n",
    "v_dict = vocab_dict[dec_key]\n",
    "n=len(map_dict[key])\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_speech_path = os.path.join(out_path, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*80)\n",
    "print(\"EPOCH = {0:d}\".format(last_epoch+1))\n",
    "call_pred_sents, call_utts, loss = feed_model(map_dict[key],\n",
    "                  b_dict=bucket_dict[key],\n",
    "                  vocab_dict=vocab_dict,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  x_key=enc_key,\n",
    "                  y_key=dec_key,\n",
    "                  train=train,\n",
    "                  cat_speech_path=cat_speech_path, use_y=True)\n",
    "\n",
    "print(\"{0:s} {1:s} mean loss={2:.4f}\".format(\"*\" * 10,\n",
    "                                    \"train\" if train else \"dev\",\n",
    "                                    loss))\n",
    "print(\"-\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_words(m_dict, v_dict, call_pred_sents[:displayN], call_utts[:displayN],\n",
    "              dec_key, key, min_len=min_len, max_len=max_len, play_audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, _, h, r = calc_bleu(m_dict, \n",
    "                              v_dict, \n",
    "                              call_pred_sents, \n",
    "                              call_utts, \n",
    "                              dec_key, \n",
    "                              min_len=min_len, \n",
    "                              max_len=max_len, \n",
    "                              ref_index=ref_index)\n",
    "\n",
    "print(\"BLEU score on: {0:s} = {1:.2f}\".format(key, b * 100))\n",
    "print(\"-\"*60)\n",
    "\n",
    "all_weights=[(1.,0.,0.,0.),\n",
    "             (0.,1.,0.,0.),\n",
    "             (0.,0.,1.,0.),\n",
    "             (0.,0.,0.,1.),\n",
    "             (1./2,1./2,0.,0.),\n",
    "             (1./3,1./3,1./3,0.),\n",
    "             (.25,.25,.25,.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:>20s} | {1:20s}\".format(\"bleu score (0-100)\", \"uni-bi-tri-quad\"))\n",
    "for weights in all_weights:\n",
    "    b = corpus_bleu(r, h, weights=weights, smoothing_function=smooth_fun.method2)\n",
    "    print(\"{0:20.2f} | {1:20s}\".format(b * 100, \"-\".join(map(\"{0:.2f}\".format, weights))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = corpus_precision_recall(r, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_en_ref, call_en_hyp = write_predictions_to_file(m_dict, v_dict, call_pred_sents, call_utts, \n",
    "                                                     dec_key, key, min_len=min_len, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(call_en_ref),len(call_en_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ref = [[\"ha ha lol hue\".split()], [\"ha ha ja ha\".split()], [\"ha ha ja ha\".split()]]\n",
    "# test_h = [\"lol ja\".split(), \"ha he\".split(), \"ha ja\".split()]\n",
    "# _, _ = corpus_precision_recall(test_ref, test_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
