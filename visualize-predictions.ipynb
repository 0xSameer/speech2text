{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_utt(utt, m_dict):\n",
    "    sr, y = scipy.io.wavfile.read(os.path.join(wavs_path, utt.rsplit(\"-\",1)[0]+'.wav'))\n",
    "    start_t = min(seg['start'] for seg in m_dict[utt]['seg'])\n",
    "    end_t = max(seg['end'] for seg in m_dict[utt]['seg'])\n",
    "    print(start_t, end_t)\n",
    "    start_t_samples, end_t_samples = int(start_t*sr), int(end_t*sr)\n",
    "    display(Audio(y[start_t_samples:end_t_samples], rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words(m_dict, v_dict, preds, utts, dec_key, key, play_audio=False, displayN=-1):\n",
    "    if displayN == -1:\n",
    "        displayN = len(utts)\n",
    "    es_ref = []\n",
    "    en_ref = []\n",
    "    google_pred = []\n",
    "    for u in utts:\n",
    "        es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "        if type(m_dict[u][dec_key]) == list:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "        google_pred.append(\" \".join(google_hyp_r0[u]))\n",
    "\n",
    "    en_pred = []\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    for p in preds:\n",
    "        if type(p) == list:\n",
    "            t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "            t_str = t_str[:t_str.find('_EOS')]\n",
    "            en_pred.append(t_str)\n",
    "        else:\n",
    "            en_pred.append(\"\")\n",
    "        \n",
    "\n",
    "    for u, es, en, p, g in sorted(list(zip(utts, es_ref, en_ref, en_pred, google_pred)))[:displayN]:\n",
    "        # for reference, 1st word is GO_ID, no need to display\n",
    "        print(\"Utterance: {0:s}\".format(u))\n",
    "        display_pp = PrettyTable([\"cat\",\"sent\"], hrules=True)\n",
    "        display_pp.align = \"l\"\n",
    "        display_pp.header = False\n",
    "        display_pp.add_row([\"es ref\", textwrap.fill(es,50)])\n",
    "        display_pp.add_row([\"en ref\", textwrap.fill(en,50)])\n",
    "        display_pp.add_row([\"en pred\", textwrap.fill(p,50)])\n",
    "        display_pp.add_row([\"google pred\", textwrap.fill(g,50)])\n",
    "        \n",
    "\n",
    "        print(display_pp)\n",
    "        if play_audio:\n",
    "            play_utt(u, m_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(utt, X, y=None, display_limit=10):\n",
    "    # get shape\n",
    "    batch_size = X.shape[0]\n",
    "    # encode input\n",
    "    model.forward_enc(X)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # initialize decoder LSTM to final encoder state\n",
    "    # ---------------------------------------------------------------------\n",
    "    model.set_decoder_state()\n",
    "    # ---------------------------------------------------------------------\n",
    "    # swap axes of the decoder batch\n",
    "    if y is not None:\n",
    "        y = F.swapaxes(y, 0, 1)\n",
    "    # -----------------------------------------------------------------\n",
    "    # predict\n",
    "    # -----------------------------------------------------------------\n",
    "    # make return statements consistent\n",
    "    return(decode_display(utt, batch_size=batch_size,\n",
    "                          pred_limit=model.m_cfg['max_en_pred'],\n",
    "#                           pred_limit=20,\n",
    "                          y=y, display_limit=display_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_display(utt, batch_size, pred_limit, y=None, display_limit=10):\n",
    "    xp = cuda.cupy if model.gpuid >= 0 else np\n",
    "    # max number of predictions to make\n",
    "    # if labels are provided, this variable is not used\n",
    "    stop_limit = pred_limit\n",
    "    # to track number of predictions made\n",
    "    npred = 0\n",
    "    # to store loss\n",
    "    loss = 0\n",
    "    # if labels are provided, use them for computing loss\n",
    "    compute_loss = True if y is not None else False\n",
    "    # ---------------------------------------------------------------------\n",
    "    if compute_loss:\n",
    "        stop_limit = len(y)-1\n",
    "        # get starting word to initialize decoder\n",
    "        curr_word = y[0]\n",
    "    else:\n",
    "        # intialize starting word to GO_ID symbol\n",
    "        curr_word = Variable(xp.full((batch_size,), GO_ID, dtype=xp.int32))\n",
    "    # ---------------------------------------------------------------------\n",
    "    # flag to track if all sentences in batch have predicted EOS\n",
    "    # ---------------------------------------------------------------------\n",
    "    with cupy.cuda.Device(model.gpuid):\n",
    "        check_if_all_eos = xp.full((batch_size,), False, dtype=xp.bool_)\n",
    "    # ---------------------------------------------------------------------\n",
    "    a_units = m_cfg['attn_units']\n",
    "    ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "    # ---------------------------------------------------------------------\n",
    "    prob_out = {}\n",
    "    prob_print_str = []\n",
    "    while npred < (stop_limit):\n",
    "        # -----------------------------------------------------------------\n",
    "        # decode and predict\n",
    "        pred_out, ht = model.decode(curr_word, ht)\n",
    "        pred_word = F.argmax(pred_out, axis=1)\n",
    "        # -----------------------------------------------------------------\n",
    "        # printing conditional probabilities\n",
    "        # -----------------------------------------------------------------\n",
    "        pred_probs = xp.asnumpy(F.softmax(pred_out).data[0])\n",
    "        top_n_probs = np.argsort(pred_probs)[-display_limit:]\n",
    "        #print(\"-\"*60)\n",
    "        #print(\"predicting word : {0:d}\".format(npred))\n",
    "        prob_print_str.append(\"-\" * 60)\n",
    "        prob_print_str.append(\"predicting word : {0:d}\".format(npred))\n",
    "        \n",
    "        prob_out[npred] = {}\n",
    "        for pi in top_n_probs[::-1]:\n",
    "            prob_out[npred][v_dict['i2w'][pi].decode()] = \"{0:.3f}\".format(pred_probs[pi])\n",
    "            #print(\"{0:10s} = {1:5.3f}\".format(v_dict['i2w'][pi].decode(), pred_probs[pi]))\n",
    "            prob_print_str.append(\"{0:10s} = {1:5.3f}\".format(v_dict['i2w'][pi].decode(), pred_probs[pi]))\n",
    "            \n",
    "        # -----------------------------------------------------------------\n",
    "        # save prediction at this time step\n",
    "        # -----------------------------------------------------------------\n",
    "        if npred == 0:\n",
    "            pred_sents = pred_word.data\n",
    "        else:\n",
    "            pred_sents = xp.vstack((pred_sents, pred_word.data))\n",
    "        # -----------------------------------------------------------------\n",
    "        if compute_loss:\n",
    "            # compute loss\n",
    "            loss += F.softmax_cross_entropy(pred_out, y[npred+1],\n",
    "                                               class_weight=model.mask_pad_id)\n",
    "        # -----------------------------------------------------------------\n",
    "        curr_word = pred_word\n",
    "        # -----------------------------------------------------------------\n",
    "        # check if EOS is predicted for all sentences\n",
    "        # -----------------------------------------------------------------\n",
    "        check_if_all_eos[pred_word.data == EOS_ID] = True\n",
    "        if xp.all(check_if_all_eos):\n",
    "            break\n",
    "        # -----------------------------------------------------------------\n",
    "        # increment number of predictions made\n",
    "        npred += 1\n",
    "        # -----------------------------------------------------------------\n",
    "    \n",
    "    out_fname = os.path.join(m_cfg['model_dir'], \"probs\", \"{0:s}_probs.json\".format(utt))\n",
    "    with open(out_fname, \"w\") as out_f:\n",
    "        json.dump(prob_out, out_f, indent=4)\n",
    "    print(\"saved probs in : {0:s}\".format(out_fname))\n",
    "    return pred_sents.T, loss, \"\\n\".join(prob_print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_loss(eg_utt, curr_set='fisher_dev', teacher_ratio=1.0):\n",
    "    # get shape\n",
    "    if \"train\" in curr_set:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "        play_audio = False\n",
    "    else:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "        play_audio = True\n",
    "        \n",
    "    eg_utt_bucket = -1\n",
    "    for i, bucket in enumerate(bucket_dict[curr_set][\"buckets\"]):\n",
    "        if eg_utt in bucket:\n",
    "            eg_utt_bucket = i\n",
    "            #print(\"found\")\n",
    "        # end if\n",
    "    # end for\n",
    "    #print(\"found in bucket : {0:d}\".format(eg_utt_bucket))\n",
    "    width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "    utt_list = [eg_utt]\n",
    "    \n",
    "    batch_data = get_batch(map_dict[curr_set], \n",
    "                           enc_key,\n",
    "                           dec_key,\n",
    "                           utt_list,\n",
    "                           vocab_dict,\n",
    "                           (eg_utt_bucket+1) * width_b,\n",
    "                           200,\n",
    "                           input_path=local_input_path)\n",
    "    \n",
    "    X, y = batch_data['X'], batch_data['y']\n",
    "    \n",
    "    batch_size = X.shape[0]\n",
    "    # encode input\n",
    "    model.forward_enc(X)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # initialize decoder LSTM to final encoder state\n",
    "    # ---------------------------------------------------------------------\n",
    "    model.set_decoder_state()\n",
    "    # ---------------------------------------------------------------------\n",
    "    y = F.swapaxes(y, 0, 1)\n",
    "        \n",
    "    xp = cuda.cupy if model.gpuid >= 0 else np\n",
    "    \n",
    "    decoder_batch = y \n",
    "    batch_size = decoder_batch.shape[1]\n",
    "    loss = 0\n",
    "    # ---------------------------------------------------------------------\n",
    "    # initialize hidden states as a zero vector\n",
    "    # ---------------------------------------------------------------------\n",
    "    a_units = model.m_cfg['attn_units']\n",
    "    ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "    # ---------------------------------------------------------------------\n",
    "    decoder_input = decoder_batch[0]\n",
    "    # for all sequences in the batch, feed the characters one by one\n",
    "    for curr_word, next_word in zip(decoder_batch, decoder_batch[1:]):\n",
    "        #print(curr_word, next_word)\n",
    "        # -----------------------------------------------------------------\n",
    "        # teacher forcing logic\n",
    "        # -----------------------------------------------------------------\n",
    "        use_label = True if random.random() < teacher_ratio else False\n",
    "        if use_label:\n",
    "            decoder_input = curr_word\n",
    "        # -----------------------------------------------------------------\n",
    "        # encode tokens\n",
    "        # -----------------------------------------------------------------\n",
    "        predicted_out, ht = model.decode(decoder_input, ht)\n",
    "        decoder_input = F.argmax(predicted_out, axis=1)\n",
    "        #print(decoder_input)\n",
    "        # -----------------------------------------------------------------\n",
    "        # compute loss\n",
    "        # -----------------------------------------------------------------\n",
    "        loss_arr = F.softmax_cross_entropy(predicted_out, next_word,\n",
    "                                           class_weight=model.mask_pad_id)\n",
    "        #print(loss_arr.data.tolist())\n",
    "        loss += loss_arr\n",
    "        # -----------------------------------------------------------------\n",
    "    #print(loss, loss / (y.shape[0]-2), y.shape)\n",
    "    return loss.data.tolist(), (loss / (y.shape[0]-1)).data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utt_data(eg_utt, curr_set='fisher_dev'):\n",
    "    # get shape\n",
    "    if \"train\" in curr_set:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "        play_audio = False\n",
    "    else:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "        play_audio = True\n",
    "        \n",
    "    eg_utt_bucket = -1\n",
    "    for i, bucket in enumerate(bucket_dict[curr_set][\"buckets\"]):\n",
    "        if eg_utt in bucket:\n",
    "            eg_utt_bucket = i\n",
    "            #print(\"found\")\n",
    "        # end if\n",
    "    # end for\n",
    "    #print(\"found in bucket : {0:d}\".format(eg_utt_bucket))\n",
    "    width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "    utt_list = [eg_utt]\n",
    "    \n",
    "    \n",
    "    batch_data = get_batch(map_dict[curr_set], \n",
    "                           enc_key,\n",
    "                           dec_key,\n",
    "                           utt_list,\n",
    "                           vocab_dict,\n",
    "                           (eg_utt_bucket+1) * width_b,\n",
    "                           200,\n",
    "                           input_path=local_input_path)\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"./sp2enw_mel-80_vocab-nltk/sp_1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_out_dim = rnn_in_units =  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/project/lowres/work/miniconda3/envs/chainer3/lib/python3.6/site-packages/chainer/utils/experimental.py:104: FutureWarning: chainer.links.normalization.layer_normalization.py is experimental. The interface can change in the future.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using SGD optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "model found = \n",
      "./sp2enw_mel-80_vocab-nltk/sp_1.0/seq2seq_83.model\n",
      "finished loading ..\n",
      "optimizer found = ./sp2enw_mel-80_vocab-nltk/sp_1.0/train.opt\n",
      "finished loading optimizer ...\n"
     ]
    }
   ],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "loading dict: fbanks_80dim_nltk/map.dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3977/3977 [00:00<00:00, 484553.29it/s]\n",
      "100%|██████████| 3960/3960 [00:00<00:00, 351984.48it/s]\n",
      "  0%|          | 0/3641 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dict: fbanks_80dim_nltk/train_vocab.dict\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "loading info_dict from=fbanks_80dim_nltk/info.dict\n",
      "--------------------------------------------------\n",
      "creating buckets for: fisher_dev\n",
      "creating buckets for key: sp\n",
      "creating buckets for: fisher_dev2\n",
      "creating buckets for key: sp\n",
      "creating buckets for: fisher_test\n",
      "creating buckets for key: sp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3641/3641 [00:00<00:00, 492675.45it/s]\n",
      " 36%|███▌      | 49338/138720 [00:00<00:00, 493235.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating buckets for: fisher_train\n",
      "creating buckets for key: sp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138720/138720 [00:00<00:00, 547085.19it/s]\n",
      "100%|██████████| 3803/3803 [00:00<00:00, 498217.71it/s]\n",
      "100%|██████████| 1824/1824 [00:00<00:00, 506381.42it/s]\n",
      "100%|██████████| 14294/14294 [00:00<00:00, 503627.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating buckets for: callhome_devtest\n",
      "creating buckets for key: sp\n",
      "creating buckets for: callhome_evltest\n",
      "creating buckets for key: sp\n",
      "creating buckets for: callhome_train\n",
      "creating buckets for key: sp\n",
      "--------------------------------------------------\n",
      "saving info dict in: fbanks_80dim_nltk/buckets_sp.dict\n",
      "all done ...\n",
      "loading dict: fbanks_80dim_nltk/buckets_sp.dict\n",
      "--------------------------------------------------\n",
      "utterances in fisher_dev = 3979\n",
      "utterances in fisher_dev2 = 3961\n",
      "utterances in fisher_test = 3641\n",
      "utterances in fisher_train = 138819\n",
      "utterances in callhome_devtest = 3966\n",
      "utterances in callhome_evltest = 1829\n",
      "utterances in callhome_train = 15080\n",
      "vocab size for sp = 0\n",
      "vocab size for en_w = 17834\n"
     ]
    }
   ],
   "source": [
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict = get_data_dicts(m_cfg)\n",
    "batch_size = {'max': 96, 'med': 128, 'min': 256, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, m_cfg['max_en_pred']\n",
    "# min_len, max_len = 0, 5\n",
    "displayN = 50\n",
    "m_dict=map_dict[dev_key]\n",
    "# wavs_path = os.path.join(m_cfg['data_path'], \"wavs\")\n",
    "wavs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"wavs\")\n",
    "v_dict = vocab_dict['en_w']\n",
    "key = m_cfg['dev_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"/afs/inf.ed.ac.uk/group/project/lowres/work/speech2text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load google refs and preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_s2t_refs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"google_s2t_refs.dict\")\n",
    "google_s2t_hyps_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"google_s2t_hyps.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_s2t_hyps = pickle.load(open(google_s2t_hyps_path, \"rb\"))\n",
    "google_hyp_r0 = google_s2t_hyps['fisher_dev_r0']\n",
    "\n",
    "google_s2t_refs = pickle.load(open(google_s2t_refs_path, \"rb\"))\n",
    "google_dev_ref_0 = google_s2t_refs['fisher_dev_ref_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s2t_refs = pickle.load(open(os.path.join(cfg_path, \"model_s2t_refs.dict\"), \"rb\"))\n",
    "model_s2t_hyps = pickle.load(open(os.path.join(cfg_path, \"model_s2t_hyps.dict\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translate_probs(eg_utt, curr_set=\"fisher_dev\", display_limit=5, display_probs=True):\n",
    "    if \"train\" in curr_set:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])\n",
    "        play_audio = False\n",
    "    else:\n",
    "        local_input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "        play_audio = True\n",
    "        \n",
    "    eg_utt_bucket = -1\n",
    "    for i, bucket in enumerate(bucket_dict[curr_set][\"buckets\"]):\n",
    "        if eg_utt in bucket:\n",
    "            eg_utt_bucket = i\n",
    "            #make_predprint(\"found\")\n",
    "        # end if\n",
    "    # end for\n",
    "    #print(\"found in bucket : {0:d}\".format(eg_utt_bucket))\n",
    "    width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "    utt_list = [eg_utt]\n",
    "    \n",
    "    \n",
    "    batch_data = get_batch(map_dict[curr_set], \n",
    "                           enc_key,\n",
    "                           dec_key,\n",
    "                           utt_list,\n",
    "                           vocab_dict,\n",
    "                           (eg_utt_bucket+1) * width_b,\n",
    "                           200,\n",
    "                           input_path=local_input_path)\n",
    "    \n",
    "    with chainer.using_config('train', False):\n",
    "        cuda.get_device(t_cfg['gpuid']).use()\n",
    "        preds, _, probs_str = make_pred(eg_utt, X=batch_data['X'], display_limit=display_limit)\n",
    "        #preds, _ = make_pred(eg_utt, X=batch_data['X'][:,-150:,:], display_limit=10)\n",
    "        loss_val = 0.0\n",
    "    \n",
    "    display_words(map_dict[curr_set], v_dict, \n",
    "                  preds.tolist(), \n",
    "                  utt_list, dec_key, \n",
    "                  key, \n",
    "                  play_audio=play_audio, \n",
    "                  displayN=displayN)\n",
    "    \n",
    "    if display_probs:\n",
    "        print(probs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_utts_with_word(word, set_key=\"fisher_dev\", show_max_found=10):\n",
    "    total_found = 0\n",
    "    out_str = []\n",
    "    for utt, entry in map_dict[set_key].items():\n",
    "        if \"train\" in set_key:\n",
    "            words_in_utt = \" \".join([w.decode() for w in entry['en_w']])\n",
    "        else:\n",
    "            words_in_utt = \" \".join([w.decode() for w in entry['en_w'][0]])\n",
    "        es_words_in_utt = \" \".join([w.decode() for w in entry['es_w']])        \n",
    "        #if \"puerto\" in words_in_utt:\n",
    "        if word in words_in_utt:\n",
    "            out_str.append(\"{0:s} | {1:s} | {2:s}\".format(utt, words_in_utt, es_words_in_utt))\n",
    "            total_found += 1\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"total instances found = {0:d}\".format(total_found))\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\n\".join(out_str[:show_max_found]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_utts_with_word(\"puerto\", set_key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051023_232057_325_fsp-A-3\"\n",
    "generate_translate_probs(eg_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_fname = os.path.join(m_cfg['model_dir'], \"{0:s}_probs.json\".format(eg_utt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eg_utt = \"20051017_234550_276_fsp-B-34\"\n",
    "print(check_loss(eg_utt, curr_set='fisher_dev'))\n",
    "generate_translate_probs(eg_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict['en_w']['i2w'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051026_180724_341_fsp-A-26\"\n",
    "generate_translate_probs(eg_utt)\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051017_234550_276_fsp-A-13\"\n",
    "generate_translate_probs(eg_utt)\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051018_210220_279_fsp-A-26\"\n",
    "generate_translate_probs(eg_utt)\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_utts_with_word(\"mhm\", set_key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051019_210146_289_fsp-A-54\"\n",
    "generate_translate_probs(eg_utt, curr_set='fisher_dev')\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_loss = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051017_220530_275_fsp-B-21.npy\"\n",
    "try:\n",
    "    check_loss(eg_utt, curr_set='fisher_dev')\n",
    "except:\n",
    "    print(\"{0:s} not found\".format(utt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "utt_loss = {}\n",
    "for utt in tqdm(map_dict['fisher_dev'], ncols=50):\n",
    "    if utt not in utt_loss:\n",
    "        try:\n",
    "            loss = check_loss(utt, curr_set='fisher_dev')\n",
    "            utt_loss[utt] = loss.data.tolist()\n",
    "        except:\n",
    "            print(\"{0:s} not found\".format(utt))\n",
    "    #     print(utt, \"{0:5.3f}\".format(loss.data.tolist()))\n",
    "#     i += 1\n",
    "#     if i > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize by length\n",
    "utt_loss_normalize = {}\n",
    "for utt in tqdm(utt_loss, ncols=50):\n",
    "    utt_loss_normalize[utt] = utt_loss[utt] / (len(map_dict['fisher_dev'][utt]['en_w'][0])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(utt_loss.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(utt_loss, open(os.path.join(cfg_path, \"dev_utts_loss.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(utt_loss_normalize, open(os.path.join(cfg_path, \"dev_utts_loss_normalized.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_loss = pickle.load(open(os.path.join(cfg_path, \"dev_utts_loss.dict\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_loss_normalize = pickle.load(open(os.path.join(cfg_path, \"dev_utts_loss_normalized.dict\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_utts = sorted(utt_loss_normalize.items(), reverse=True, key=lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BAD_UTTS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = '20051026_180724_341_fsp-A-26'\n",
    "generate_translate_probs(eg_utt, curr_set='fisher_dev')\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_utts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_utts[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*utt_loss_normalize.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if i < 1 else 0 for i in y]), sum([1 if i > 5 else 0 for i in y]), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev utts - avg loss per word in utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8,5)\n",
    "\n",
    "ax = sns.distplot(y, kde=False, rug=False, ax=ax, color=tableau20[0]);\n",
    "ax.set_xlabel(\"dev utts - avg loss per word in utt\", size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (u, l) in enumerate(sorted(utt_loss_normalize.items(), reverse=True, key=lambda t: t[1])[:50]):\n",
    "    print(\"-\"*80)\n",
    "    print(\"{0:d}\".format(i))\n",
    "    print(\"-\"*80)\n",
    "    generate_translate_probs(u, curr_set='fisher_dev', display_limit=3, display_probs=True)\n",
    "    loss_v, loss_by_w = check_loss(u, curr_set='fisher_dev')\n",
    "    #print(\"{0:20s} ||| {1:5.2f} ||| {2:5.2f} ||| {3:5.2f}\".format(u, l, loss_v, loss_by_w))\n",
    "    print(\"{0:20s} ||| {3:5.2f}\".format(u, l, loss_v, loss_by_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To share:\n",
    "Several utterrance labels have typos, giving a misleading signal about the prediction quality\n",
    "\n",
    "\"20051017_180712_270_fsp-B-62\"\n",
    "dogs barking\n",
    "\n",
    "\"20051018_210220_279_fsp-A-71\"\n",
    "monopoly money -- monopoly occurs only 5 times in the train set, and never in the context of the game\n",
    "\n",
    "\"20051017_220530_275_fsp-B-61\"\n",
    "the decode probabilities show that maybe beam decoding (? or probably language model) will help catch up to Google. the Google model outputs Texas. We have Texas as the second most probable word as per the acoustic model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[ .759,  0.141,  .053]], dtype=np.float32)\n",
    "t = np.array([1]).astype('i')\n",
    "y = F.softmax_cross_entropy(x, t)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_utt_data(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = batch_data['X'], batch_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[vocab_dict['en_w']['i2w'][i] for i in xp.asnumpy(y.data[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20050908_182943_22_fsp-A-1',\n",
       " '20050908_182943_22_fsp-B-1',\n",
       " '20050908_182943_22_fsp-A-2',\n",
       " '20050908_182943_22_fsp-B-2',\n",
       " '20050908_182943_22_fsp-A-3']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_dict['fisher_train']['buckets'][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_list = bucket_dict['fisher_train']['buckets'][0][:5]\n",
    "width_b = bucket_dict['fisher_train']['width_b']\n",
    "local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_batch(map_dict[\"fisher_train\"], \n",
    "                       enc_key,\n",
    "                       dec_key,\n",
    "                       utt_list,\n",
    "                       vocab_dict,\n",
    "                       (0+1) * width_b,\n",
    "                       200,\n",
    "                       input_path=local_input_path)\n",
    "    \n",
    "X, y = batch_data['X'], batch_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 7), (5, 150, 80))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input\n",
    "model.forward_enc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = F.swapaxes(y, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'next_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-041ced74308d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'next_word' is not defined"
     ]
    }
   ],
   "source": [
    "next_word.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros(shape=(len(next_word),10), dtype='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0,[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = pickle.load(open(\"../speech2text/fbanks_80dim_nltk/sim.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1044, 2477, 13760, 3510, 842, 1148, 3382, 3305, 1345, 2846]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_dict['i'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([1 1 1 1 1])\n",
      "[1, 1, 1, 1, 1]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "variable([4 4 4 4 5])\n",
      "[4, 4, 4, 4, 5]\n",
      "[1 1 0 1]\n",
      "[1 1 0 1]\n",
      "[1 1 0 1]\n",
      "[1 1 0 1]\n",
      "[0 0 0 0]\n",
      "variable([2 2 2 2 6])\n",
      "[2, 2, 2, 2, 6]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "variable([0 0 0 0 7])\n",
      "[0, 0, 0, 0, 7]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "variable([0 0 0 0 8])\n",
      "[0, 0, 0, 0, 8]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "variable([0 0 0 0 9])\n",
      "[0, 0, 0, 0, 9]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "variable([0 0 0 0 2])\n",
      "[0, 0, 0, 0, 2]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for next_word in y_t:\n",
    "    print(next_word)\n",
    "    t = np.zeros(shape=(len(next_word.data), 17000), dtype='i')\n",
    "    print(next_word.data.tolist())\n",
    "    for i,w in enumerate(next_word.data.tolist()):\n",
    "        t[i,sim_dict['i'][w]] = 1\n",
    "        print(t[i,[4,1044, 1045, 2477]])\n",
    "    #print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = xp.zeros((5,10)).astype('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 2\n",
      "2 3\n",
      "3 4\n",
      "4 5\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(next_word):\n",
    "    print(i, w)\n",
    "    labels[i,[w]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-96cdfeeece12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "next_word.data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1,17000).astype('f')\n",
    "x = np.zeros((1,17000)).astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,2] = 10.0\n",
    "x[0,0] = -10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.7409687, -9.7409687, -9.7409687, ..., -9.7409687, -9.7409687,\n",
       "        -9.7409687]], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(x).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0 * np.ones((1,17000), dtype='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0,2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-2.,  3., -2.],\n",
       "        [-2.,  3., -2.]], dtype=float32), array([[-1, -1, -1],\n",
       "        [ 0,  1,  0]], dtype=int32))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([[-0.        ,  0.        , -0.        ],\n",
       "          [ 0.126928  ,  0.04858735,  0.126928  ]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid_cross_entropy(x, t, reduce='no', normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable([[-0.        ,  0.        , -0.        ],\n",
       "          [ 0.126928  ,  0.04858735,  0.126928  ]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid_cross_entropy(x, t, reduce='no', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable(0.10081445425748825)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid_cross_entropy(x, t, reduce='mean', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable(0.15122167766094208)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid_cross_entropy(x, t, reduce='mean', normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable(0.10081445425748825)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid_cross_entropy(x, t, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-2.0, 3.0, -2.0], [-2.0, 3.0, -2.0]]).astype('f')\n",
    "t = np.array([[-1, -1, -1], [0, 1, 0]]).astype('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable(0.10081445425748825)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid_cross_entropy(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((1,10), dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,[2]] = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,list(range(2))+list(range(3,10))] = -.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.2,  -0.2,  10. ,  -0.2,  -0.2,  -0.2,  -0.2,  -0.2,  -0.2,\n",
       "         -0.2]], dtype=float32)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros((1,10), dtype=\"i\")\n",
    "t[0,5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable(1.558329463005066)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid_cross_entropy(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(variable([[ -0.54293811,  -0.54772395,  13.52175999, ...,  -0.53887445,\n",
       "             -0.40857357,  -0.45140707]]), array(2, dtype=int64))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_out[:1], xp.argmax(predicted_out[:1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 0 * xp.ones(predicted_out[:1].shape).astype('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 17834)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = xp.zeros(predicted_out[:1].shape).astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 17834)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0,[2,5]] = 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0,[2,5]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable(0.6930694580078125)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid_cross_entropy(t, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 1, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,  50.,   0.,   0.,  50.,   0.,   0.,   0.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable(0.6931415796279907)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid_cross_entropy(F.softmax(predicted_out[:1]), xp.expand_dims(labels, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([2]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.where(labels > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([2]))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.where(labels > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([b'rico', b'puerto', b'puertorican', b'ricardo', b'rica'],\n",
       " [4, 1044, 2477, 13760, 3510, 842, 1148, 3382, 3305, 1345, 2846])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_dict['w'][b'rico'], sim_dict['i'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3305]), [2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.random.choice(sim_dict['i'][4], 1), sim_dict['i'][w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 4 5] 0\n",
      "4\n",
      "[4 4 4 4 5] 1\n",
      "4\n",
      "[4 4 4 4 5] 2\n",
      "4\n",
      "[4 4 4 4 5] 3\n",
      "4\n",
      "[4 4 4 4 5] 4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(t_alt)):\n",
    "    print(t_alt, i)\n",
    "    print(t_alt[i])\n",
    "    #t_alt[i] = xp.random.sample(sim_dict['i'][t_alt[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 4 5]\n",
      "[1044]\n",
      "4 <class 'cupy.core.core.ndarray'> 4\n",
      "[3510]\n",
      "4 <class 'cupy.core.core.ndarray'> 4\n",
      "[842]\n",
      "4 <class 'cupy.core.core.ndarray'> 4\n",
      "[2477]\n",
      "4 <class 'cupy.core.core.ndarray'> 4\n",
      "[5]\n",
      "5 <class 'cupy.core.core.ndarray'> 5\n"
     ]
    }
   ],
   "source": [
    "print(t_alt)\n",
    "for i in range(len(t_alt)):\n",
    "    print(xp.random.choice(sim_dict['i'][int(t_alt[i])],1))\n",
    "    print(t_alt[i],type(t_alt[i]), int(t_alt[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([1 1 1 1 1]) variable([4 4 4 4 5])\n",
      "[4 4 4 4 5]\n",
      "[2477  842  842 1148    5]\n",
      "softmax cross entropy: variable(6.027211666107178) sigmoid: 8.526057243347168\n",
      "variable([4 4 4 4 5]) variable([2 2 2 2 6])\n",
      "[2 2 2 2 6]\n",
      "[2 2 2 2 6]\n",
      "softmax cross entropy: variable(0.9709686636924744) sigmoid: 0.9709686636924744\n",
      "variable([2 2 2 2 6]) variable([0 0 0 0 7])\n",
      "[0 0 0 0 7]\n",
      "[ 0  0  0  0 29]\n",
      "softmax cross entropy: variable(10.59409236907959) sigmoid: 11.633125305175781\n",
      "variable([0 0 0 0 7]) variable([0 0 0 0 8])\n",
      "[0 0 0 0 8]\n",
      "[0 0 0 0 8]\n",
      "softmax cross entropy: variable(10.959638595581055) sigmoid: 10.959638595581055\n",
      "variable([0 0 0 0 8]) variable([0 0 0 0 9])\n",
      "[0 0 0 0 9]\n",
      "[0 0 0 0 9]\n",
      "softmax cross entropy: variable(11.310018539428711) sigmoid: 11.310018539428711\n",
      "variable([0 0 0 0 9]) variable([0 0 0 0 2])\n",
      "[0 0 0 0 2]\n",
      "[0 0 0 0 2]\n",
      "softmax cross entropy: variable(11.700035095214844) sigmoid: 11.700035095214844\n",
      "55.09984588623047 13.774961471557617\n"
     ]
    }
   ],
   "source": [
    "decoder_batch = y_t\n",
    "batch_size = decoder_batch.shape[1]\n",
    "loss = 0\n",
    "# ---------------------------------------------------------------------\n",
    "# initialize hidden states as a zero vector\n",
    "# ---------------------------------------------------------------------\n",
    "a_units = model.m_cfg['attn_units']\n",
    "ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "# ---------------------------------------------------------------------\n",
    "decoder_input = decoder_batch[0]\n",
    "# for all sequences in the batch, feed the characters one by one\n",
    "for curr_word, next_word in zip(decoder_batch, decoder_batch[1:]):\n",
    "    print(curr_word, next_word)\n",
    "    decoder_input = curr_word\n",
    "    # -----------------------------------------------------------------\n",
    "    # encode tokens\n",
    "    # -----------------------------------------------------------------\n",
    "    predicted_out, ht = model.decode(decoder_input, ht)\n",
    "    decoder_input = F.argmax(predicted_out, axis=1)\n",
    "    #print(decoder_input)\n",
    "    # -----------------------------------------------------------------\n",
    "    # compute loss\n",
    "    # -----------------------------------------------------------------\n",
    "    t_alt = xp.copy(next_word.data)\n",
    "    print(t_alt)\n",
    "    for i in range(len(t_alt)):\n",
    "        t_alt[i] = xp.random.choice(sim_dict['i'][int(t_alt[i])],1)\n",
    "        #print(t[i,[4,1044, 1045, 2477]])\n",
    "    print(t_alt)\n",
    "\n",
    "#     t = xp.zeros(shape=predicted_out.shape, dtype='i')\n",
    "#     print(next_word.data.tolist())\n",
    "#     print(next_word.shape)\n",
    "#     for i,w in enumerate(next_word.data.tolist()):\n",
    "#         if w == PAD_ID:\n",
    "#             t[i,:] = -1\n",
    "#         else:\n",
    "#             t[i,sim_dict['i'][w]] = 1\n",
    "#         #print(t[i,[4,1044, 1045, 2477]])\n",
    "#     loss_arr = F.sigmoid_cross_entropy(predicted_out, t, normalize=True)\n",
    "    loss_arr = F.softmax_cross_entropy(predicted_out, t_alt, normalize=True)\n",
    "    print(\"softmax cross entropy:\", F.softmax_cross_entropy(predicted_out, next_word), \"sigmoid:\", loss_arr.data.tolist())\n",
    "    loss += loss_arr\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "#print(loss, loss / (y.shape[0]-2), y.shape)\n",
    "print(loss.data.tolist(), (loss / (y.shape[0]-1)).data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = X.shape[0]\n",
    "# encode input\n",
    "model.forward_enc(X)\n",
    "# ---------------------------------------------------------------------\n",
    "# initialize decoder LSTM to final encoder state\n",
    "# ---------------------------------------------------------------------\n",
    "model.set_decoder_state()\n",
    "# ---------------------------------------------------------------------\n",
    "y = F.swapaxes(y, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = cuda.cupy if model.gpuid >= 0 else np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_batch = y \n",
    "batch_size = decoder_batch.shape[1]\n",
    "loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_word, next_word in zip(decoder_batch, decoder_batch[1:]):\n",
    "    print(curr_word, next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_x = Variable(xp.asarray([[0,1,0]], dtype=xp.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_y = Variable(xp.asarray([[0,1,0]], dtype=xp.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-10, 5.0, -10.0]]).astype('f')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[0, 1, 0]]).astype('i')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax_cross_entropy(x, np.array([1]).astype('i'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(x, t, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# initialize hidden states as a zero vector\n",
    "# ---------------------------------------------------------------------\n",
    "a_units = model.m_cfg['attn_units']\n",
    "ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "# ---------------------------------------------------------------------\n",
    "decoder_input = decoder_batch[0]\n",
    "# for all sequences in the batch, feed the characters one by one\n",
    "for curr_word, next_word in zip(decoder_batch, decoder_batch[1:]):\n",
    "    # -----------------------------------------------------------------\n",
    "    # teacher forcing logic\n",
    "    # -----------------------------------------------------------------\n",
    "    use_label = True if random.random() < teacher_ratio else False\n",
    "    if use_label:\n",
    "        decoder_input = curr_word\n",
    "    # -----------------------------------------------------------------\n",
    "    # encode tokens\n",
    "    # -----------------------------------------------------------------\n",
    "    predicted_out, ht = model.decode(decoder_input, ht)\n",
    "    decoder_input = F.argmax(predicted_out, axis=1)\n",
    "    # -----------------------------------------------------------------\n",
    "    # compute loss\n",
    "    # -----------------------------------------------------------------\n",
    "    loss_arr = F.softmax_cross_entropy(predicted_out, next_word,\n",
    "                                       class_weight=model.mask_pad_id)\n",
    "    loss += loss_arr\n",
    "    # -----------------------------------------------------------------\n",
    "return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare BLEU scores at utterance level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051023_232057_325_fsp-A-3\"\n",
    "print(sentence_bleu([google_dev_ref_0[eg_utt]], google_hyp_r0[eg_utt], smoothing_function=smooth_fun.method2))\n",
    "print(sentence_bleu([model_s2t_refs[eg_utt]], model_s2t_hyps[eg_utt], smoothing_function=smooth_fun.method2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051019_190221_288_fsp-B-1\"\n",
    "print(sentence_bleu([google_dev_ref_0[eg_utt]], google_hyp_r0[eg_utt], smoothing_function=smooth_fun.method2))\n",
    "print(sentence_bleu([model_s2t_refs[eg_utt]], model_s2t_hyps[eg_utt], smoothing_function=smooth_fun.method2))\n",
    "print(google_dev_ref_0[eg_utt], google_hyp_r0[eg_utt])\n",
    "print(model_s2t_refs[eg_utt], model_s2t_hyps[eg_utt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"haha\")\n",
    "dev_utts = list(model_s2t_refs.keys())\n",
    "random.shuffle(dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"google beats model by factor of 2\")\n",
    "\n",
    "count = 0\n",
    "# print(\"-\"*80)\n",
    "# print(\"{0:>5s} ||| {1:30s} ||| {2:>15s} || {3:>15s}\".format(\"sn\", \"utt\", \"google utt bleu\", \"model utt bleu\"))\n",
    "# print(\"-\"*80)\n",
    "for utt in dev_utts:\n",
    "    google_utt_bleu = sentence_bleu([google_dev_ref_0[utt]], google_hyp_r0[utt], smoothing_function=smooth_fun.method2)\n",
    "    model_utt_bleu = sentence_bleu([model_s2t_refs[utt]], model_s2t_hyps[utt], smoothing_function=smooth_fun.method2)\n",
    "    if google_utt_bleu >= (2 * model_utt_bleu) and google_utt_bleu >= 0.5:\n",
    "        count += 1\n",
    "        #print(\"{0:5d} ||| {1:30s} ||| {2:15.2f} || {3:15.2f}\".format(count, utt, google_utt_bleu, model_utt_bleu))\n",
    "        print(\"-\"*80)\n",
    "        print(count)\n",
    "        print(\"-\"*80)\n",
    "        display_pp = PrettyTable([\"cat\",\"sent\"], hrules=True)\n",
    "        display_pp.align = \"l\"\n",
    "        display_pp.header = False\n",
    "        display_pp.add_row([\"en ref\", textwrap.fill(\" \".join(model_s2t_refs[utt]),50)])\n",
    "        display_pp.add_row([\"model pred\", textwrap.fill(\" \".join(model_s2t_hyps[utt]),50)])\n",
    "        display_pp.add_row([\"model utt bleu\", \"{0:.2f}\".format(model_utt_bleu)])\n",
    "        display_pp.add_row([\"google pred\", textwrap.fill(\" \".join(google_hyp_r0[utt]),50)])\n",
    "        display_pp.add_row([\"google utt bleu\", \"{0:.2f}\".format(google_utt_bleu)])\n",
    "        print(display_pp)\n",
    "        #play_utt(utt, map_dict['fisher_dev'])\n",
    "#         generate_translate_probs(utt)\n",
    "#     if count > 100:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
