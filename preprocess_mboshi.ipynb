{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import bisect\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import random\n",
    "import math\n",
    "import scipy as sp\n",
    "import scipy.io.wavfile as wav\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator, \\\n",
    "     FormatStrFormatter, AutoMinorLocator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOX = \"sox\"\n",
    "SPH2PIPE = \"../installs/wav/sph2pipe_v2.5/sph2pipe\"\n",
    "FEACALC = \"../installs/wav/icsi-scenic-tools-20120105/feacalc-0.92/feacalc\"\n",
    "\n",
    "STANDFEAT = \"../installs/origZRTools/plebdisc/standfeat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_path = \"/afs/inf.ed.ac.uk/group/project/ast/work/corpora/mboshi-french-parallel-corpus/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wavs_path = os.path.join(mboshi_path, \"train_wav\")\n",
    "test_wavs_path = os.path.join(mboshi_path, \"dev_wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_ids = [w.replace(\".wav\", \"\") for w in os.listdir(train_wavs_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [w.replace(\".wav\", \"\") for w in os.listdir(test_wavs_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4616, 4616)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dev_ids), len(set(train_dev_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"haha\")\n",
    "np.random.seed(10)\n",
    "dev_ids = sorted(list(set(np.random.choice(train_dev_ids, 200, replace=False))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_ids), len(set(dev_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = sorted(list(set(train_dev_ids) - set(dev_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4416, 200, 514)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ids), len(dev_ids), len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_out_str(out_str):\n",
    "    out_str = out_str.replace(\"=\", \" \")\n",
    "    out_str = out_str.replace(\",\", \" \")\n",
    "    out_str = out_str.replace(\".\", \" \")\n",
    "    out_str = out_str.replace(\",\", \" \")\n",
    "    out_str = out_str.replace(\"`\", \"\")\n",
    "    out_str = out_str.replace('\"', '')\n",
    "    out_str = out_str.replace('¿', '')\n",
    "    out_str = out_str.replace(\"''\", \"\")\n",
    "    out_str = out_str.replace(\":\", \"\")\n",
    "    out_str = out_str.replace(\"!\", \" \")\n",
    "    out_str = out_str.replace(\"|\", \"\")\n",
    "    out_str = out_str.replace(\";\", \" \")\n",
    "    out_str = out_str.replace(\"‐\", \" \")\n",
    "    out_str = out_str.replace(\"-\", \" \")\n",
    "    \n",
    "\n",
    "    # for BPE\n",
    "    out_str = out_str.replace(\"@@ \", \"\")\n",
    "    out_str = out_str.replace(\"@@\", \"\")\n",
    "\n",
    "    out_str = out_str.strip().lower()\n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(text_path):\n",
    "    all_words = []\n",
    "    all_chars = []\n",
    "    utt2words = {}\n",
    "    with open(text_path, \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for line in in_f:\n",
    "            u, t = line.split(\" \", 1)\n",
    "            t = clean_out_str(t)\n",
    "            utt2words[u] = t\n",
    "            curr_words = [w.strip() for w in t.split()]\n",
    "            all_words.extend(curr_words)\n",
    "            all_chars.extend([c for c in t])\n",
    "        # end for line\n",
    "    # end with\n",
    "    fr_chars = Counter(all_chars)\n",
    "#     fr_chars = {}\n",
    "#     for w in all_words:\n",
    "#         for c in w:\n",
    "#             if c not in fr_chars:\n",
    "#                 fr_chars[c] = 1\n",
    "#             else:\n",
    "#                 fr_chars[c] += 1\n",
    "    return utt2words, Counter(all_words), fr_chars\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_train_dev_text, mboshi_train_dev_words, mboshi_train_dev_chars = read_text(os.path.join(mboshi_path, \n",
    "                                                                                               \"train.fr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36030, 5116)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mboshi_train_dev_words.values()), len(mboshi_train_dev_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189396, 45)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mboshi_train_dev_chars.values()), len(mboshi_train_dev_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_test_text, mboshi_test_words, mboshi_test_chars = read_text(os.path.join(mboshi_path, \"dev.fr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3954, 1200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mboshi_test_words.values()), len(mboshi_test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(mboshi_train_dev_text.keys()) - set(train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_id_file(ids, out_fname):\n",
    "    with open(out_fname, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for i in ids:\n",
    "            out_f.write(\"{0:s}\\n\".format(i))\n",
    "        # end for\n",
    "    # end with        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_file(ids, text_dict, out_fname):\n",
    "    with open(out_fname, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for i in ids:\n",
    "            out_f.write(\"{0:s}\\n\".format(text_dict[i]))\n",
    "        # end for\n",
    "    # end with        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_sets = {\"mboshi_train\": train_ids, \"mboshi_dev\": dev_ids, \"mboshi_test\": test_ids}\n",
    "text_sets = {\"mboshi_train\": mboshi_train_dev_text, \n",
    "             \"mboshi_dev\": mboshi_train_dev_text, \n",
    "             \"mboshi_test\": mboshi_test_text}\n",
    "set_paths = {k: os.path.join(\"./mboshi\", \"{0:s}.fr\".format(k)) for k in id_sets}\n",
    "set_id_paths = {k: os.path.join(\"./mboshi\", \"{0:s}.ids\".format(k)) for k in id_sets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_paths, set_id_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in id_sets:\n",
    "    write_id_file(id_sets[i], set_id_paths[i])\n",
    "    write_text_file(id_sets[i], text_sets[i], set_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bpe_text(text_path, ids):\n",
    "    all_words = []\n",
    "    utt2words = {}\n",
    "    with open(text_path, \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for u, line in tqdm(zip(ids, in_f)):\n",
    "            t = line.split()\n",
    "            utt2words[u] = t\n",
    "            all_words.extend(t)\n",
    "        # end for line\n",
    "    # end with\n",
    "    return utt2words, dict(Counter(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_train_bpe_text, mboshi_train_bpe_words = read_bpe_text(os.path.join(\"mboshi\", \n",
    "                                                              \"mboshi_train.BPE_1000.fr\"),\n",
    "                                                              train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_dev_bpe_text, mboshi_dev_bpe_words = read_bpe_text(os.path.join(\"mboshi\", \n",
    "                                                          \"mboshi_dev.BPE_1000.fr\"),\n",
    "                                                           dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_train_bpe_text[train_ids[4]], mboshi_train_dev_text[train_ids[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mboshi_train_bpe_words), len(mboshi_dev_bpe_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_text_sets = {\"mboshi_train\": mboshi_train_bpe_text, \n",
    "             \"mboshi_dev\": mboshi_dev_bpe_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in mboshi_train_dev_chars:\n",
    "    print(c, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_vocab(words):\n",
    "    out = {\"w2i\":{}, \"i2w\":{}, \"freq\":{}}\n",
    "    START_VOCAB = [PAD, GO, EOS, UNK]\n",
    "    for w in START_VOCAB:\n",
    "        out['w2i'][w] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][w] = 1\n",
    "    #for w in words_list['words']:\n",
    "    sorted_w = sorted(words.items(), reverse=True, key=lambda t: t[1])\n",
    "    for w in sorted_w:\n",
    "        encoded_word = w[0].encode()\n",
    "        out[\"w2i\"][encoded_word] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][encoded_word] = w[1]\n",
    "\n",
    "    out[\"i2w\"] = {val:key for key, val in out[\"w2i\"].items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab[\"en_c\"] = create_new_vocab(mboshi_train_dev_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab[\"en_w\"] = create_new_vocab(mboshi_train_dev_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab[\"bpe_w\"] = create_new_vocab(mboshi_train_bpe_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab[\"en_c\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fr_vocab, open(\"./mfcc_13dim/mboshi_train_vocab.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mboshi_map():\n",
    "    oov = {}\n",
    "    mboshi_map = {}\n",
    "    \n",
    "    for id_set in id_sets:\n",
    "        print(id_set)\n",
    "        mboshi_map[id_set] = {}\n",
    "        oov[id_set] = []\n",
    "        for i in id_sets[id_set]:\n",
    "            mboshi_map[id_set][i] = {}            \n",
    "            words = text_sets[id_set][i].split()\n",
    "            encoded_words = [w.encode() for w in words]\n",
    "            encoded_chars = [c.encode() for c in text_sets[id_set][i]]\n",
    "            \n",
    "            mboshi_map[id_set][i][\"en_w\"] = encoded_words\n",
    "            mboshi_map[id_set][i][\"en_c\"] = encoded_chars\n",
    "            \n",
    "            if id_set in bpe_text_sets:\n",
    "                bpe_tokens = [w.encode() for w in bpe_text_sets[id_set][i]]\n",
    "                \n",
    "            else:\n",
    "                bpe_tokens = []\n",
    "            mboshi_map[id_set][i][\"bpe_w\"] = bpe_tokens\n",
    "            \n",
    "            for c in bpe_tokens:\n",
    "                if c not in fr_vocab[\"bpe_w\"][\"w2i\"]:\n",
    "                    oov[id_set].append(c)\n",
    "    return mboshi_map, oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_map, oov = create_mboshi_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mboshi_map[\"mboshi_dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mboshi_map, open(\"./mfcc_13dim/mboshi_map.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = en_path = os.path.join(ainu_path, \"wav\")\n",
    "en_path = os.path.join(ainu_path, \"encl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [w.replace(\".wav\", \"\") for w in os.listdir(wav_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'0.000'.split(\".\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt2spk = {k: k.split(\".\",1)[0] for k in ids}\n",
    "spk2utt = {}\n",
    "\n",
    "for k in ids:\n",
    "    spk_key = k.split(\".\",1)[0]\n",
    "    if spk_key in spk2utt:\n",
    "        spk2utt[spk_key].append(k)\n",
    "    else:\n",
    "        spk2utt[spk_key] = []\n",
    "        \n",
    "print(\"# of utts = {0:d}\".format(len(ids)))\n",
    "print(\"# of narratives = {0:d}\".format(len(spk2utt)))\n",
    "for s in spk2utt:\n",
    "    print(s, len(spk2utt[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(ainu_path, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_file = os.path.join(out_path, \"ainu10.ids\")\n",
    "text_file = os.path.join(out_path, \"ainu10.clean.en\")\n",
    "bpe_file = os.path.join(out_path, \"ainu10.BPE_1000.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ids_file, \"w\", encoding=\"utf-8\") as ids_f, open(text_file, \"w\", encoding=\"utf-8\") as text_f:\n",
    "    for i in ids:\n",
    "        ids_f.write(\"{0:s}\\n\".format(i))\n",
    "        with open(os.path.join(en_path, \"{0:s}.en.cl\".format(i)), \"r\") as en_f:\n",
    "            lines = en_f.readlines()\n",
    "            text = clean_out_str(lines[0].strip())\n",
    "            if len(lines) > 1:\n",
    "                print(i, len(lines))\n",
    "            text_f.write(\"{0:s}\\n\".format(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MFCCs, and normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = '/afs/inf.ed.ac.uk/group/project/ast/work/corpora/mboshi-french-parallel-corpus/wavs'\n",
    "mfcc_path = '/afs/inf.ed.ac.uk/group/project/ast/work/corpora/mboshi-french-parallel-corpus/mfcc_raw'\n",
    "mfcc_std_path = '/afs/inf.ed.ac.uk/group/project/ast/work/corpora/mboshi-french-parallel-corpus/mfcc_std'\n",
    "mfcc_final_path = '/afs/inf.ed.ac.uk/group/project/ast/work/corpora/mboshi-french-parallel-corpus/mboshi_mfccs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_set in id_sets:\n",
    "    ids = id_sets[id_set]\n",
    "    for i in tqdm(ids, ncols=80):\n",
    "        wav_fname = os.path.join(wav_path, \"{0:s}.wav\".format(i))\n",
    "        mfcc_fname = os.path.join(mfcc_path, \"{0:s}.mfcc\".format(i))\n",
    "        mfcc_std_fname = os.path.join(mfcc_std_path, \"{0:s}.std.mfcc\".format(i))\n",
    "        mfcc_final_fname = os.path.join(mfcc_final_path, \"{0:s}\".format(i))\n",
    "\n",
    "        !$FEACALC -plp no -cep 13 -dom cep -deltaorder 0 -dither -frqaxis bark \\\n",
    "            -samplerate 16000 -win 25 -step 10 -ip MSWAVE -rasta false -compress true \\\n",
    "            -op swappedraw -o $mfcc_fname $wav_fname\n",
    "\n",
    "        !$STANDFEAT -D 13 -infile $mfcc_fname -outfile $mfcc_std_fname\n",
    "\n",
    "        out_mfcc = np.fromfile(mfcc_std_fname, dtype=np.float32)\n",
    "        out_mfcc = out_mfcc.reshape((-1,13))\n",
    "        print(out_mfcc.shape)\n",
    "        np.save(mfcc_final_fname, out_mfcc)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha = np.load(os.path.join(mfcc_final_path, \"abiayi_2015-09-08-11-18-39_samsung-SM-T530_mdw_elicit_Dico18_1.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = ha.reshape((-1,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha.shape, haha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in set(swbd1_ids) - {\"swbd1_train_nodev\"}:\n",
    "info = {}\n",
    "for c in id_sets:\n",
    "    print(c)\n",
    "    info[c] = {}\n",
    "    for x in tqdm(id_sets[c], ncols=80):\n",
    "        info[c][x] = {}\n",
    "        t_data = np.load(\"./mfcc_13dim/mboshi_mfccs/{0:s}.npy\".format(x))\n",
    "        info[c][x][\"sp\"] = t_data.shape[0]\n",
    "        info[c][x][\"es_w\"] = 0\n",
    "        info[c][x][\"es_c\"] = 0\n",
    "        info[c][x][\"en_w\"] = len(mboshi_map[c][x][\"en_w\"])\n",
    "        info[c][x][\"en_c\"] = len(mboshi_map[c][x][\"en_c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(info, open(\"./mfcc_13dim/info_mboshi.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durs = {}\n",
    "for c in id_sets:\n",
    "    print(c)\n",
    "    durs[c] = []\n",
    "    for x in tqdm(id_sets[c], ncols=80):\n",
    "        t_data = np.load(\"./mfcc_13dim/mboshi_mfccs/{0:s}.npy\".format(x))\n",
    "        durs[c].append(t_data.shape[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in durs:\n",
    "    print(c)\n",
    "    print(\"total hrs = {0:.3f}\".format(sum(durs[c]) / 100. / 3600))\n",
    "    print(\"min = {0:.2f}, max = {1:.2f}, mean = {2:.2f}\".format(np.min(durs[c])/100, \n",
    "                                                                np.max(durs[c])/100, np.mean(durs[c])/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"mboshi/mboshi_train.fr\"\n",
    "dev_text = \"mboshi/mboshi_test.fr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text_fname):\n",
    "    words = []\n",
    "    with open(text_fname, \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for line in in_f:\n",
    "            words.extend(line.strip().split())\n",
    "    return Counter(words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter = get_words(train_text)\n",
    "dev_counter = get_words(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 1369),\n",
       " ('la', 1320),\n",
       " ('le', 1179),\n",
       " ('est', 937),\n",
       " ('a', 923),\n",
       " ('il', 858),\n",
       " ('les', 710),\n",
       " ('à', 509),\n",
       " ('dans', 445),\n",
       " ('un', 424)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lengths(text_fname):\n",
    "    lengths = []\n",
    "    with open(text_fname, \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for line in in_f:\n",
    "            lengths.append(len(line.strip().split()))\n",
    "    return np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths = get_lengths(train_text)\n",
    "dev_lengths = get_lengths(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.798460144927536, 1, 27)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_lengths), np.min(train_lengths), np.max(train_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.692607003891051, 2, 21)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(dev_lengths), np.min(dev_lengths), np.max(dev_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n"
     ]
    }
   ],
   "source": [
    "K = 8\n",
    "N = len(dev_lengths)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_k_most_common(out_fname, K, N):\n",
    "    k_words = \" \".join([i[0] for i in train_counter.most_common(K)])\n",
    "    out_line = \"{0:s}\\n\".format(k_words)\n",
    "    with open(out_fname, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for n in range(N):\n",
    "            out_f.write(out_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_k_most_common(\"./mboshi/mboshi_test_dummy_top-{0:d}_words.fr\".format(K), K=K, N=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common sentences in Mboshi Train\n",
    "sort mboshi/mboshi_train.fr | uniq -c | sort -rn | head -n 12\n",
    "\n",
    "```\n",
    "6 les pêcheurs ont rapporté beaucoup de poisson\n",
    "6 les bananes sont pleines\n",
    "6 il est très bavard\n",
    "6 attends moi  j'arrive\n",
    "5 il n'aime pas être battu au jeu\n",
    "5 celui ci est mon champ  celui là est à mon oncle\n",
    "4 tu peux partir devant  je t'atteindrai en route\n",
    "4 si tu attends encore un peu  il va venir\n",
    "4 ses cheveux sont brillants\n",
    "4 on brûle les herbes\n",
    "4 on a augmenté la paie des trvailleurs\n",
    "4 l'éléphant barrit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common sentences in Fisher Train\n",
    "sort ../subword-nmt/fisher_train.en | uniq -c | sort -rn | head -n 20\n",
    "\n",
    "```\n",
    "   6804 yes\n",
    "   3106 aha\n",
    "   1975 mm\n",
    "   1237 hmm\n",
    "   1217 sure\n",
    "   1150 oh\n",
    "   1054 ah\n",
    "    930 mhm\n",
    "    790 yeah\n",
    "    726 yes yes\n",
    "    708 right\n",
    "    632 uh huh\n",
    "    434 hello\n",
    "    429 exactly\n",
    "    424 no\n",
    "    409 okay\n",
    "    403 uh uh\n",
    "    362 hm mm\n",
    "    358 oh yes\n",
    "    346 um\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare seq-to-seq dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"./mfcc_13dim/\"\n",
    "map_dict = pickle.load(open(\"../speech2text/mfcc_13dim/bpe_map.dict\", \"rb\"))\n",
    "vocab_dict = pickle.load(open(\"../speech2text/mfcc_13dim/bpe_train_vocab.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk2utt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ainu_map(train_spkrs, dev_spkrs, test_spkrs):\n",
    "    oov = {}\n",
    "    ainu_map = {}\n",
    "    train_ids_str = \"_\".join(map(str, train_spkrs))\n",
    "    dev_ids_str = \"_\".join(map(str, dev_spkrs))\n",
    "    test_ids_str = \"_\".join(map(str, test_spkrs))\n",
    "    \n",
    "    ainu_map_fname = \"ainu_train-{0:s}-dev-{1:s}-test-{2:s}_map.dict\".format(train_ids_str, \n",
    "                                                                             dev_ids_str, \n",
    "                                                                             test_ids_str)\n",
    "        \n",
    "    \n",
    "    with open(os.path.join(\"../subword-nmt/\", \"ainu10.BPE_1000.en\"), \"rb\") as text_f, \\\n",
    "         open(os.path.join(\"../subword-nmt/\", \"ainu10.ids\"), \"r\") as id_f, \\\n",
    "         open(os.path.join(\"../subword-nmt/\", \"ainu10.clean.en\"), \"rb\") as words_f:\n",
    "        for i, t, e in zip(id_f, text_f, words_f):\n",
    "            curr_spkr = utt2spk[i.strip()]\n",
    "            if curr_spkr in train_spkrs:\n",
    "                c = \"ainu_train-{0:s}\".format(train_ids_str)\n",
    "            elif curr_spkr in dev_spkrs:\n",
    "                c = \"ainu_dev-{0:s}\".format(dev_ids_str)\n",
    "            elif curr_spkr in test_spkrs:\n",
    "                c = \"ainu_test-{0:s}\".format(test_ids_str)\n",
    "            else:\n",
    "                print(i, curr_spkr)\n",
    "                print(\"Achtung!!\")\n",
    "            if c not in ainu_map:\n",
    "                ainu_map[c] = {}\n",
    "                oov[c] = []\n",
    "            ainu_map[c][i.strip()] = {}\n",
    "            ainu_map[c][i.strip()][\"bpe_w\"] = t.strip().split()\n",
    "            ainu_map[c][i.strip()][\"en_w\"] = e.strip().split()\n",
    "            ainu_map[c][i.strip()][\"en_c\"] = [tt.encode() for tt in e.strip().decode()]\n",
    "            for w in t.strip().split():\n",
    "                if w not in vocab_dict[\"bpe_w\"][\"w2i\"]:\n",
    "                    oov[c].append(w)\n",
    "                    \n",
    "            for w in t.strip().split():\n",
    "                if w not in vocab_dict[\"bpe_w\"][\"w2i\"]:\n",
    "                    oov[c].append(w)\n",
    "\n",
    "    print(ainu_map_fname)\n",
    "    return ainu_map, ainu_map_fname, oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spkrs=[str(i) for i in range(2,10)]\n",
    "dev_spkrs=['0']\n",
    "test_spkrs=['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainu_map, ainu_map_fname, oov = create_ainu_map(train_spkrs=train_spkrs, \n",
    "                                                dev_spkrs=dev_spkrs, test_spkrs=test_spkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([w.decode() for w in oov['ainu_train-2_3_4_5_6_7_8_9']])\n",
    "print([w.decode() for w in oov['ainu_dev-0']])\n",
    "print([w.decode() for w in oov['ainu_test-1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainu_map['ainu_train-2_3_4_5_6_7_8_9']['2.000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ainu_ref(train_spkrs, dev_spkrs, test_spkrs):\n",
    "    train_ids_str = \"_\".join(map(str, train_spkrs))\n",
    "    dev_ids_str = \"_\".join(map(str, dev_spkrs))\n",
    "    test_ids_str = \"_\".join(map(str, test_spkrs))\n",
    "        \n",
    "    ainu_ids = {}\n",
    "    ainu_text = {}\n",
    "    with open(os.path.join(\"../subword-nmt/\", \"ainu10.ids\"), \"r\") as id_f, \\\n",
    "         open(os.path.join(\"../subword-nmt/\", \"ainu10.clean.en\"), \"r\") as words_f:\n",
    "        for i, e in zip(id_f, words_f):\n",
    "            curr_spkr = utt2spk[i.strip()]\n",
    "            if curr_spkr in train_spkrs:\n",
    "                c = \"ainu_train-{0:s}\".format(train_ids_str)\n",
    "            elif curr_spkr in dev_spkrs:\n",
    "                c = \"ainu_dev-{0:s}\".format(dev_ids_str)\n",
    "            elif curr_spkr in test_spkrs:\n",
    "                c = \"ainu_test-{0:s}\".format(test_ids_str)\n",
    "            else:\n",
    "                print(i, curr_spkr)\n",
    "                print(\"Achtung!!\")\n",
    "            if c not in ainu_ids:\n",
    "                ainu_ids[c] = []\n",
    "                ainu_text[c] = []\n",
    "            \n",
    "            ainu_ids[c].append(i.strip())\n",
    "            ainu_text[c].append(e.strip())\n",
    "\n",
    "    print(list(ainu_ids.keys()))\n",
    "    return ainu_ids, ainu_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainu_ids, ainu_text = create_ainu_ref(train_spkrs=train_spkrs, dev_spkrs=dev_spkrs, test_spkrs=test_spkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ainu_text:\n",
    "    with open(\"./mfcc_13dim/ainu/{0:s}.en\".format(c), \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for i in ainu_text[c]:\n",
    "            out_f.write(\"{0:s}\\n\".format(i))\n",
    "        # end for\n",
    "    # end with\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing ids to: {0:s}\".format(ainu_map_fname.replace(\"map\", \"ids\")))\n",
    "pickle.dump(ainu_ids, open(\"./mfcc_13dim/ainu/{0:s}\".format(ainu_map_fname.replace(\"map\", \"ids\")), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing map to: {0:s}\".format(ainu_map_fname))\n",
    "pickle.dump(ainu_map, open(os.path.join(cfg_path, ainu_map_fname), \"wb\"))\n",
    "print(\"writing vocab to: {0:s}\".format(ainu_map_fname.replace(\"_map\", \"_train_vocab\")))\n",
    "pickle.dump(vocab_dict, open(os.path.join(cfg_path, ainu_map_fname.replace(\"_map\", \"_train_vocab\")), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pickle.load(open(\"../speech2text/mfcc_13dim/info.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainu_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in set(swbd1_ids) - {\"swbd1_train_nodev\"}:\n",
    "info = {}\n",
    "for c in set(ainu_map.keys()):    \n",
    "    info[c] = {}\n",
    "    for x in tqdm(ainu_ids[c], ncols=80):\n",
    "        info[c][x] = {}\n",
    "        t_data = np.load(\"./mfcc_13dim/ainu_mfccs/{0:s}.npy\".format(x))\n",
    "        info[c][x][\"sp\"] = t_data.shape[0]\n",
    "        info[c][x][\"es_w\"] = 0\n",
    "        info[c][x][\"es_c\"] = 0\n",
    "        info[c][x][\"en_w\"] = len(ainu_map[c][x][\"en_w\"])\n",
    "        info[c][x][\"en_c\"] = len(\" \".join([w.decode() for w in ainu_map[c][x][\"en_w\"]]))\n",
    "    # end for        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing info to: {0:s}\".format(\"./mfcc_13dim/{0:s}\".format(ainu_map_fname.replace(\"map\", \"info\"))))\n",
    "pickle.dump(info, open(\"./mfcc_13dim/{0:s}\".format(ainu_map_fname.replace(\"map\", \"info\")), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array([len(ainu_map[\"ainu_train-2_3_4_5_6_7_8_9\"][x][\"bpe_w\"]) for x in info[\"ainu_train-2_3_4_5_6_7_8_9\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array([info[\"ainu_train-2_3_4_5_6_7_8_9\"][x][\"en_w\"] for x in info[\"ainu_train-2_3_4_5_6_7_8_9\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mfb(wav_fname, mfb_fname, mfb_std_fname, nfilt=40):\n",
    "    rate, sig = wav.read(wav_fname)\n",
    "    mfb_feat = logfbank(sig,rate, nfilt=nfilt)\n",
    "    d_mfb_feat = delta(mfb_feat, 2)\n",
    "    dd_mfb_feat = delta(d_mfb_feat, 2)\n",
    "    mfb_feat = np.concatenate((mfb_feat, d_mfb_feat), axis=1)\n",
    "    mfb_feat = np.concatenate((mfb_feat, dd_mfb_feat), axis=1)\n",
    "    try:\n",
    "        mfb_feat_std = (mfb_feat - np.mean(mfb_feat, axis=0)) / np.std(mfb_feat, axis=0)\n",
    "    except:\n",
    "        print(wav_fname)\n",
    "    # save mfb files\n",
    "#     np.save(open(mfb_fname, \"wb\"), mfb_feat)\n",
    "    np.save(open(mfb_std_fname, \"wb\"), mfb_feat_std.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mfcc(wav_fname, mfcc_fname, mfcc_std_fname):\n",
    "    rate, sig = wav.read(wav_fname)\n",
    "    mfcc_feat = mfcc(sig,rate)\n",
    "    d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "    dd_mfcc_feat = delta(d_mfcc_feat, 2)\n",
    "    mfcc_feat = np.concatenate((mfcc_feat, d_mfcc_feat), axis=1)\n",
    "    mfcc_feat = np.concatenate((mfcc_feat, dd_mfcc_feat), axis=1)\n",
    "    std_vals = np.std(mfcc_feat, axis=0)\n",
    "    mfcc_feat_std = (mfcc_feat - np.mean(mfcc_feat, axis=0)) / np.std(mfcc_feat, axis=0)\n",
    "    if not np.isfinite(std_vals).all() or not np.isfinite(std_vals).all():\n",
    "        print(\"HAAAALP\", wav_fname)\n",
    "    # save mfcc files\n",
    "#     np.save(open(mfcc_fname, \"wb\"), mfcc_feat)\n",
    "    np.save(open(mfcc_std_fname, \"wb\"), mfcc_feat_std.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_speech_features(mffc1mfb0=True):\n",
    "    with tqdm(total=len(align_dict)) as pbar:\n",
    "        for wav_fil in sorted(align_dict.keys()):\n",
    "            for j, uttr in enumerate(sorted(align_dict[wav_fil].keys())):\n",
    "                wav_fname = os.path.join(fa_vad_wavs_path, \"{0:s}_fa_vad.wav\".format(uttr))\n",
    "                if mffc1mfb0:\n",
    "                    mfcc_fname = os.path.join(fa_vad_mfcc_path, \"{0:s}_fa_vad.mfcc\".format(uttr))\n",
    "                    mfcc_std_fname = os.path.join(fa_vad_std_mfcc_path, \"{0:s}_fa_vad.std.mfcc\".format(uttr))\n",
    "                    create_mfcc(wav_fname, mfcc_fname, mfcc_std_fname)\n",
    "                else:\n",
    "                    mfb_fname = os.path.join(fa_vad_mfb_path, \"{0:s}_fa_vad.mfb\".format(uttr))\n",
    "                    mfb_std_fname = os.path.join(fa_vad_std_mfb_path, \"{0:s}_fa_vad.std.mfb\".format(uttr))\n",
    "                    create_mfb(wav_fname, mfb_fname, mfb_std_fname)\n",
    "                \n",
    "            # end for uttr\n",
    "            pbar.update(1)\n",
    "        # end for file\n",
    "    # end pbar\n",
    "\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_all_speech_features(mffc1mfb0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, uttr in enumerate(sorted(align_dict['038'].keys())[20:]):\n",
    "    print(uttr, end=', ')\n",
    "    wav_fname = os.path.join(fa_vad_wavs_path, \"{0:s}_fa_vad.wav\".format(uttr))\n",
    "    mfcc_fname = os.path.join(fa_vad_mfcc_path, \"{0:s}_fa_vad.mfcc\".format(uttr))\n",
    "    mfcc_std_fname = os.path.join(fa_vad_std_mfcc_path, \"{0:s}_fa_vad.std.mfcc\".format(uttr))\n",
    "    create_mfcc(wav_fname, mfcc_fname, mfcc_std_fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../uttr_fa_vad_wavs/mfcc/ | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = np.load(\"../uttr_fa_vad_wavs/mfcc/001.002_fa_vad.mfcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha_mfb = np.load(\"../uttr_fa_vad_wavs/mfb/001.002_fa_vad.mfb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha.shape, haha_mfb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha_mfb[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(haha_mfb, 0)[0,0,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flipud(haha_mfb)[-1,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### etc code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfile = os.path.join(uttr_wavs_path, \"001.002.wav\")\n",
    "(rate,sig) = wav.read(wavfile)\n",
    "mfcc_feat = mfcc(sig,rate)\n",
    "mfb = logfbank(sig, rate, nfilt=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfb.shape, mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rate,sig) = wav.read(wavfile)\n",
    "mfcc_feat = mfcc(sig,rate)\n",
    "d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "dd_mfcc_feat = delta(d_mfcc_feat, 2)\n",
    "mfcc_feat = np.concatenate((mfcc_feat, d_mfcc_feat), axis=1)\n",
    "mfcc_feat = np.concatenate((mfcc_feat, dd_mfcc_feat), axis=1)\n",
    "mfcc_feat_std = (mfcc_feat - np.mean(mfcc_feat, axis=0)) / np.std(mfcc_feat, axis=0)\n",
    "mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mfcc(wavfile, fa_vad_mfcc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../uttr_fa_vad_wavs/mfcc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(wavfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat.shape, d_mfcc_feat.shape, dd_mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat_base[0,:], mfcc_feat[0, :13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mfcc_feat[0,:], mfcc_feat[0, 13:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mfcc_feat, axis=0).shape, np.std(mfcc_feat, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat_std = (mfcc_feat - np.mean(mfcc_feat, axis=0)) / np.std(mfcc_feat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat_std[0, 13:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(os.path.join(uttr_wavs_path, \"001.002.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(os.path.join(fa_vad_wavs_path, \"001.002_fa_vad.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join([w.word for w in align_dict[\"110\"][\"110.005\"][\"es\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!soxi ../uttr_fa_vad_wavs/fa_vad_wavs/110.005_fa_vad.wav\n",
    "!soxi ../uttr_fa_vad_wavs/uttr_wavs/110.005.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MFCCs and Log Mel Filterbanks generated using Kaldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaldi_out_path = \"../uttr_fa_vad_wavs/kaldi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = np.load(\"../uttr_fa_vad_wavs/mfcc_std/001.002_fa_vad.std.mfcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha.shape, haha[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $kaldi_out_path/mfcc_cmvn_dd_vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaldi_test = np.load(\"../uttr_fa_vad_wavs/kaldi/mfcc_cmvn_dd_vad/test.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaldi_dev['001.002'].shape, kaldi_dev['001.002'][0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
