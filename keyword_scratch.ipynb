{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from basics import *\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"interspeech_new_vocab/sp_160hrs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fisher dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict = get_data_dicts(m_cfg)\n",
    "info_dict = pickle.load(open(\"fbanks_80dim_nltk/info.dict\", \"rb\"))\n",
    "sim_dict = pickle.load(open(\"./fbanks_80dim_nltk/mix_sim.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in map_dict[\"fisher_train\"]:\n",
    "    train_text.append(\" \".join([w.decode() for w in map_dict[\"fisher_train\"][u][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'hello',\n",
       " 'hello',\n",
       " 'hello',\n",
       " 'with whom am i speaking',\n",
       " 'eh silvia yes what is your name',\n",
       " 'hello silvia eh my name is nicole',\n",
       " 'ah nice to meet you',\n",
       " 'nice to meet you em and where are you from',\n",
       " \"eh i 'm in philadelphia\"]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_to_dump = \"\\n\".join(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../subword-nmt/fisher_train.en\", \"w\") as out_f:\n",
    "    out_f.write(train_text_to_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./learn_joint_bpe_and_vocab.py --input {train_file}.L1 {train_file}.L2 -s {num_operations} -o {codes_file} --write-vocabulary {vocab_file}.L1 {vocab_file}.L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.Load(\"test/test_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_len = 1\n",
    "top_k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_stop_words = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "len(es_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(m_dict, key=\"en_w\"):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u][key]) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u][key]])\n",
    "        else:\n",
    "            for ref in m_dict[u][key]:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3979"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_dict['fisher_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train word types   |      17688\n",
      "# train word tokens  |    1496533\n",
      "--------------------------------------------------------------------------------\n",
      "# dev word types     |       4798\n",
      "# dev word tokens    |     165047\n"
     ]
    }
   ],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))\n",
    "\n",
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train word types   |      32168\n",
      "# train word tokens  |    1494878\n",
      "--------------------------------------------------------------------------------\n",
      "# dev word types     |       4073\n",
      "# dev word tokens    |      41014\n"
     ]
    }
   ],
   "source": [
    "# words in train\n",
    "es_train_words = get_words(map_dict['fisher_train'], key=\"es_w\")\n",
    "es_train_words_top_k = [(w,f) for w, f in sorted(es_train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in es_stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "es_train_only_words = set(es_train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(es_train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(es_train_words.values())))\n",
    "\n",
    "es_dev_words = get_words(map_dict['fisher_dev'], key=\"es_w\")\n",
    "es_dev_words_top_k = [(w,f) for w, f in sorted(es_dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in es_stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "es_dev_only_words = set(es_dev_words.keys())\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(es_dev_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(es_dev_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('yes', 35085),\n",
       "  (\"'s\", 24162),\n",
       "  (\"n't\", 19184),\n",
       "  ('like', 14337),\n",
       "  ('well', 12354)],\n",
       " [('ah', 12598), ('eh', 11578), ('si', 9541), ('ajá', 7992), ('bueno', 7874)])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_top_k[:5], es_train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 24162), (\"n't\", 19184), (\"'m\", 5546), (\"'re\", 2832), (\"'ve\", 2392)]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('yes', 3652), (\"n't\", 1999), (\"'s\", 1866), ('like', 1827), ('know', 1294)],\n",
       " [('ah', 349), ('ajá', 343), ('si', 251), ('entonces', 249), ('mhm', 236)])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_words_top_k[:5], es_dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# oov word types     |        999\n",
      "# oov word tokens    |       1562\n"
     ]
    }
   ],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# oov word types     |        440\n",
      "# oov word tokens    |        517\n"
     ]
    }
   ],
   "source": [
    "es_oov_words = {w:f for w,f in es_dev_words.items() if w not in es_train_only_words}\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(es_oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(es_oov_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9%'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level - get train, dev frequency, and utts in which they occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17688, 11869)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_only_words), len(set([stem(w) for w in train_only_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_level_details(word_key):\n",
    "    word_utt_count = {\"train\": {}, \"dev\": {}, \"train_utts\": {}, \"dev_utts\": {}}\n",
    "    for u in tqdm(map_dict[\"fisher_train\"].keys()):\n",
    "        for w in set(map_dict[\"fisher_train\"][u][word_key]):\n",
    "            curr_word = w.decode()\n",
    "            if curr_word not in word_utt_count[\"train\"]:\n",
    "                word_utt_count[\"train\"][curr_word] = 0\n",
    "                word_utt_count[\"train_utts\"][curr_word] = set()\n",
    "            word_utt_count[\"train\"][curr_word] += 1\n",
    "            word_utt_count[\"train_utts\"][curr_word].update({u})\n",
    "        # end for words in current utt\n",
    "    # end for all utts\n",
    "    for u in tqdm(map_dict[\"fisher_dev\"].keys()):\n",
    "        if word_key == \"en_w\":\n",
    "            for ref in map_dict[\"fisher_dev\"][u][word_key]:\n",
    "                for w in set(ref):\n",
    "                    curr_word = w.decode()\n",
    "                    if curr_word not in word_utt_count[\"dev\"]:\n",
    "                        word_utt_count[\"dev\"][curr_word] = 0\n",
    "                        word_utt_count[\"dev_utts\"][curr_word] = set()\n",
    "                    word_utt_count[\"dev\"][curr_word] += 1            \n",
    "                    word_utt_count[\"dev_utts\"][curr_word].update({u})\n",
    "                # end for words in current ref\n",
    "            # end for all references\n",
    "        # end if multiple references\n",
    "        else:\n",
    "            ref = map_dict[\"fisher_dev\"][u][word_key]\n",
    "            for w in set(ref):\n",
    "                curr_word = w.decode()\n",
    "                if curr_word not in word_utt_count[\"dev\"]:\n",
    "                    word_utt_count[\"dev\"][curr_word] = 0\n",
    "                    word_utt_count[\"dev_utts\"][curr_word] = set()\n",
    "                word_utt_count[\"dev\"][curr_word] += 1            \n",
    "                word_utt_count[\"dev_utts\"][curr_word].update({u})\n",
    "            \n",
    "    # end for all utts\n",
    "    all_train_utts = set()\n",
    "    for w in word_utt_count[\"train_utts\"]:\n",
    "        all_train_utts.update(word_utt_count[\"train_utts\"][w])\n",
    "    # end for\n",
    "\n",
    "    all_dev_utts = set()\n",
    "    for w in word_utt_count[\"dev_utts\"]:\n",
    "        all_dev_utts.update(word_utt_count[\"dev_utts\"][w])\n",
    "    # end for\n",
    "    \n",
    "    return word_utt_count, all_train_utts, all_dev_utts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138819/138819 [00:02<00:00, 62220.42it/s]\n",
      "100%|██████████| 3979/3979 [00:00<00:00, 18813.80it/s]\n"
     ]
    }
   ],
   "source": [
    "en_word_utt_count, en_train_utts, en_dev_utts = get_word_level_details(\"en_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138794, 3979)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_train_utts), len(en_dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138819/138819 [00:02<00:00, 60435.14it/s]\n",
      "100%|██████████| 3979/3979 [00:00<00:00, 58295.33it/s]\n"
     ]
    }
   ],
   "source": [
    "es_word_utt_count, es_train_utts, es_dev_utts = get_word_level_details(\"es_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138795, 3977)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(es_train_utts), len(es_dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word types\n",
      "17688 4798\n",
      "common word types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3799"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"word types\")\n",
    "print(len(en_word_utt_count['train']), len(en_word_utt_count['dev']))\n",
    "print(\"common word types\")\n",
    "en_common_words = set(en_word_utt_count['train'].keys()) & set(en_word_utt_count['dev'].keys())\n",
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word types\n",
      "32168 4073\n",
      "common word types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3633"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"word types\")\n",
    "print(len(es_word_utt_count['train']), len(es_word_utt_count['dev']))\n",
    "print(\"common word types\")\n",
    "es_common_words = set(es_word_utt_count['train'].keys()) & set(es_word_utt_count['dev'].keys())\n",
    "len(es_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details_for_words(words, common_words, word_utt_count, \n",
    "                          min_dev_freq, max_dev_freq, min_train_freq, max_train_freq, min_len):\n",
    "    details = {\"words\": {}, \"train_utts\": set(), \"dev_utts\": set()}\n",
    "    \n",
    "    in_vocab_words = set(words) & set(common_words)\n",
    "    print(\"number of in-vocab words = {0:d}\".format(len(in_vocab_words)))\n",
    "\n",
    "    for w in in_vocab_words:\n",
    "        t_count, d_count = len(word_utt_count[\"train_utts\"][w]), len(word_utt_count[\"dev_utts\"][w])\n",
    "        if ((d_count >= min_dev_freq) and \n",
    "            (d_count <= max_dev_freq) and\n",
    "            (len(w) >= min_len) and\n",
    "            (t_count >= min_train_freq) and \n",
    "            (t_count <= max_train_freq)):\n",
    "            details[\"words\"][w] = {\"train\": t_count, \"dev\": d_count}\n",
    "            details[\"train_utts\"].update(word_utt_count[\"train_utts\"][w])\n",
    "            details[\"dev_utts\"].update(word_utt_count[\"dev_utts\"][w])\n",
    "        # end meets criteria\n",
    "    # end for in-vocab word\n",
    "    return details\n",
    "# end function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration(utts, key):\n",
    "    dur = 0\n",
    "    utts_not_found = []\n",
    "    for u in utts:\n",
    "        if u not in info_dict[key]:\n",
    "            #print(\"argh!\", u)\n",
    "            utts_not_found.append(u)\n",
    "        else:\n",
    "            dur += (info_dict[key][u]['sp'] * 10)\n",
    "    dur = dur / 60 / 60 / 1000\n",
    "    print(\"-\"*80)\n",
    "    print(\"{0:d} total utts\".format(len(utts)))\n",
    "    print(\"{0:d} not found\".format(len(utts_not_found)))\n",
    "    print(\"selected utts from {0:s} -- duration = {1:.2f} hours\".format(key, dur))\n",
    "    return dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(words_list):\n",
    "    out = {\"w2i\":{}, \"i2w\":{}, \"freq\":{}, \"freq_dev\":{}}\n",
    "    START_VOCAB = [PAD, GO, EOS, UNK]\n",
    "    for w in START_VOCAB:\n",
    "        out['w2i'][w] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][w] = 1\n",
    "    #for w in words_list['words']:\n",
    "    sorted_w = sorted(words_list['words'].items(), reverse=True, key=lambda t: t[1]['train'])\n",
    "    for w in sorted_w:\n",
    "        encoded_word = w[0].encode()\n",
    "        out[\"w2i\"][encoded_word] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][encoded_word] = w[1][\"train\"]\n",
    "        out[\"freq_dev\"][encoded_word] = w[1][\"dev\"]\n",
    "\n",
    "    out[\"i2w\"] = {val:key for key, val in out[\"w2i\"].items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "138794 total utts\n",
      "89 not found\n",
      "selected utts from fisher_train -- duration = 161.62 hours\n",
      "--------------------------------------------------------------------------------\n",
      "3979 total utts\n",
      "2 not found\n",
      "selected utts from fisher_dev -- duration = 4.35 hours\n"
     ]
    }
   ],
   "source": [
    "train_dur, dev_dur = get_duration(en_train_utts, key=\"fisher_train\"), get_duration(en_dev_utts, key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=10\n",
    "max_dev_freq=10000\n",
    "min_train_freq=50 \n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(common_words, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_top_500_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_top_500_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_top_500_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - randomly selected frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=10 \n",
    "max_dev_freq=100\n",
    "min_train_freq=100\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(common_words, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_top_100_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_top_100_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_top_100_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2 - topics as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_fname= \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\"\n",
    "topics_fname = \"../criseslex/fsp06_topics_in_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [ \"peace\", \"Music\", \"Marriage\", \"Religion\", \"Cell phones\", \n",
    "           \"Dating\", \"Telemarketing and SPAM\", \"Politics\", \"Travel\", \n",
    "           \"Technical devices\", \"Healthcare\", \"Advertisements\", \"Power\", \n",
    "           \"Occupations\", \"Movies\", \"Welfare\", \"Breaking up\", \"Location\", \n",
    "           \"Justice\", \"Memories\", \"Crime\", \"Violence against women\", \"Equality\", \n",
    "            \"Housing\", \"Immigration\",     \n",
    "            # new topics\n",
    "           \"Interracial\", \"Christians\", \"muslims\", \"jews\", \"e-mail\", \n",
    "           \"phone\", \"democracy\", \"Democratic\", \"Republican\", \"technology\", \n",
    "           \"leadership\", \"community\", \"jury\", \"police\", \"inequality\", \n",
    "           \"renting\", \"Violence\", \"immigrants\", \"immigrant\", \"skilled\", \n",
    "           \"Telemarketing\", \"SPAM\", \"skill\", \"job\", \"health\", \"mobile\", \n",
    "            \"ads\", \"physical\", \"emotional\", \"bubble\", \"rent\", \"economy\", \n",
    "            \"abuse\", \"women\", \"city\", \"country\", \"suburban\", \"dollar\", \n",
    "            \"united states\", \"laws\", \"phone\", \"race\", \"biracial\", \"interracial\", \n",
    "            \"marriage\", \"lyrics\", \"sexuality\", \"medicine\", \"television\", \"european\",\n",
    "            \"home\", \"protect\", \"spouse\", \"language\", \"cellphone\", \"money\",\n",
    "            \"doctor\", \"insurance\", \"cigarettes\", \"alcohol\", \"income\", \"salary\",\n",
    "            \"class\", \"censor\", \"rating\", \"programs\", \"government\",\n",
    "            \"relationship\", \"legal\", \"event\", \"life\", \"safe\", \"victim\", \"cops\",\n",
    "            \"wage\", \"illegal\"\n",
    "            ]\n",
    "topics = list(set(t.lower() for t in topics))\n",
    "topics_stem = [stem(t) for t in topics]\n",
    "\n",
    "# add similar topic words\n",
    "new_topics = []\n",
    "# for t in topics:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_topics.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "topics.extend(new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_details = get_details_for_words(topics, en_common_words, en_word_utt_count, \n",
    "                                       min_dev_freq=5, \n",
    "                                       max_dev_freq=10000, \n",
    "                                       min_train_freq=10, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(topics_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(topics_details[\"train_utts\"], key=\"fisher_train\"), get_duration(topics_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(list(topics_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_details['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_topics_vocab = create_vocab(topics_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = pickle.load(open(os.path.join(m_cfg['data_path'], \"bow_topics_vocab.dict\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(haha['w2i']) & set(bow_topics_vocab['w2i'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_topics_vocab, open(os.path.join(m_cfg['data_path'], \"bow_topics_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - crises terms as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises = set()\n",
    "with open(crises_lex_fname, \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        crises.update(line.strip().split())\n",
    "crises = list(crises)\n",
    "crises_stem = [stem(w) for w in crises]\n",
    "\n",
    "# new_crises = []\n",
    "# for t in crises:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_crises.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "# crises.extend(new_crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details = get_details_for_words(crises, en_common_words, en_word_utt_count,\n",
    "                                       min_dev_freq=10, \n",
    "                                       max_dev_freq=1000, \n",
    "                                       min_train_freq=100, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \"bow_crises_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details = get_details_for_words(crises, en_common_words, en_word_utt_count,\n",
    "                                       min_dev_freq=5, \n",
    "                                       max_dev_freq=1000, \n",
    "                                       min_train_freq=50, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \"bow_crises_vocab_more.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Spanish - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=18\n",
    "max_dev_freq=10000\n",
    "min_train_freq=200\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(es_common_words,\n",
    "                                          es_common_words,\n",
    "                                          es_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"].keys()), len(terms_of_interest[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_terms = [\"bueno\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms,\n",
    "                                             es_common_words,\n",
    "                                             es_word_utt_count,\n",
    "                                              min_dev_freq=min_dev_freq, \n",
    "                                              max_dev_freq=max_dev_freq, \n",
    "                                              min_train_freq=min_train_freq, \n",
    "                                              min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_terms_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details[\"train_utts\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details['words']['bueno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab['freq'][b'bueno'], bow_es_top_words_vocab['freq_dev'][b'bueno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_100word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_1word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!su s1444673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_100word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_top_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details['words']['colorado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(w, sample_terms_details['words'][w]['train']) \n",
    "       for w in sample_terms_details['words']], reverse=True, key=lambda t: t[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(w, sample_terms_details['words'][w]['dev']) \n",
    "       for w in sample_terms_details['words']], reverse=True, key=lambda t: t[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg['data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls fbanks_80dim_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg[\"sim_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'pre_trained_sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w'][b'sure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'mix_sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in mix_sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in [(w.decode(),[j.decode() for j in i]) for w, i in mix_sim_dict['w'].items() if len(i) > 2]:\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'pre_trained_sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in pre_sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in [(w.decode(),[j.decode() for j in i]) for w, i in pre_sim_dict['w'].items() if len(i) > 2]:\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sim_dict['w'][b'sure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in [(w.decode(),[j.decode() for j in i]) for w, i in sim_dict['w'].items() if len(i) > 2]:\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_words = [w for w, i in pre_sim_dict['w'].items() if len(i)>1]\n",
    "fisher_words = [w for w, i in sim_dict['w'].items() if len(i)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_words), len(fisher_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_only = set(pre_words) - set(fisher_words)\n",
    "fisher_only = set(fisher_words) - set(pre_words)\n",
    "common_only = set(pre_words) & set(fisher_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_only), len(fisher_only),  len(common_only), (len(set(pre_words) | set(fisher_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in pre_sim_dict['w'].items() if len(i) > 2 and w in pre_only]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in sim_dict['w'].items() if len(i) > 2 and w in fisher_only]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in pre_sim_dict['w'].items() \n",
    "                    if (len(i) > 1 and w in common_only)]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in sim_dict['w'].items() \n",
    "                    if (len(i) > 1 and w in common_only)]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(curr_set, word_type, max_len=1):\n",
    "    found_count = 0\n",
    "    eng_tokens = []\n",
    "    for utt in map_dict[curr_set]:\n",
    "        if word_type.encode() in map_dict[curr_set][utt][\"es_w\"] and len(map_dict[curr_set][utt][\"es_w\"]) <= max_len:\n",
    "            found_count+=1\n",
    "            if curr_set == \"fisher_train\":\n",
    "                eng_tokens.append(\" \".join([w.decode() for w in map_dict[curr_set][utt][\"en_w\"]]))\n",
    "            else:\n",
    "                for r in map_dict[curr_set][utt][\"en_w\"]:\n",
    "                    eng_tokens.append(\" \".join([w.decode() for w in r]))\n",
    "    print(found_count, len(map_dict[curr_set]), \"{0:.2f}\".format(found_count / len(map_dict[curr_set]) * 100))\n",
    "    print(len(set(eng_tokens)))\n",
    "    return Counter(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = check_word(\"fisher_train\", \"si\", 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = check_word(\"fisher_dev\", \"mhm\", 1)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(set([i[0] for i in t.most_common(10)]) and set([i[0] for i in d.most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_count = 0\n",
    "eng_tokens = []\n",
    "curr_set= \"fisher_train\"\n",
    "for utt in map_dict[curr_set]:\n",
    "    if b\"claro\" in map_dict[curr_set][utt][\"es_w\"] and len(map_dict[curr_set][utt][\"es_w\"]) <= 1:\n",
    "        found_count+=1\n",
    "        eng_tokens.append(\" \".join([w.decode() for w in map_dict[curr_set][utt][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count, len(map_dict[curr_set]), found_count / len(map_dict[curr_set]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count = 0\n",
    "eng_tokens = []\n",
    "curr_set= \"fisher_dev\"\n",
    "for utt in map_dict[curr_set]:\n",
    "    if b\"claro\" in map_dict[curr_set][utt][\"es_w\"] and len(map_dict[curr_set][utt][\"es_w\"]) <= 1:\n",
    "        found_count+=1\n",
    "        eng_tokens.append(\" \".join([w.decode() for w in map_dict[curr_set][utt][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count, len(map_dict[curr_set]), found_count / len(map_dict[curr_set]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(eng_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_names = [\"ref_min-0_max-300.en{0:d}\".format(i) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_names = []\n",
    "google_ref_names = []\n",
    "edin_ref_names = []\n",
    "for r in ref_names:\n",
    "    google_ref_names.append(os.path.join(\"google\", r))\n",
    "    edin_ref_names.append(os.path.join(\"./sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\", r))\n",
    "#     all_ref_names.append(os.path.join(\"sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\", r))\n",
    "    \n",
    "all_ref_names = google_ref_names + edin_ref_names    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_lines_in_file(fname):\n",
    "    all_lines = []\n",
    "    with open(fname, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            all_lines.append(set(line.strip().split()))\n",
    "    return all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_in_file(fname):\n",
    "    all_words = []\n",
    "    with open(fname, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            all_words.extend(line.strip().split())\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_ref = get_all_words_in_file(all_ref_names[0])\n",
    "all_words_ref = get_all_words_in_file(all_ref_names[0])\n",
    "\n",
    "for r in all_ref_names[1:]:\n",
    "    common_words_ref &= get_all_words_in_file(r)\n",
    "    all_words_ref |= get_all_words_in_file(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_words_ref), len(all_words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_common_words_ref = get_all_words_in_file(google_ref_names[0])\n",
    "google_all_words_ref = get_all_words_in_file(google_ref_names[0])\n",
    "\n",
    "for r in google_ref_names[1:]:\n",
    "    google_common_words_ref &= get_all_words_in_file(r)\n",
    "    google_all_words_ref |= get_all_words_in_file(r)\n",
    "print(len(google_common_words_ref), len(google_all_words_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edin_common_words_ref = get_all_words_in_file(edin_ref_names[0])\n",
    "edin_all_words_ref = get_all_words_in_file(edin_ref_names[0])\n",
    "\n",
    "for r in edin_ref_names[1:]:\n",
    "    edin_common_words_ref &= get_all_words_in_file(r)\n",
    "    edin_all_words_ref |= get_all_words_in_file(r)\n",
    "print(len(edin_common_words_ref), len(edin_all_words_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(google_common_words_ref & edin_common_words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ref_words = google_common_words_ref & edin_common_words_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_ref_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(google_all_words_ref - edin_all_words_ref), len(edin_all_words_ref - google_all_words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_lines = {}\n",
    "for i,r in enumerate(edin_ref_names):\n",
    "    all_ref_lines[i] = read_all_lines_in_file(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_all_refs = []\n",
    "\n",
    "for i in range(len(all_ref_lines[0])):\n",
    "    words_ref = all_ref_lines[0][i]\n",
    "    for j in range(1,4):\n",
    "        words_ref &= all_ref_lines[j][i]\n",
    "    words_in_all_refs.append(words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "\n",
    "for ref in words_in_all_refs:\n",
    "    now_words = ref & common_ref_words - stop_words - es_stop_words\n",
    "    for w in now_words:\n",
    "        if w not in word_freq:\n",
    "            word_freq[w] = 0\n",
    "        word_freq[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words = [(w,f) for w, f in word_freq.items() if f >= 2 and f <=5 and len(w) >= 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval 1 - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=25\n",
    "max_dev_freq=10000\n",
    "min_train_freq=25\n",
    "max_train_freq=10000\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_content_words = ((en_common_words & common_ref_words) - (es_stop_words | stop_words))\n",
    "en_content_words = {w for w in en_content_words if '¿' not in w and \"'\" not in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(en_content_words, en_content_words, en_word_utt_count, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"]), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, en_common_words, en_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = (get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), \n",
    "       get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join(sample_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_freq_content = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_freq_content, \n",
    "            open(os.path.join(m_cfg['data_path'], \n",
    "                              \"eval_en_freq_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval 2 - 500 randomly selected infrequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=2\n",
    "max_dev_freq=10\n",
    "min_train_freq=2\n",
    "max_train_freq=25\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_content_words = (en_common_words - (es_stop_words | stop_words))\n",
    "# en_content_words = {w for w in en_content_words if '¿' not in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(en_content_words, en_content_words, en_word_utt_count, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"]), 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, en_common_words, en_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = (get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), \n",
    "       get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join(sample_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_content = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_content, \n",
    "            open(os.path.join(m_cfg['data_path'], \n",
    "                              \"eval_en_rare_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval 3 - common es, en words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=2\n",
    "max_dev_freq=10000\n",
    "min_train_freq=2\n",
    "max_train_freq=100000\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_content_words = (en_common_words - (es_stop_words | stop_words))\n",
    "# en_content_words = {w for w in en_content_words if '¿' not in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_common_words = (es_common_words & en_common_words & common_ref_words)  - (es_stop_words | stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(es_en_common_words, en_content_words, en_word_utt_count, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"]), 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, en_common_words, en_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = (get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), \n",
    "       get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join(sample_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_content = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_content, \n",
    "            open(os.path.join(m_cfg['data_path'], \n",
    "                              \"eval_en_es_common_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_common_words = [w for w in es_common_words & en_common_words if len(w) >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval crisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises = set()\n",
    "with open(crises_lex_fname, \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        crises.update(line.strip().split())\n",
    "crises = list(crises)\n",
    "crises_stem = [stem(w) for w in crises]\n",
    "\n",
    "# new_crises = []\n",
    "# for t in crises:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_crises.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "# crises.extend(new_crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details = get_details_for_words(crises, (en_common_words & common_ref_words), en_word_utt_count,\n",
    "                                       min_dev_freq=2, \n",
    "                                       max_dev_freq=5000, \n",
    "                                       min_train_freq=2,\n",
    "                                       max_train_freq=5000,\n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(crises_details[\"words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" -- \".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \n",
    "                                                \"eval_en_crisis_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words = set(en_word_utt_count['train'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(en_words)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"howdy\".encode() in vocab_dict[\"en_w\"][\"w2i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = {}\n",
    "\n",
    "for w in tqdm(en_words):\n",
    "    s = []\n",
    "#     print(w)\n",
    "    w_syn = wn.synsets(w)\n",
    "    for item in w_syn:\n",
    "        for lm in item.lemma_names():\n",
    "            if lm in en_words:\n",
    "                s.append(lm)\n",
    "        # end for all lemmas\n",
    "    # end for syns\n",
    "    s.append(w)\n",
    "    syns[w] = list(set(s))\n",
    "# end for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_sim = {\"w\":{}, \"i\":{}}\n",
    "\n",
    "for w in tqdm(vocab_dict[\"en_w\"][\"w2i\"].keys()):\n",
    "    syn_sim[\"w\"][w] = [w.encode() for w in syns.get(w.decode(), [w.decode()])]\n",
    "    i = vocab_dict[\"en_w\"][\"w2i\"][w]\n",
    "    syn_sim[\"i\"][i] = [vocab_dict[\"en_w\"][\"w2i\"][j] for j in syn_sim[\"w\"][w]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(syn_sim[\"w\"].items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(syn_sim, open(os.path.join(m_cfg['data_path'], \"en_syns_train.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = {w:v for w, v in syns.items() if len(v) >= 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(haha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"en_w\"][\"i2w\"][14261]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_keys = set([w for w in sim_dict['w'] if len(sim_dict[\"w\"][w]) >= 2])\n",
    "syn_keys = set([w for w in syn_sim['w'] if len(syn_sim[\"w\"][w]) >= 2])\n",
    "print(len(sim_keys), len(syn_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join([w.decode() for w in syn_sim[\"w\"][b'run']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_keys - sim_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = \"hello\"\n",
    "w_syn = wn.synsets(w)\n",
    "for item in w_syn:\n",
    "    for lm in item.lemma_names():\n",
    "        if lm in en_words:\n",
    "            syns[w].append(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_set = []\n",
    "for synset in wn.synsets(\"hello\"):\n",
    "    for item in synset.lemma_names():\n",
    "        syn_set.append(item)\n",
    "print(syn_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns[1].lemmas()[3].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in syns:\n",
    "    print(s.lemmas()[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.lemmas[0] for s in syns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ES, EN common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_out_str(out_str):\n",
    "    out_str = out_str.replace(\"`\", \"\")\n",
    "    out_str = out_str.replace('\"', '')\n",
    "    out_str = out_str.replace('¿', '')\n",
    "    out_str = out_str.replace(\"''\", \"\")\n",
    "    out_str = out_str.strip()\n",
    "    return out_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_str(h):\n",
    "    out_str = \"\"\n",
    "    for w in [i.decode() for i in h]:\n",
    "        out_str += \"{0:s}\".format(w) if (w.startswith(\"'\") or w==\"n't\") else \" {0:s}\".format(w)\n",
    "\n",
    "    out_str = clean_out_str(out_str)\n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_es_en(es_words, en_words):\n",
    "    utt_es_en_stats = {\"common\":{}, \"es\": {}, \"en\": {}}\n",
    "    \n",
    "    utt_es_en_stats[\"es\"] = Counter(es_words)\n",
    "    utt_es_en_stats[\"en\"] = Counter(en_words)\n",
    "    \n",
    "    common_es_en = set(utt_es_en_stats[\"es\"].keys()) & set(utt_es_en_stats[\"en\"].keys())\n",
    "    \n",
    "    for w in common_es_en:\n",
    "        utt_es_en_stats[\"common\"][w] = min(utt_es_en_stats[\"es\"][w], utt_es_en_stats[\"en\"][w])\n",
    "    \n",
    "    return utt_es_en_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop_words = (set(nltk.corpus.stopwords.words(\"english\")) | \n",
    "                    set(nltk.corpus.stopwords.words(\"spanish\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_details(common_results, show=20, min_len=5, max_freq=2, filter_stop=False):\n",
    "    for k in common_results:\n",
    "        print(\"{0:10s} = {1:>10d} types\".format(k,len(common_results[k])))\n",
    "        \n",
    "    tot = {}\n",
    "    print(\"-\"*60)\n",
    "    for k in common_results:\n",
    "        tot[k] = sum(common_results[k].values())\n",
    "        print(\"{0:10s} = {1:>10d} tokens\".format(k,tot[k]))\n",
    "        \n",
    "    print(\"-\"*60)\n",
    "    print(\"common / es = {0:.2f}%\".format(tot[\"common\"]/tot[\"es\"]*100))\n",
    "    print(\"common / en = {0:.2f}%\".format(tot[\"common\"]/tot[\"en\"]*100))\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    common_words = sorted(common_results[\"common\"].items(), reverse=True, key=lambda t: t[1])\n",
    "    \n",
    "    if filter_stop:\n",
    "        common_words = [(w,c) for w,c in common_words \n",
    "                        if w not in all_stop_words and \n",
    "                        len(w) >= min_len and \"'\" not in w and common_results[\"en\"][w] <= max_freq]\n",
    "        \n",
    "    non_cv_words = {}\n",
    "    set_common_words = set(common_results[\"common\"].keys())\n",
    "    for w,c in common_results[\"en\"].items():\n",
    "        if ((w not in set_common_words) and (w not in all_stop_words) and \n",
    "            (len(w) >= min_len) and (\"'\" not in w) and \n",
    "            (common_results[\"en\"][w] <= max_freq)):\n",
    "            non_cv_words[w] = common_results[\"en\"][w]\n",
    "    \n",
    "    print(\"total content word types: {0:d}\".format(len(common_words)))\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    tot_content = sum([c for w,c in common_words])\n",
    "    print(\"total content tokens: {0:d}\".format(tot_content))\n",
    "    print(\"common content / es = {0:.2f}%\".format(tot_content/tot[\"es\"]*100))\n",
    "    print(\"common content / en = {0:.2f}%\".format(tot_content/tot[\"en\"]*100))\n",
    "    print(\"-\"*60)\n",
    "    print(\"Top common words\")\n",
    "    for w, c in common_words[:show]:\n",
    "        print(\"{0:20s} || {1:10d}\".format(w,c))\n",
    "        \n",
    "    return {w:c for w,c in common_words}, non_cv_words\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_level_es_en(set_key, ref_num=0):\n",
    "    utts_with_common = []\n",
    "    es_en_stats = {\"common\":{}, \"es\": {}, \"en\": {}}\n",
    "\n",
    "    for u in tqdm(map_dict[set_key], ncols=80):\n",
    "        es_words = get_out_str(map_dict[set_key][u][\"es_w\"]).strip().split()\n",
    "        if set_key == \"fisher_dev\":\n",
    "            en_words = get_out_str(map_dict[set_key][u][\"en_w\"][ref_num]).strip().split()\n",
    "        else:\n",
    "            en_words = get_out_str(map_dict[set_key][u][\"en_w\"]).strip().split()\n",
    "        \n",
    "        utt_es_en_stats = match_es_en(es_words, en_words)\n",
    "    \n",
    "        common_words = [w for w,c in utt_es_en_stats[\"common\"].items() \n",
    "                            if w not in all_stop_words \n",
    "                            and len(w) >= 5 and \"'\" not in w]\n",
    "        \n",
    "        #print(common_words)\n",
    "        \n",
    "        if len(common_words) > 0:\n",
    "            utts_with_common.append(u)\n",
    "        \n",
    "        #print(utt_es_en_stats)\n",
    "        for k in utt_es_en_stats:\n",
    "            for w in utt_es_en_stats[k]:\n",
    "                if w not in es_en_stats[k]:\n",
    "                    es_en_stats[k][w] = 0\n",
    "                es_en_stats[k][w] += utt_es_en_stats[k][w]\n",
    "        # end for update counts\n",
    "    # end for all utts\n",
    "    \n",
    "    print(\"utts with common words: {0:d} / {1:d}\".format(len(utts_with_common), \n",
    "                                                         len(map_dict[set_key])))\n",
    "    return es_en_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 3979/3979 [00:00<00:00, 11042.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utts with common words: 542 / 3979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_common = corpus_level_es_en(\"fisher_dev\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common     =        596 types\n",
      "es         =       4079 types\n",
      "en         =       2998 types\n",
      "------------------------------------------------------------\n",
      "common     =       3396 tokens\n",
      "es         =      40969 tokens\n",
      "en         =      40041 tokens\n",
      "------------------------------------------------------------\n",
      "common / es = 8.29%\n",
      "common / en = 8.48%\n",
      "------------------------------------------------------------\n",
      "total content word types: 125\n",
      "------------------------------------------------------------\n",
      "total content tokens: 125\n",
      "common content / es = 0.31%\n",
      "common content / en = 0.31%\n",
      "------------------------------------------------------------\n",
      "Top common words\n",
      "ricardo              ||          1\n",
      "natural              ||          1\n",
      "exact                ||          1\n",
      "milgred              ||          1\n",
      "mercedes             ||          1\n",
      "imposible            ||          1\n",
      "taballas             ||          1\n",
      "infalible            ||          1\n",
      "altar                ||          1\n",
      "swami                ||          1\n"
     ]
    }
   ],
   "source": [
    "dev_comm_cont, dev_non_cv = show_details(dev_common, show=10, \n",
    "                                         min_len=5, \n",
    "                                         max_freq=1, \n",
    "                                         filter_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_comm_cont[\"annie\"], dev_common[\"en\"][\"annie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dev_comm_cont.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 138819/138819 [00:11<00:00, 11892.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utts with common words: 18491 / 138819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_common = corpus_level_es_en(\"fisher_train\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common     =       4737 types\n",
      "es         =      32185 types\n",
      "en         =      18140 types\n",
      "------------------------------------------------------------\n",
      "common     =      92445 tokens\n",
      "es         =    1494776 tokens\n",
      "en         =    1440914 tokens\n",
      "------------------------------------------------------------\n",
      "common / es = 6.18%\n",
      "common / en = 6.42%\n",
      "------------------------------------------------------------\n",
      "total content word types: 2485\n",
      "------------------------------------------------------------\n",
      "total content tokens: 5010\n",
      "common content / es = 0.34%\n",
      "common content / en = 0.35%\n",
      "------------------------------------------------------------\n",
      "Top common words\n",
      "brenda               ||         10\n",
      "enrique              ||         10\n",
      "brasil               ||         10\n",
      "latina               ||         10\n",
      "jennifer             ||         10\n",
      "virus                ||         10\n",
      "fuentes              ||         10\n",
      "athens               ||         10\n",
      "arlington            ||         10\n",
      "maría                ||         10\n"
     ]
    }
   ],
   "source": [
    "train_comm_cont, train_non_cv = show_details(train_common, show=10, \n",
    "                                             min_len=5, \n",
    "                                             max_freq=10, \n",
    "                                             filter_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2485, 9243, 125, 990)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_comm_cont), len(train_non_cv), len(dev_comm_cont), len(dev_non_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5010, 23015, 125, 990)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_comm_cont.values()), sum(train_non_cv.values()), sum(dev_comm_cont.values()), sum(dev_non_cv.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dev_non_cv.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_words = set(train_comm_cont.keys()) & all_dev_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cv_words = (set(train_non_cv.keys())-in_vocab_rare_words) & all_dev_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2485, 84, 388)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_comm_cont.keys())), len(cv_words), len(non_cv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_noncv_words = {\"cv\": cv_words, \"noncv\": non_cv_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv_noncv_words, open(os.path.join(m_cfg[\"data_path\"],\"cv_noncv.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_common_words = {\"train\": train_comm_cont, \"dev\": dev_comm_cont, \n",
    "                        \"train_non_cv\": train_non_cv, \"dev_non_cv\": dev_non_cv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fbanks_80dim_nltk'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_cfg[\"data_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(content_common_words, open(os.path.join(m_cfg[\"data_path\"],\"common_content.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_words = get_out_str(map_dict[\"fisher_dev\"][\"20051009_182032_217_fsp-B-2\"][\"es_w\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mi nombre es carmen de chicago y tu'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"chemistry\" in dev_common[\"common\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_non_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_comm_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = set(dev_common[\"en\"].keys()) - set(train_common[\"en\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words = []\n",
    "with open(\"interspeech/sp_160hrs/ref_min-0_max-300.en1\", \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        t_words.extend(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40031"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words_count = Counter(t_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = set(dev_comm_cont.keys())\n",
    "k2 = set(t_words_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dev_comm_cont.keys()) & oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dev_non_cv.keys()) & oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 173, 173)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k1), len(k2), len(k1 & k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasty 1 2\n",
      "closer 1 2\n",
      "camping 1 2\n",
      "ticket 1 2\n",
      "highway 1 2\n",
      "olaya 1 2\n",
      "cover 1 2\n",
      "marketing 1 2\n",
      "health 1 2\n",
      "teenager 1 2\n",
      "tickets 1 2\n",
      "cultural 1 2\n",
      "emails 1 2\n",
      "brutal 1 2\n",
      "standing 1 2\n"
     ]
    }
   ],
   "source": [
    "for w in k1:\n",
    "    if dev_comm_cont[w] != t_words_count[w]:\n",
    "        print(w, dev_comm_cont[w], t_words_count[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words = []\n",
    "with open(\"interspeech/sp_160hrs/ref_min-0_max-300.en0\", \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        t_words.extend(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40031"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words_count = Counter(t_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dev_en = set(t_words_count.keys())\n",
    "all_train_en = set(train_common[\"en\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2997, 18140)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dev_en), len(all_train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = all_dev_en - all_train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([v for w,v in t_words_count.items() if w in oov])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_counter = {w:v for w,v in t_words_count.items() if w in oov}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(oov_counter, open(os.path.join(m_cfg[\"data_path\"],\"oov.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words in google preds not in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_words = []\n",
    "with open(\"google/beam_min-0_max-300.en\", \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        g_words.extend(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39705"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_word_counter = Counter(g_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2424"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_new_words = set(g_word_counter.keys()) - all_train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([v for w,v in g_word_counter.items() if w in g_new_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"craigslists\" in all_train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actory',\n",
       " 'addibility',\n",
       " 'alejandra',\n",
       " 'alist',\n",
       " 'animore',\n",
       " 'annex',\n",
       " 'anterest',\n",
       " 'arc',\n",
       " 'artifactive',\n",
       " 'audicio',\n",
       " 'audito',\n",
       " 'auround',\n",
       " 'austinian',\n",
       " 'backups',\n",
       " 'bandera',\n",
       " 'bandies',\n",
       " 'banic',\n",
       " 'barricans',\n",
       " 'baseballand',\n",
       " 'bonda',\n",
       " 'borton',\n",
       " 'bowls',\n",
       " 'calmadist',\n",
       " 'carmer',\n",
       " 'catalogs',\n",
       " 'charlottes',\n",
       " 'competed',\n",
       " 'compets',\n",
       " 'compromiser',\n",
       " 'contemoration',\n",
       " 'contentive',\n",
       " 'contraries',\n",
       " 'conversate',\n",
       " 'cooper',\n",
       " 'costable',\n",
       " 'cotial',\n",
       " 'couscos',\n",
       " 'craigslists',\n",
       " 'cristo',\n",
       " 'cupons',\n",
       " 'curity',\n",
       " 'dependantly',\n",
       " 'dequited',\n",
       " 'disquisition',\n",
       " 'distay',\n",
       " 'dividement',\n",
       " 'dremits',\n",
       " 'emilar',\n",
       " 'evidenced',\n",
       " 'exility',\n",
       " 'exports',\n",
       " \"exwife's\",\n",
       " 'fantastics',\n",
       " 'faxemary',\n",
       " 'faxs',\n",
       " 'fef',\n",
       " 'fem',\n",
       " 'fitter',\n",
       " 'godfathers',\n",
       " 'handuras',\n",
       " 'hypock',\n",
       " 'idealization',\n",
       " 'immodel',\n",
       " 'impolited',\n",
       " 'indest',\n",
       " 'indistivated',\n",
       " 'inexplained',\n",
       " 'infective',\n",
       " 'inguage',\n",
       " 'institutionalist',\n",
       " 'insula',\n",
       " 'internated',\n",
       " 'itality',\n",
       " 'janan',\n",
       " 'laska',\n",
       " \"lawyer's\",\n",
       " 'luba',\n",
       " 'malenato',\n",
       " 'marijuality',\n",
       " 'millionaile',\n",
       " 'monsters',\n",
       " 'montial',\n",
       " 'nightmale',\n",
       " 'observative',\n",
       " 'observes',\n",
       " 'omartic',\n",
       " 'orience',\n",
       " 'perview',\n",
       " 'pervise',\n",
       " 'pilars',\n",
       " 'pitter',\n",
       " 'pizzed',\n",
       " 'planeta',\n",
       " 'polosi',\n",
       " 'preached',\n",
       " 'premieved',\n",
       " 'preparable',\n",
       " 'presidently',\n",
       " 'prohibition',\n",
       " 'provinced',\n",
       " 'prudently',\n",
       " 'punishments',\n",
       " 'refurbish',\n",
       " 'runnings',\n",
       " 'serviced',\n",
       " 'sibling',\n",
       " 'skins',\n",
       " 'soccers',\n",
       " 'specifications',\n",
       " 'studys',\n",
       " 'supermarketing',\n",
       " 'syn',\n",
       " 'tairs',\n",
       " 'tasco',\n",
       " 'tasteman',\n",
       " 'temola',\n",
       " 'temors',\n",
       " 'terrial',\n",
       " 'theorical',\n",
       " 'tmore',\n",
       " 'toucher',\n",
       " 'trage',\n",
       " 'traumit',\n",
       " 'trobi',\n",
       " 'tyroters',\n",
       " 'unifys',\n",
       " 'vactivity',\n",
       " 'validations',\n",
       " 'vicition',\n",
       " 'victi',\n",
       " 'vocabulations',\n",
       " 'writting'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([v for w,v in g_word_counter.items() if w in (g_new_words & all_dev_en)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alejandra', 'annex', 'competed', 'idealization', 'institutionalist'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_new_words & oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alejandra', 'annex', 'catalogs', \"exwife's\", 'institutionalist', 'preached'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_new_words & oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alejandra',\n",
       " 'annex',\n",
       " 'competed',\n",
       " 'idealization',\n",
       " 'observes',\n",
       " 'preached',\n",
       " 'sibling'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_new_words & oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([v for w,v in g_word_counter.items() if w in (g_new_words & oov)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18140"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_common[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_train_words(word_freq, show=5, min_len=5, min_freq=1, max_freq=10):\n",
    "    tot = sum(word_freq.values())\n",
    "    print(\"-\"*60)    \n",
    "    print(\"total types={0:>10d}, tokens = {1:>10d}\".format(len(word_freq), tot))\n",
    "    print(\"-\"*60)    \n",
    "    \n",
    "    common_words = sorted(word_freq.items(), reverse=True, key=lambda t: t[1])\n",
    "    \n",
    "    common_words = [(w,c) for w,c in common_words \n",
    "                    if w not in all_stop_words and \n",
    "                    len(w) >= min_len and \n",
    "                    \"'\" not in w and \n",
    "                    word_freq[w] >= min_freq and\n",
    "                    word_freq[w] <= max_freq]\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    print(\"total filter word types: {0:d}\".format(len(common_words)))\n",
    "    \n",
    "    tot_content = sum([c for w,c in common_words])\n",
    "    print(\"total filter tokens: {0:d}\".format(tot_content))\n",
    "    print(\"filter content / en = {0:.2f}%\".format(tot_content/tot*100))\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    print(\"-\"*60)    \n",
    "    in_dev = all_dev_en & set([w for w,c in common_words])\n",
    "    in_dev_tokens = sum([t_words_count[w] for w in in_dev])\n",
    "    print(\"total filter word types in dev: {0:d}, tokens: {1:d}\".format(len(in_dev), in_dev_tokens))\n",
    "    print(\"-\"*60)    \n",
    "    \n",
    "    \n",
    "    sel_words = {w:c for w,c in common_words if w in in_dev}\n",
    "    print(\"Top common words\")\n",
    "    for w, c in sorted(sel_words.items(), reverse=True, key=lambda t:t[1])[:show]:\n",
    "        print(\"{0:20s} || {1:10d} || {2:10d}\".format(w,c, t_words_count[w]))\n",
    "        \n",
    "    return sel_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "total types=     18140, tokens =    1440914\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "total filter word types: 11728\n",
      "total filter tokens: 29923\n",
      "filter content / en = 2.08%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "total filter word types in dev: 472, tokens: 616\n",
      "------------------------------------------------------------\n",
      "Top common words\n",
      "televisions          ||         10 ||          1\n",
      "visits               ||         10 ||          1\n",
      "balls                ||         10 ||          1\n",
      "basement             ||         10 ||          3\n",
      "bless                ||         10 ||          1\n"
     ]
    }
   ],
   "source": [
    "train_rare = filter_train_words(train_common[\"en\"], min_len=5, min_freq=1, max_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "total types=     18140, tokens =    1440914\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "total filter word types: 1143\n",
      "total filter tokens: 56357\n",
      "filter content / en = 3.91%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "total filter word types in dev: 607, tokens: 1498\n",
      "------------------------------------------------------------\n",
      "Top common words\n",
      "marry                ||        100 ||          5\n",
      "needed               ||        100 ||          3\n",
      "building             ||        100 ||          1\n",
      "idaho                ||        100 ||         13\n",
      "hardly               ||         99 ||          4\n"
     ]
    }
   ],
   "source": [
    "train_medium = filter_train_words(train_common[\"en\"], min_len=5, min_freq=25, max_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "total types=     18140, tokens =    1440914\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "total filter word types: 386\n",
      "total filter tokens: 199438\n",
      "filter content / en = 13.84%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "total filter word types in dev: 365, tokens: 5167\n",
      "------------------------------------------------------------\n",
      "Top common words\n",
      "right                ||       7249 ||        163\n",
      "think                ||       6676 ||        120\n",
      "people               ||       6522 ||        183\n",
      "really               ||       4265 ||         72\n",
      "things               ||       3058 ||         91\n"
     ]
    }
   ],
   "source": [
    "train_frequent = filter_train_words(train_common[\"en\"], min_len=5, min_freq=150, max_freq=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tables = {\"rare\": train_rare, \"medium\": train_medium, \"frequent\": train_frequent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(freq_tables, open(os.path.join(m_cfg[\"data_path\"],\"word_freq.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
