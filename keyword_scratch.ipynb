{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from basics import *\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"sp2enw_mel-80_vocab-nltk/sp_1.0_h-256_e-128_drpt-rnn-.3_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fisher dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict = get_data_dicts(m_cfg)\n",
    "info_dict = pickle.load(open(\"fbanks_80dim_nltk/info.dict\", \"rb\"))\n",
    "sim_dict = pickle.load(open(\"./fbanks_80dim_nltk/mix_sim.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_len = 1\n",
    "top_k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(m_dict):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u]['en_w']) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u]['en_w']])\n",
    "        else:\n",
    "            for ref in m_dict[u]['en_w']:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train word types   |      17830\n",
      "# train word tokens  |    1497352\n"
     ]
    }
   ],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yes', 35054),\n",
       " (\"'s\", 24162),\n",
       " (\"n't\", 19184),\n",
       " ('like', 14334),\n",
       " ('well', 12354)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 24162), (\"n't\", 19184), (\"'m\", 5546), (\"'re\", 2832), (\"'ve\", 2392)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yes', 3652), (\"n't\", 1999), (\"'s\", 1866), ('like', 1826), ('know', 1294)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# dev word types     |       4835\n",
      "# dev word tokens    |     165206\n",
      "# oov word types     |       1011\n",
      "# oov word tokens    |       1599\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_only_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0%'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level - get train, dev frequency, and utts in which they occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_utt_count = {\"train\": {}, \"dev\": {}, \"train_utts\": {}, \"dev_utts\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17830, 12011)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_only_words), len(set([stem(w) for w in train_only_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138819/138819 [00:02<00:00, 52107.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for u in tqdm(map_dict[\"fisher_train\"].keys()):\n",
    "    for w in set(map_dict[\"fisher_train\"][u][\"en_w\"]):\n",
    "        curr_word = w.decode()\n",
    "        if curr_word not in word_utt_count[\"train\"]:\n",
    "            word_utt_count[\"train\"][curr_word] = 0\n",
    "            word_utt_count[\"train_utts\"][curr_word] = set()\n",
    "        word_utt_count[\"train\"][curr_word] += 1\n",
    "        word_utt_count[\"train_utts\"][curr_word].update({u})\n",
    "    # end for words in current utt\n",
    "# end for all utts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3979/3979 [00:00<00:00, 15016.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for u in tqdm(map_dict[\"fisher_dev\"].keys()):\n",
    "    for ref in map_dict[\"fisher_dev\"][u][\"en_w\"]:\n",
    "        for w in set(ref):\n",
    "            curr_word = w.decode()\n",
    "            if curr_word not in word_utt_count[\"dev\"]:\n",
    "                word_utt_count[\"dev\"][curr_word] = 0\n",
    "                word_utt_count[\"dev_utts\"][curr_word] = set()\n",
    "            word_utt_count[\"dev\"][curr_word] += 1            \n",
    "            word_utt_count[\"dev_utts\"][curr_word].update({u})\n",
    "        # end for words in current ref\n",
    "    # end for all references\n",
    "# end for all utts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_utts = set()\n",
    "for w in word_utt_count[\"train_utts\"]:\n",
    "    all_train_utts.update(word_utt_count[\"train_utts\"][w])\n",
    "# end for\n",
    "\n",
    "all_dev_utts = set()\n",
    "for w in word_utt_count[\"dev_utts\"]:\n",
    "    all_dev_utts.update(word_utt_count[\"dev_utts\"][w])\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138795, 3979)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_utts), len(all_dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17830, 4835)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"word types\")\n",
    "len(word_utt_count['train']), len(word_utt_count['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common word types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3824"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"common word types\")\n",
    "common_words = set(word_utt_count['train'].keys()) & set(word_utt_count['dev'].keys())\n",
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details_for_words(words, min_dev_freq, max_dev_freq, min_train_freq, min_len):\n",
    "    details = {\"words\": {}, \"train_utts\": set(), \"dev_utts\": set()}\n",
    "    \n",
    "    in_vocab_words = set(words) & set(common_words)\n",
    "    print(\"number of in-vocab words = {0:d}\".format(len(in_vocab_words)))\n",
    "\n",
    "    for w in in_vocab_words:\n",
    "        t_count, d_count = word_utt_count[\"train\"][w], word_utt_count[\"dev\"][w]\n",
    "        if ((d_count >= min_dev_freq) and \n",
    "            (d_count <= max_dev_freq) and\n",
    "            (len(w) >= min_len) and\n",
    "            (t_count >= min_train_freq)):\n",
    "            details[\"words\"][w] = {\"train\": t_count, \"dev\": d_count}\n",
    "            details[\"train_utts\"].update(word_utt_count[\"train_utts\"][w])\n",
    "            details[\"dev_utts\"].update(word_utt_count[\"dev_utts\"][w])\n",
    "        # end meets criteria\n",
    "    # end for in-vocab word\n",
    "    return details\n",
    "# end function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration(utts, key):\n",
    "    dur = 0\n",
    "    utts_not_found = []\n",
    "    for u in utts:\n",
    "        if u not in info_dict[key]:\n",
    "            #print(\"argh!\", u)\n",
    "            utts_not_found.append(u)\n",
    "        else:\n",
    "            dur += (info_dict[key][u]['sp'] * 10)\n",
    "    dur = dur / 60 / 60 / 1000\n",
    "    print(\"-\"*80)\n",
    "    print(\"{0:d} total utts\".format(len(utts)))\n",
    "    print(\"{0:d} not found\".format(len(utts_not_found)))\n",
    "    print(\"selected utts from {0:s} -- duration = {1:.2f} hours\".format(key, dur))\n",
    "    return dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(words_list):\n",
    "    out = {\"w2i\":{}, \"i2w\":{}, \"freq\":{}, \"freq_dev\":{}}\n",
    "    START_VOCAB = [PAD, GO, EOS, UNK]\n",
    "    for w in START_VOCAB:\n",
    "        out['w2i'][w] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][w] = 1\n",
    "    #for w in words_list['words']:\n",
    "    sorted_w = sorted(words_list['words'].items(), reverse=True, key=lambda t: t[1]['train'])\n",
    "    for w in sorted_w:\n",
    "        encoded_word = w[0].encode()\n",
    "        out[\"w2i\"][encoded_word] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][encoded_word] = w[1][\"train\"]\n",
    "        out[\"freq_dev\"][encoded_word] = w[1][\"dev\"]\n",
    "\n",
    "    out[\"i2w\"] = {val:key for key, val in out[\"w2i\"].items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "138795 total utts\n",
      "89 not found\n",
      "selected utts from fisher_train -- duration = 161.62 hours\n",
      "--------------------------------------------------------------------------------\n",
      "3979 total utts\n",
      "2 not found\n",
      "selected utts from fisher_dev -- duration = 4.35 hours\n"
     ]
    }
   ],
   "source": [
    "train_dur, dev_dur = get_duration(all_train_utts, key=\"fisher_train\"), get_duration(all_dev_utts, key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - randomly selected frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 3824\n"
     ]
    }
   ],
   "source": [
    "terms_of_interest = get_details_for_words(common_words, \n",
    "                                          min_dev_freq=10, \n",
    "                                          max_dev_freq=100, \n",
    "                                          min_train_freq=100, \n",
    "                                          min_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words meeting criteria = 372\n"
     ]
    }
   ],
   "source": [
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "62610 total utts\n",
      "35 not found\n",
      "selected utts from fisher_train -- duration = 109.48 hours\n",
      "--------------------------------------------------------------------------------\n",
      "2084 total utts\n",
      "1 not found\n",
      "selected utts from fisher_dev -- duration = 3.30 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 100\n"
     ]
    }
   ],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, \n",
    "                                              min_dev_freq=10, \n",
    "                                              max_dev_freq=100, \n",
    "                                              min_train_freq=100, \n",
    "                                              min_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words meeting criteria = 100\n"
     ]
    }
   ],
   "source": [
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "23284 total utts\n",
      "15 not found\n",
      "selected utts from fisher_train -- duration = 44.29 hours\n",
      "--------------------------------------------------------------------------------\n",
      "999 total utts\n",
      "1 not found\n",
      "selected utts from fisher_dev -- duration = 1.77 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['around',\n",
       " 'outside',\n",
       " 'salsa',\n",
       " 'america',\n",
       " 'classes',\n",
       " 'terrible',\n",
       " 'atlanta',\n",
       " 'least',\n",
       " 'email',\n",
       " 'father']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_top_100_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_top_100_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_top_100_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2 - topics as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_fname= \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\"\n",
    "topics_fname = \"../criseslex/fsp06_topics_in_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [ \"peace\", \"Music\", \"Marriage\", \"Religion\", \"Cell phones\", \n",
    "           \"Dating\", \"Telemarketing and SPAM\", \"Politics\", \"Travel\", \n",
    "           \"Technical devices\", \"Healthcare\", \"Advertisements\", \"Power\", \n",
    "           \"Occupations\", \"Movies\", \"Welfare\", \"Breaking up\", \"Location\", \n",
    "           \"Justice\", \"Memories\", \"Crime\", \"Violence against women\", \"Equality\", \n",
    "            \"Housing\", \"Immigration\",     \n",
    "            # new topics\n",
    "           \"Interracial\", \"Christians\", \"muslims\", \"jews\", \"e-mail\", \n",
    "           \"phone\", \"democracy\", \"Democratic\", \"Republican\", \"technology\", \n",
    "           \"leadership\", \"community\", \"jury\", \"police\", \"inequality\", \n",
    "           \"renting\", \"Violence\", \"immigrants\", \"immigrant\", \"skilled\", \n",
    "           \"Telemarketing\", \"SPAM\", \"skill\", \"job\", \"health\", \"mobile\", \n",
    "            \"ads\", \"physical\", \"emotional\", \"bubble\", \"rent\", \"economy\", \n",
    "            \"abuse\", \"women\", \"city\", \"country\", \"suburban\", \"dollar\", \n",
    "            \"united states\", \"laws\", \"phone\", \"race\", \"biracial\", \"interracial\", \n",
    "            \"marriage\", \"lyrics\", \"sexuality\", \"medicine\", \"television\", \"european\",\n",
    "            \"home\", \"protect\", \"spouse\", \"language\", \"cellphone\", \"money\",\n",
    "            \"doctor\", \"insurance\", \"cigarettes\", \"alcohol\", \"income\", \"salary\",\n",
    "            \"class\", \"censor\", \"rating\", \"programs\", \"government\",\n",
    "            \"relationship\", \"legal\", \"event\", \"life\", \"safe\", \"victim\", \"cops\",\n",
    "            \"wage\", \"illegal\"\n",
    "            ]\n",
    "topics = list(set(t.lower() for t in topics))\n",
    "topics_stem = [stem(t) for t in topics]\n",
    "\n",
    "# add similar topic words\n",
    "new_topics = []\n",
    "for t in topics:\n",
    "    if t.encode() in sim_dict['w']:\n",
    "        new_topics.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "topics.extend(new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 170\n",
      "total words meeting criteria = 30\n"
     ]
    }
   ],
   "source": [
    "topics_details = get_details_for_words(topics, \n",
    "                                       min_dev_freq=10, \n",
    "                                       max_dev_freq=100, \n",
    "                                       min_train_freq=100, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(topics_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "9595 total utts\n",
      "4 not found\n",
      "selected utts from fisher_train -- duration = 20.29 hours\n",
      "--------------------------------------------------------------------------------\n",
      "312 total utts\n",
      "0 not found\n",
      "selected utts from fisher_dev -- duration = 0.59 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(topics_details[\"train_utts\"], key=\"fisher_train\"), get_duration(topics_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tv\n",
      "dollars\n",
      "country\n",
      "language\n",
      "job\n",
      "police\n",
      "religious\n",
      "religions\n",
      "relationship\n",
      "phone\n",
      "race\n",
      "movies\n",
      "politics\n",
      "immigration\n",
      "crime\n",
      "program\n",
      "rent\n",
      "government\n",
      "class\n",
      "life\n",
      "classes\n",
      "television\n",
      "marriage\n",
      "travel\n",
      "programs\n",
      "christian\n",
      "women\n",
      "jury\n",
      "home\n",
      "europe\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(list(topics_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_topics_vocab = create_vocab(topics_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_topics_vocab, open(os.path.join(m_cfg['data_path'], \"bow_topics_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - crises terms as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises = set()\n",
    "with open(crises_lex_fname, \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        crises.update(line.strip().split())\n",
    "crises = list(crises)\n",
    "crises_stem = [stem(w) for w in crises]\n",
    "\n",
    "# new_crises = []\n",
    "# for t in crises:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_crises.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "# crises.extend(new_crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 123\n",
      "total words meeting criteria = 42\n"
     ]
    }
   ],
   "source": [
    "crises_details = get_details_for_words(crises, \n",
    "                                       min_dev_freq=10, \n",
    "                                       max_dev_freq=100, \n",
    "                                       min_train_freq=100, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "14444 total utts\n",
      "5 not found\n",
      "selected utts from fisher_train -- duration = 29.37 hours\n",
      "--------------------------------------------------------------------------------\n",
      "602 total utts\n",
      "0 not found\n",
      "selected utts from fisher_dev -- duration = 1.17 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lives\n",
      "free\n",
      "lost\n",
      "police\n",
      "stay\n",
      "morning\n",
      "high\n",
      "town\n",
      "change\n",
      "make\n",
      "black\n",
      "situation\n",
      "return\n",
      "give\n",
      "waiting\n",
      "areas\n",
      "public\n",
      "huge\n",
      "service\n",
      "found\n",
      "number\n",
      "kill\n",
      "love\n",
      "news\n",
      "government\n",
      "gets\n",
      "coming\n",
      "life\n",
      "terrible\n",
      "send\n",
      "remember\n",
      "saying\n",
      "died\n",
      "women\n",
      "girl\n",
      "first\n",
      "case\n",
      "leave\n",
      "home\n",
      "water\n",
      "watch\n",
      "need\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \"bow_crises_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fbanks_80dim_nltk'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_cfg['data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_crises_vocab.dict\t      info.dict\r\n",
      "bow_top_100_words_vocab.dict  kaldi_segment_map.dict\r\n",
      "bow_topics_vocab.dict\t      map.dict\r\n",
      "buckets_sp.dict\t\t      mix_sim.dict\r\n",
      "callhome_devtest\t      pre_trained_sim.dict\r\n",
      "callhome_evltest\t      rev_map.dict\r\n",
      "callhome_train\t\t      sim.dict\r\n",
      "ch_train_vocab.dict\t      train_reduced_vocab_enw.dict\r\n",
      "fisher_dev\t\t      train_top_K_enw_1000.dict\r\n",
      "fisher_dev2\t\t      train_top_K_enw.dict\r\n",
      "fisher_test\t\t      train_vocab.dict\r\n",
      "fisher_train\r\n"
     ]
    }
   ],
   "source": [
    "!ls fbanks_80dim_nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
