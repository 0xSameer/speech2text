{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from basics import *\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"interspeech_new_vocab/sp_160hrs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fisher dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict = get_data_dicts(m_cfg)\n",
    "info_dict = pickle.load(open(\"fbanks_80dim_nltk/info.dict\", \"rb\"))\n",
    "sim_dict = pickle.load(open(\"./fbanks_80dim_nltk/mix_sim.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in map_dict[\"fisher_train\"]:\n",
    "    train_text.append(\" \".join([w.decode() for w in map_dict[\"fisher_train\"][u][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_to_dump = \"\\n\".join(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../subword-nmt/fisher_train.en\", \"w\") as out_f:\n",
    "    out_f.write(train_text_to_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./learn_joint_bpe_and_vocab.py --input {train_file}.L1 {train_file}.L2 -s {num_operations} -o {codes_file} --write-vocabulary {vocab_file}.L1 {vocab_file}.L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.Load(\"test/test_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_len = 1\n",
    "top_k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_stop_words = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "len(es_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(m_dict, key=\"en_w\"):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u][key]) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u][key]])\n",
    "        else:\n",
    "            for ref in m_dict[u][key]:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(map_dict['fisher_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))\n",
    "\n",
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in train\n",
    "es_train_words = get_words(map_dict['fisher_train'], key=\"es_w\")\n",
    "es_train_words_top_k = [(w,f) for w, f in sorted(es_train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in es_stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "es_train_only_words = set(es_train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(es_train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(es_train_words.values())))\n",
    "\n",
    "es_dev_words = get_words(map_dict['fisher_dev'], key=\"es_w\")\n",
    "es_dev_words_top_k = [(w,f) for w, f in sorted(es_dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in es_stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "es_dev_only_words = set(es_dev_words.keys())\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(es_dev_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(es_dev_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_top_k[:5], es_train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words_top_k[:5], es_dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_oov_words = {w:f for w,f in es_dev_words.items() if w not in es_train_only_words}\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(es_oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(es_oov_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level - get train, dev frequency, and utts in which they occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_only_words), len(set([stem(w) for w in train_only_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_level_details(word_key):\n",
    "    word_utt_count = {\"train\": {}, \"dev\": {}, \"train_utts\": {}, \"dev_utts\": {}}\n",
    "    for u in tqdm(map_dict[\"fisher_train\"].keys()):\n",
    "        for w in set(map_dict[\"fisher_train\"][u][word_key]):\n",
    "            curr_word = w.decode()\n",
    "            if curr_word not in word_utt_count[\"train\"]:\n",
    "                word_utt_count[\"train\"][curr_word] = 0\n",
    "                word_utt_count[\"train_utts\"][curr_word] = set()\n",
    "            word_utt_count[\"train\"][curr_word] += 1\n",
    "            word_utt_count[\"train_utts\"][curr_word].update({u})\n",
    "        # end for words in current utt\n",
    "    # end for all utts\n",
    "    for u in tqdm(map_dict[\"fisher_dev\"].keys()):\n",
    "        if word_key == \"en_w\":\n",
    "            for ref in map_dict[\"fisher_dev\"][u][word_key]:\n",
    "                for w in set(ref):\n",
    "                    curr_word = w.decode()\n",
    "                    if curr_word not in word_utt_count[\"dev\"]:\n",
    "                        word_utt_count[\"dev\"][curr_word] = 0\n",
    "                        word_utt_count[\"dev_utts\"][curr_word] = set()\n",
    "                    word_utt_count[\"dev\"][curr_word] += 1            \n",
    "                    word_utt_count[\"dev_utts\"][curr_word].update({u})\n",
    "                # end for words in current ref\n",
    "            # end for all references\n",
    "        # end if multiple references\n",
    "        else:\n",
    "            ref = map_dict[\"fisher_dev\"][u][word_key]\n",
    "            for w in set(ref):\n",
    "                curr_word = w.decode()\n",
    "                if curr_word not in word_utt_count[\"dev\"]:\n",
    "                    word_utt_count[\"dev\"][curr_word] = 0\n",
    "                    word_utt_count[\"dev_utts\"][curr_word] = set()\n",
    "                word_utt_count[\"dev\"][curr_word] += 1            \n",
    "                word_utt_count[\"dev_utts\"][curr_word].update({u})\n",
    "            \n",
    "    # end for all utts\n",
    "    all_train_utts = set()\n",
    "    for w in word_utt_count[\"train_utts\"]:\n",
    "        all_train_utts.update(word_utt_count[\"train_utts\"][w])\n",
    "    # end for\n",
    "\n",
    "    all_dev_utts = set()\n",
    "    for w in word_utt_count[\"dev_utts\"]:\n",
    "        all_dev_utts.update(word_utt_count[\"dev_utts\"][w])\n",
    "    # end for\n",
    "    \n",
    "    return word_utt_count, all_train_utts, all_dev_utts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count, en_train_utts, en_dev_utts = get_word_level_details(\"en_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_train_utts), len(en_dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_word_utt_count, es_train_utts, es_dev_utts = get_word_level_details(\"es_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_train_utts), len(es_dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"word types\")\n",
    "print(len(en_word_utt_count['train']), len(en_word_utt_count['dev']))\n",
    "print(\"common word types\")\n",
    "en_common_words = set(en_word_utt_count['train'].keys()) & set(en_word_utt_count['dev'].keys())\n",
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"word types\")\n",
    "print(len(es_word_utt_count['train']), len(es_word_utt_count['dev']))\n",
    "print(\"common word types\")\n",
    "es_common_words = set(es_word_utt_count['train'].keys()) & set(es_word_utt_count['dev'].keys())\n",
    "len(es_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details_for_words(words, common_words, word_utt_count, \n",
    "                          min_dev_freq, max_dev_freq, min_train_freq, max_train_freq, min_len):\n",
    "    details = {\"words\": {}, \"train_utts\": set(), \"dev_utts\": set()}\n",
    "    \n",
    "    in_vocab_words = set(words) & set(common_words)\n",
    "    print(\"number of in-vocab words = {0:d}\".format(len(in_vocab_words)))\n",
    "\n",
    "    for w in in_vocab_words:\n",
    "        t_count, d_count = len(word_utt_count[\"train_utts\"][w]), len(word_utt_count[\"dev_utts\"][w])\n",
    "        if ((d_count >= min_dev_freq) and \n",
    "            (d_count <= max_dev_freq) and\n",
    "            (len(w) >= min_len) and\n",
    "            (t_count >= min_train_freq) and \n",
    "            (t_count <= max_train_freq)):\n",
    "            details[\"words\"][w] = {\"train\": t_count, \"dev\": d_count}\n",
    "            details[\"train_utts\"].update(word_utt_count[\"train_utts\"][w])\n",
    "            details[\"dev_utts\"].update(word_utt_count[\"dev_utts\"][w])\n",
    "        # end meets criteria\n",
    "    # end for in-vocab word\n",
    "    return details\n",
    "# end function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration(utts, key):\n",
    "    dur = 0\n",
    "    utts_not_found = []\n",
    "    for u in utts:\n",
    "        if u not in info_dict[key]:\n",
    "            #print(\"argh!\", u)\n",
    "            utts_not_found.append(u)\n",
    "        else:\n",
    "            dur += (info_dict[key][u]['sp'] * 10)\n",
    "    dur = dur / 60 / 60 / 1000\n",
    "    print(\"-\"*80)\n",
    "    print(\"{0:d} total utts\".format(len(utts)))\n",
    "    print(\"{0:d} not found\".format(len(utts_not_found)))\n",
    "    print(\"selected utts from {0:s} -- duration = {1:.2f} hours\".format(key, dur))\n",
    "    return dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(words_list):\n",
    "    out = {\"w2i\":{}, \"i2w\":{}, \"freq\":{}, \"freq_dev\":{}}\n",
    "    START_VOCAB = [PAD, GO, EOS, UNK]\n",
    "    for w in START_VOCAB:\n",
    "        out['w2i'][w] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][w] = 1\n",
    "    #for w in words_list['words']:\n",
    "    sorted_w = sorted(words_list['words'].items(), reverse=True, key=lambda t: t[1]['train'])\n",
    "    for w in sorted_w:\n",
    "        encoded_word = w[0].encode()\n",
    "        out[\"w2i\"][encoded_word] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][encoded_word] = w[1][\"train\"]\n",
    "        out[\"freq_dev\"][encoded_word] = w[1][\"dev\"]\n",
    "\n",
    "    out[\"i2w\"] = {val:key for key, val in out[\"w2i\"].items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dur, dev_dur = get_duration(en_train_utts, key=\"fisher_train\"), get_duration(en_dev_utts, key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=10\n",
    "max_dev_freq=10000\n",
    "min_train_freq=50 \n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(common_words, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_top_500_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_top_500_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_top_500_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - randomly selected frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=10 \n",
    "max_dev_freq=100\n",
    "min_train_freq=100\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(common_words, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_top_100_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_top_100_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_top_100_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2 - topics as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_fname= \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\"\n",
    "topics_fname = \"../criseslex/fsp06_topics_in_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [ \"peace\", \"Music\", \"Marriage\", \"Religion\", \"Cell phones\", \n",
    "           \"Dating\", \"Telemarketing and SPAM\", \"Politics\", \"Travel\", \n",
    "           \"Technical devices\", \"Healthcare\", \"Advertisements\", \"Power\", \n",
    "           \"Occupations\", \"Movies\", \"Welfare\", \"Breaking up\", \"Location\", \n",
    "           \"Justice\", \"Memories\", \"Crime\", \"Violence against women\", \"Equality\", \n",
    "            \"Housing\", \"Immigration\",     \n",
    "            # new topics\n",
    "           \"Interracial\", \"Christians\", \"muslims\", \"jews\", \"e-mail\", \n",
    "           \"phone\", \"democracy\", \"Democratic\", \"Republican\", \"technology\", \n",
    "           \"leadership\", \"community\", \"jury\", \"police\", \"inequality\", \n",
    "           \"renting\", \"Violence\", \"immigrants\", \"immigrant\", \"skilled\", \n",
    "           \"Telemarketing\", \"SPAM\", \"skill\", \"job\", \"health\", \"mobile\", \n",
    "            \"ads\", \"physical\", \"emotional\", \"bubble\", \"rent\", \"economy\", \n",
    "            \"abuse\", \"women\", \"city\", \"country\", \"suburban\", \"dollar\", \n",
    "            \"united states\", \"laws\", \"phone\", \"race\", \"biracial\", \"interracial\", \n",
    "            \"marriage\", \"lyrics\", \"sexuality\", \"medicine\", \"television\", \"european\",\n",
    "            \"home\", \"protect\", \"spouse\", \"language\", \"cellphone\", \"money\",\n",
    "            \"doctor\", \"insurance\", \"cigarettes\", \"alcohol\", \"income\", \"salary\",\n",
    "            \"class\", \"censor\", \"rating\", \"programs\", \"government\",\n",
    "            \"relationship\", \"legal\", \"event\", \"life\", \"safe\", \"victim\", \"cops\",\n",
    "            \"wage\", \"illegal\"\n",
    "            ]\n",
    "topics = list(set(t.lower() for t in topics))\n",
    "topics_stem = [stem(t) for t in topics]\n",
    "\n",
    "# add similar topic words\n",
    "new_topics = []\n",
    "# for t in topics:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_topics.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "topics.extend(new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_details = get_details_for_words(topics, en_common_words, en_word_utt_count, \n",
    "                                       min_dev_freq=5, \n",
    "                                       max_dev_freq=10000, \n",
    "                                       min_train_freq=10, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(topics_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(topics_details[\"train_utts\"], key=\"fisher_train\"), get_duration(topics_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(list(topics_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_details['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_topics_vocab = create_vocab(topics_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = pickle.load(open(os.path.join(m_cfg['data_path'], \"bow_topics_vocab.dict\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(haha['w2i']) & set(bow_topics_vocab['w2i'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_topics_vocab, open(os.path.join(m_cfg['data_path'], \"bow_topics_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - crises terms as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises = set()\n",
    "with open(crises_lex_fname, \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        crises.update(line.strip().split())\n",
    "crises = list(crises)\n",
    "crises_stem = [stem(w) for w in crises]\n",
    "\n",
    "# new_crises = []\n",
    "# for t in crises:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_crises.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "# crises.extend(new_crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details = get_details_for_words(crises, en_common_words, en_word_utt_count,\n",
    "                                       min_dev_freq=10, \n",
    "                                       max_dev_freq=1000, \n",
    "                                       min_train_freq=100, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \"bow_crises_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details = get_details_for_words(crises, en_common_words, en_word_utt_count,\n",
    "                                       min_dev_freq=5, \n",
    "                                       max_dev_freq=1000, \n",
    "                                       min_train_freq=50, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \"bow_crises_vocab_more.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Spanish - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=18\n",
    "max_dev_freq=10000\n",
    "min_train_freq=200\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(es_common_words,\n",
    "                                          es_common_words,\n",
    "                                          es_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"].keys()), len(terms_of_interest[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_terms = [\"bueno\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms,\n",
    "                                             es_common_words,\n",
    "                                             es_word_utt_count,\n",
    "                                              min_dev_freq=min_dev_freq, \n",
    "                                              max_dev_freq=max_dev_freq, \n",
    "                                              min_train_freq=min_train_freq, \n",
    "                                              min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_terms_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details[\"train_utts\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details['words']['bueno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab['freq'][b'bueno'], bow_es_top_words_vocab['freq_dev'][b'bueno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_100word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_1word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!su s1444673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_100word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_top_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details['words']['colorado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(w, sample_terms_details['words'][w]['train']) \n",
    "       for w in sample_terms_details['words']], reverse=True, key=lambda t: t[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(w, sample_terms_details['words'][w]['dev']) \n",
    "       for w in sample_terms_details['words']], reverse=True, key=lambda t: t[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg['data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls fbanks_80dim_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg[\"sim_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'pre_trained_sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w'][b'sure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'mix_sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in mix_sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in [(w.decode(),[j.decode() for j in i]) for w, i in mix_sim_dict['w'].items() if len(i) > 2]:\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'pre_trained_sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in pre_sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in [(w.decode(),[j.decode() for j in i]) for w, i in pre_sim_dict['w'].items() if len(i) > 2]:\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sim_dict['w'][b'sure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in [(w.decode(),[j.decode() for j in i]) for w, i in sim_dict['w'].items() if len(i) > 2]:\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_words = [w for w, i in pre_sim_dict['w'].items() if len(i)>1]\n",
    "fisher_words = [w for w, i in sim_dict['w'].items() if len(i)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_words), len(fisher_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_only = set(pre_words) - set(fisher_words)\n",
    "fisher_only = set(fisher_words) - set(pre_words)\n",
    "common_only = set(pre_words) & set(fisher_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_only), len(fisher_only),  len(common_only), (len(set(pre_words) | set(fisher_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in pre_sim_dict['w'].items() if len(i) > 2 and w in pre_only]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in sim_dict['w'].items() if len(i) > 2 and w in fisher_only]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in pre_sim_dict['w'].items() \n",
    "                    if (len(i) > 1 and w in common_only)]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in sim_dict['w'].items() \n",
    "                    if (len(i) > 1 and w in common_only)]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(curr_set, word_type, max_len=1):\n",
    "    found_count = 0\n",
    "    eng_tokens = []\n",
    "    for utt in map_dict[curr_set]:\n",
    "        if word_type.encode() in map_dict[curr_set][utt][\"es_w\"] and len(map_dict[curr_set][utt][\"es_w\"]) <= max_len:\n",
    "            found_count+=1\n",
    "            if curr_set == \"fisher_train\":\n",
    "                eng_tokens.append(\" \".join([w.decode() for w in map_dict[curr_set][utt][\"en_w\"]]))\n",
    "            else:\n",
    "                for r in map_dict[curr_set][utt][\"en_w\"]:\n",
    "                    eng_tokens.append(\" \".join([w.decode() for w in r]))\n",
    "    print(found_count, len(map_dict[curr_set]), \"{0:.2f}\".format(found_count / len(map_dict[curr_set]) * 100))\n",
    "    print(len(set(eng_tokens)))\n",
    "    return Counter(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = check_word(\"fisher_train\", \"si\", 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = check_word(\"fisher_dev\", \"mhm\", 1)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(set([i[0] for i in t.most_common(10)]) and set([i[0] for i in d.most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_count = 0\n",
    "eng_tokens = []\n",
    "curr_set= \"fisher_train\"\n",
    "for utt in map_dict[curr_set]:\n",
    "    if b\"claro\" in map_dict[curr_set][utt][\"es_w\"] and len(map_dict[curr_set][utt][\"es_w\"]) <= 1:\n",
    "        found_count+=1\n",
    "        eng_tokens.append(\" \".join([w.decode() for w in map_dict[curr_set][utt][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count, len(map_dict[curr_set]), found_count / len(map_dict[curr_set]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count = 0\n",
    "eng_tokens = []\n",
    "curr_set= \"fisher_dev\"\n",
    "for utt in map_dict[curr_set]:\n",
    "    if b\"claro\" in map_dict[curr_set][utt][\"es_w\"] and len(map_dict[curr_set][utt][\"es_w\"]) <= 1:\n",
    "        found_count+=1\n",
    "        eng_tokens.append(\" \".join([w.decode() for w in map_dict[curr_set][utt][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count, len(map_dict[curr_set]), found_count / len(map_dict[curr_set]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(eng_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_names = [\"ref_min-0_max-300.en{0:d}\".format(i) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_names = []\n",
    "google_ref_names = []\n",
    "edin_ref_names = []\n",
    "for r in ref_names:\n",
    "    google_ref_names.append(os.path.join(\"google\", r))\n",
    "    edin_ref_names.append(os.path.join(\"./sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\", r))\n",
    "#     all_ref_names.append(os.path.join(\"sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\", r))\n",
    "    \n",
    "all_ref_names = google_ref_names + edin_ref_names    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_lines_in_file(fname):\n",
    "    all_lines = []\n",
    "    with open(fname, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            all_lines.append(set(line.strip().split()))\n",
    "    return all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_in_file(fname):\n",
    "    all_words = []\n",
    "    with open(fname, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            all_words.extend(line.strip().split())\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_ref = get_all_words_in_file(all_ref_names[0])\n",
    "all_words_ref = get_all_words_in_file(all_ref_names[0])\n",
    "\n",
    "for r in all_ref_names[1:]:\n",
    "    common_words_ref &= get_all_words_in_file(r)\n",
    "    all_words_ref |= get_all_words_in_file(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_words_ref), len(all_words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_common_words_ref = get_all_words_in_file(google_ref_names[0])\n",
    "google_all_words_ref = get_all_words_in_file(google_ref_names[0])\n",
    "\n",
    "for r in google_ref_names[1:]:\n",
    "    google_common_words_ref &= get_all_words_in_file(r)\n",
    "    google_all_words_ref |= get_all_words_in_file(r)\n",
    "print(len(google_common_words_ref), len(google_all_words_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edin_common_words_ref = get_all_words_in_file(edin_ref_names[0])\n",
    "edin_all_words_ref = get_all_words_in_file(edin_ref_names[0])\n",
    "\n",
    "for r in edin_ref_names[1:]:\n",
    "    edin_common_words_ref &= get_all_words_in_file(r)\n",
    "    edin_all_words_ref |= get_all_words_in_file(r)\n",
    "print(len(edin_common_words_ref), len(edin_all_words_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(google_common_words_ref & edin_common_words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ref_words = google_common_words_ref & edin_common_words_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_ref_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(google_all_words_ref - edin_all_words_ref), len(edin_all_words_ref - google_all_words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_lines = {}\n",
    "for i,r in enumerate(edin_ref_names):\n",
    "    all_ref_lines[i] = read_all_lines_in_file(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_all_refs = []\n",
    "\n",
    "for i in range(len(all_ref_lines[0])):\n",
    "    words_ref = all_ref_lines[0][i]\n",
    "    for j in range(1,4):\n",
    "        words_ref &= all_ref_lines[j][i]\n",
    "    words_in_all_refs.append(words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "\n",
    "for ref in words_in_all_refs:\n",
    "    now_words = ref & common_ref_words - stop_words - es_stop_words\n",
    "    for w in now_words:\n",
    "        if w not in word_freq:\n",
    "            word_freq[w] = 0\n",
    "        word_freq[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words = [(w,f) for w, f in word_freq.items() if f >= 2 and f <=5 and len(w) >= 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval 1 - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=25\n",
    "max_dev_freq=10000\n",
    "min_train_freq=25\n",
    "max_train_freq=10000\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_content_words = ((en_common_words & common_ref_words) - (es_stop_words | stop_words))\n",
    "en_content_words = {w for w in en_content_words if '¿' not in w and \"'\" not in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(en_content_words, en_content_words, en_word_utt_count, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"]), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, en_common_words, en_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = (get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), \n",
    "       get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join(sample_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_freq_content = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_freq_content, \n",
    "            open(os.path.join(m_cfg['data_path'], \n",
    "                              \"eval_en_freq_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval 2 - 500 randomly selected infrequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=2\n",
    "max_dev_freq=10\n",
    "min_train_freq=2\n",
    "max_train_freq=25\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_content_words = (en_common_words - (es_stop_words | stop_words))\n",
    "# en_content_words = {w for w in en_content_words if '¿' not in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(en_content_words, en_content_words, en_word_utt_count, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"]), 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, en_common_words, en_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = (get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), \n",
    "       get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join(sample_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_content = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_content, \n",
    "            open(os.path.join(m_cfg['data_path'], \n",
    "                              \"eval_en_rare_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval 3 - common es, en words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=2\n",
    "max_dev_freq=10000\n",
    "min_train_freq=2\n",
    "max_train_freq=100000\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_content_words = (en_common_words - (es_stop_words | stop_words))\n",
    "# en_content_words = {w for w in en_content_words if '¿' not in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_common_words = (es_common_words & en_common_words & common_ref_words)  - (es_stop_words | stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(es_en_common_words, en_content_words, en_word_utt_count, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"]), 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, en_common_words, en_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = (get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), \n",
    "       get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join(sample_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_content = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_content, \n",
    "            open(os.path.join(m_cfg['data_path'], \n",
    "                              \"eval_en_es_common_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_common_words = [w for w in es_common_words & en_common_words if len(w) >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval crisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises = set()\n",
    "with open(crises_lex_fname, \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        crises.update(line.strip().split())\n",
    "crises = list(crises)\n",
    "crises_stem = [stem(w) for w in crises]\n",
    "\n",
    "# new_crises = []\n",
    "# for t in crises:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_crises.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "# crises.extend(new_crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details = get_details_for_words(crises, (en_common_words & common_ref_words), en_word_utt_count,\n",
    "                                       min_dev_freq=2, \n",
    "                                       max_dev_freq=5000, \n",
    "                                       min_train_freq=2,\n",
    "                                       max_train_freq=5000,\n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(crises_details[\"words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" -- \".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \n",
    "                                                \"eval_en_crisis_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words = set(en_word_utt_count['train'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(en_words)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"howdy\".encode() in vocab_dict[\"en_w\"][\"w2i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = {}\n",
    "\n",
    "for w in tqdm(en_words):\n",
    "    s = []\n",
    "#     print(w)\n",
    "    w_syn = wn.synsets(w)\n",
    "    for item in w_syn:\n",
    "        for lm in item.lemma_names():\n",
    "            if lm in en_words:\n",
    "                s.append(lm)\n",
    "        # end for all lemmas\n",
    "    # end for syns\n",
    "    s.append(w)\n",
    "    syns[w] = list(set(s))\n",
    "# end for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_sim = {\"w\":{}, \"i\":{}}\n",
    "\n",
    "for w in tqdm(vocab_dict[\"en_w\"][\"w2i\"].keys()):\n",
    "    syn_sim[\"w\"][w] = [w.encode() for w in syns.get(w.decode(), [w.decode()])]\n",
    "    i = vocab_dict[\"en_w\"][\"w2i\"][w]\n",
    "    syn_sim[\"i\"][i] = [vocab_dict[\"en_w\"][\"w2i\"][j] for j in syn_sim[\"w\"][w]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(syn_sim[\"w\"].items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(syn_sim, open(os.path.join(m_cfg['data_path'], \"en_syns_train.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = {w:v for w, v in syns.items() if len(v) >= 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(haha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"en_w\"][\"i2w\"][14261]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_keys = set([w for w in sim_dict['w'] if len(sim_dict[\"w\"][w]) >= 2])\n",
    "syn_keys = set([w for w in syn_sim['w'] if len(syn_sim[\"w\"][w]) >= 2])\n",
    "print(len(sim_keys), len(syn_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join([w.decode() for w in syn_sim[\"w\"][b'run']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_keys - sim_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = \"hello\"\n",
    "w_syn = wn.synsets(w)\n",
    "for item in w_syn:\n",
    "    for lm in item.lemma_names():\n",
    "        if lm in en_words:\n",
    "            syns[w].append(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_set = []\n",
    "for synset in wn.synsets(\"hello\"):\n",
    "    for item in synset.lemma_names():\n",
    "        syn_set.append(item)\n",
    "print(syn_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns[1].lemmas()[3].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in syns:\n",
    "    print(s.lemmas()[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.lemmas[0] for s in syns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ES, EN common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_out_str(out_str):\n",
    "    out_str = out_str.replace(\"`\", \"\")\n",
    "    out_str = out_str.replace('\"', '')\n",
    "    out_str = out_str.replace('¿', '')\n",
    "    out_str = out_str.replace(\"''\", \"\")\n",
    "    out_str = out_str.strip()\n",
    "    return out_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_str(h):\n",
    "    out_str = \"\"\n",
    "    for w in [i.decode() for i in h]:\n",
    "        out_str += \"{0:s}\".format(w) if (w.startswith(\"'\") or w==\"n't\") else \" {0:s}\".format(w)\n",
    "\n",
    "    out_str = clean_out_str(out_str)\n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_es_en(es_words, en_words):\n",
    "    utt_es_en_stats = {\"common\":{}, \"es\": {}, \"en\": {}}\n",
    "    \n",
    "    utt_es_en_stats[\"es\"] = Counter(es_words)\n",
    "    utt_es_en_stats[\"en\"] = Counter(en_words)\n",
    "    \n",
    "    common_es_en = set(utt_es_en_stats[\"es\"].keys()) & set(utt_es_en_stats[\"en\"].keys())\n",
    "    \n",
    "    for w in common_es_en:\n",
    "        utt_es_en_stats[\"common\"][w] = min(utt_es_en_stats[\"es\"][w], utt_es_en_stats[\"en\"][w])\n",
    "    \n",
    "    return utt_es_en_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_level_es_en(set_key, ref_num=0):\n",
    "    es_en_stats = {\"common\":{}, \"es\": {}, \"en\": {}}\n",
    "    for u in tqdm(map_dict[set_key], ncols=80):\n",
    "        es_words = get_out_str(map_dict[set_key][u][\"es_w\"]).strip().split()\n",
    "        if set_key == \"fisher_dev\":\n",
    "            en_words = get_out_str(map_dict[set_key][u][\"en_w\"][ref_num]).strip().split()\n",
    "        else:\n",
    "            en_words = get_out_str(map_dict[set_key][u][\"en_w\"]).strip().split()\n",
    "        \n",
    "        utt_es_en_stats = match_es_en(es_words, en_words)\n",
    "        \n",
    "        #print(utt_es_en_stats)\n",
    "        for k in utt_es_en_stats:\n",
    "            for w in utt_es_en_stats[k]:\n",
    "                if w not in es_en_stats[k]:\n",
    "                    es_en_stats[k][w] = 0\n",
    "                es_en_stats[k][w] += utt_es_en_stats[k][w]\n",
    "        # end for update counts\n",
    "    # end for all utts\n",
    "    return es_en_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_details(common_results, show=20, min_len=0, filter_stop=False):\n",
    "    for k in common_results:\n",
    "        print(\"{0:10s} = {1:>10d} types\".format(k,len(common_results[k])))\n",
    "        \n",
    "    tot = {}\n",
    "    print(\"-\"*60)\n",
    "    for k in common_results:\n",
    "        tot[k] = sum(common_results[k].values())\n",
    "        print(\"{0:10s} = {1:>10d} tokens\".format(k,tot[k]))\n",
    "        \n",
    "    print(\"-\"*60)\n",
    "    print(\"common / es = {0:.2f}%\".format(tot[\"common\"]/tot[\"es\"]*100))\n",
    "    print(\"common / en = {0:.2f}%\".format(tot[\"common\"]/tot[\"en\"]*100))\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    common_words = sorted(common_results[\"common\"].items(), reverse=True, key=lambda t: t[1])\n",
    "    \n",
    "    all_stop_words = set(nltk.corpus.stopwords.words(\"english\")) | set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "    \n",
    "    if filter_stop:\n",
    "        common_words = [(w,c) for w,c in common_words \n",
    "                        if w not in all_stop_words and len(w) >= min_len][:show]\n",
    "    else:\n",
    "        common_words = common_words[:show]\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    print(\"Top common words\")\n",
    "    for w, c in common_words:\n",
    "        print(\"{0:20s} || {1:10d}\".format(w,c))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 138819/138819 [00:08<00:00, 16526.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train_common = corpus_level_es_en(\"fisher_train\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common     =       4737 types\n",
      "es         =      32185 types\n",
      "en         =      18140 types\n",
      "------------------------------------------------------------\n",
      "common     =      92445 tokens\n",
      "es         =    1494776 tokens\n",
      "en         =    1440914 tokens\n",
      "------------------------------------------------------------\n",
      "common / es = 6.18%\n",
      "common / en = 6.42%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "Top common words\n",
      "internet             ||        754\n",
      "puerto               ||        470\n",
      "argentina            ||        452\n",
      "chile                ||        421\n",
      "miami                ||        291\n",
      "venezuela            ||        279\n",
      "colombia             ||        260\n",
      "texas                ||        236\n",
      "chicago              ||        233\n",
      "right                ||        230\n"
     ]
    }
   ],
   "source": [
    "show_details(train_common, show=10, min_len=5, filter_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 3979/3979 [00:00<00:00, 14880.47it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_common = corpus_level_es_en(\"fisher_dev\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common     =        596 types\n",
      "es         =       4079 types\n",
      "en         =       2998 types\n",
      "------------------------------------------------------------\n",
      "common     =       3396 tokens\n",
      "es         =      40969 tokens\n",
      "en         =      40041 tokens\n",
      "------------------------------------------------------------\n",
      "common / es = 8.29%\n",
      "common / en = 8.48%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "Top common words\n",
      "puerto               ||         46\n",
      "chicago              ||         20\n",
      "general              ||         14\n",
      "idaho                ||         13\n",
      "colorado             ||         13\n",
      "colombia             ||         12\n",
      "salsa                ||         12\n",
      "philly               ||         12\n",
      "hello                ||         11\n",
      "florida              ||         10\n"
     ]
    }
   ],
   "source": [
    "show_details(dev_common, show=10, min_len=5, filter_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mi nombre es carmen de chicago y tu'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_words = get_out_str(map_dict[\"fisher_dev\"][\"20051009_182032_217_fsp-B-2\"][\"es_w\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
