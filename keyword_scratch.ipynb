{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from basics import *\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"sp2enw_mel-80_vocab-nltk/sp_1.0_h-256_e-128_drpt-rnn-.3_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fisher dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict = get_data_dicts(m_cfg)\n",
    "info_dict = pickle.load(open(\"fbanks_80dim_nltk/info.dict\", \"rb\"))\n",
    "sim_dict = pickle.load(open(\"./fbanks_80dim_nltk/mix_sim.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in map_dict[\"fisher_train\"]:\n",
    "    train_text.append(\" \".join([w.decode() for w in map_dict[\"fisher_train\"][u][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'hello',\n",
       " 'hello',\n",
       " 'hello',\n",
       " 'with whom am i speaking',\n",
       " 'eh silvia yes what is your name',\n",
       " 'hello silvia eh my name is nicole',\n",
       " 'ah nice to meet you',\n",
       " 'nice to meet you em and where are you from',\n",
       " \"eh i 'm in philadelphia\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_to_dump = \"\\n\".join(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../subword-nmt/fisher_train.en\", \"w\") as out_f:\n",
    "    out_f.write(train_text_to_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./learn_joint_bpe_and_vocab.py --input {train_file}.L1 {train_file}.L2 -s {num_operations} -o {codes_file} --write-vocabulary {vocab_file}.L1 {vocab_file}.L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.Load(\"test/test_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_len = 1\n",
    "top_k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_stop_words = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "len(es_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(m_dict, key=\"en_w\"):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u][key]) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u][key]])\n",
    "        else:\n",
    "            for ref in m_dict[u][key]:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train word types   |      17830\n",
      "# train word tokens  |    1497352\n",
      "--------------------------------------------------------------------------------\n",
      "# dev word types     |       4835\n",
      "# dev word tokens    |     165206\n"
     ]
    }
   ],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))\n",
    "\n",
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train word types   |      32860\n",
      "# train word tokens  |    1496792\n",
      "--------------------------------------------------------------------------------\n",
      "# dev word types     |       4145\n",
      "# dev word tokens    |      41098\n"
     ]
    }
   ],
   "source": [
    "# words in train\n",
    "es_train_words = get_words(map_dict['fisher_train'], key=\"es_w\")\n",
    "es_train_words_top_k = [(w,f) for w, f in sorted(es_train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in es_stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "es_train_only_words = set(es_train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(es_train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(es_train_words.values())))\n",
    "\n",
    "es_dev_words = get_words(map_dict['fisher_dev'], key=\"es_w\")\n",
    "es_dev_words_top_k = [(w,f) for w, f in sorted(es_dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in es_stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "es_dev_only_words = set(es_dev_words.keys())\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(es_dev_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(es_dev_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('yes', 35054),\n",
       "  (\"'s\", 24162),\n",
       "  (\"n't\", 19184),\n",
       "  ('like', 14334),\n",
       "  ('well', 12354)],\n",
       " [('ah', 12325), ('eh', 11447), ('si', 9423), ('ajá', 7988), ('bueno', 7838)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_top_k[:5], es_train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 24162), (\"n't\", 19184), (\"'m\", 5546), (\"'re\", 2832), (\"'ve\", 2392)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('yes', 3652), (\"n't\", 1999), (\"'s\", 1866), ('like', 1826), ('know', 1294)],\n",
       " [('ajá', 343), ('ah', 341), ('entonces', 249), ('si', 247), ('mhm', 236)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_words_top_k[:5], es_dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# oov word types     |       1011\n",
      "# oov word tokens    |       1599\n"
     ]
    }
   ],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# oov word types     |        448\n",
      "# oov word tokens    |        525\n"
     ]
    }
   ],
   "source": [
    "es_oov_words = {w:f for w,f in es_dev_words.items() if w not in es_train_only_words}\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(es_oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(es_oov_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0%'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level - get train, dev frequency, and utts in which they occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17830, 12011)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_only_words), len(set([stem(w) for w in train_only_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_level_details(word_key):\n",
    "    word_utt_count = {\"train\": {}, \"dev\": {}, \"train_utts\": {}, \"dev_utts\": {}}\n",
    "    for u in tqdm(map_dict[\"fisher_train\"].keys()):\n",
    "        for w in set(map_dict[\"fisher_train\"][u][word_key]):\n",
    "            curr_word = w.decode()\n",
    "            if curr_word not in word_utt_count[\"train\"]:\n",
    "                word_utt_count[\"train\"][curr_word] = 0\n",
    "                word_utt_count[\"train_utts\"][curr_word] = set()\n",
    "            word_utt_count[\"train\"][curr_word] += 1\n",
    "            word_utt_count[\"train_utts\"][curr_word].update({u})\n",
    "        # end for words in current utt\n",
    "    # end for all utts\n",
    "    for u in tqdm(map_dict[\"fisher_dev\"].keys()):\n",
    "        if word_key == \"en_w\":\n",
    "            for ref in map_dict[\"fisher_dev\"][u][word_key]:\n",
    "                for w in set(ref):\n",
    "                    curr_word = w.decode()\n",
    "                    if curr_word not in word_utt_count[\"dev\"]:\n",
    "                        word_utt_count[\"dev\"][curr_word] = 0\n",
    "                        word_utt_count[\"dev_utts\"][curr_word] = set()\n",
    "                    word_utt_count[\"dev\"][curr_word] += 1            \n",
    "                    word_utt_count[\"dev_utts\"][curr_word].update({u})\n",
    "                # end for words in current ref\n",
    "            # end for all references\n",
    "        # end if multiple references\n",
    "        else:\n",
    "            ref = map_dict[\"fisher_dev\"][u][word_key]\n",
    "            for w in set(ref):\n",
    "                curr_word = w.decode()\n",
    "                if curr_word not in word_utt_count[\"dev\"]:\n",
    "                    word_utt_count[\"dev\"][curr_word] = 0\n",
    "                    word_utt_count[\"dev_utts\"][curr_word] = set()\n",
    "                word_utt_count[\"dev\"][curr_word] += 1            \n",
    "                word_utt_count[\"dev_utts\"][curr_word].update({u})\n",
    "            \n",
    "    # end for all utts\n",
    "    all_train_utts = set()\n",
    "    for w in word_utt_count[\"train_utts\"]:\n",
    "        all_train_utts.update(word_utt_count[\"train_utts\"][w])\n",
    "    # end for\n",
    "\n",
    "    all_dev_utts = set()\n",
    "    for w in word_utt_count[\"dev_utts\"]:\n",
    "        all_dev_utts.update(word_utt_count[\"dev_utts\"][w])\n",
    "    # end for\n",
    "    \n",
    "    return word_utt_count, all_train_utts, all_dev_utts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138819/138819 [00:02<00:00, 64727.23it/s]\n",
      "100%|██████████| 3979/3979 [00:00<00:00, 19567.59it/s]\n"
     ]
    }
   ],
   "source": [
    "en_word_utt_count, en_train_utts, en_dev_utts = get_word_level_details(\"en_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138795, 3979)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_train_utts), len(en_dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138819/138819 [00:02<00:00, 62389.22it/s]\n",
      "100%|██████████| 3979/3979 [00:00<00:00, 62016.27it/s]\n"
     ]
    }
   ],
   "source": [
    "es_word_utt_count, es_train_utts, es_dev_utts = get_word_level_details(\"es_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138797, 3977)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(es_train_utts), len(es_dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word types\n",
      "17830 4835\n",
      "common word types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3824"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"word types\")\n",
    "print(len(en_word_utt_count['train']), len(en_word_utt_count['dev']))\n",
    "print(\"common word types\")\n",
    "en_common_words = set(en_word_utt_count['train'].keys()) & set(en_word_utt_count['dev'].keys())\n",
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word types\n",
      "32860 4145\n",
      "common word types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3697"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"word types\")\n",
    "print(len(es_word_utt_count['train']), len(es_word_utt_count['dev']))\n",
    "print(\"common word types\")\n",
    "es_common_words = set(es_word_utt_count['train'].keys()) & set(es_word_utt_count['dev'].keys())\n",
    "len(es_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details_for_words(words, common_words, word_utt_count, \n",
    "                          min_dev_freq, max_dev_freq, min_train_freq, min_len):\n",
    "    details = {\"words\": {}, \"train_utts\": set(), \"dev_utts\": set()}\n",
    "    \n",
    "    in_vocab_words = set(words) & set(common_words)\n",
    "    print(\"number of in-vocab words = {0:d}\".format(len(in_vocab_words)))\n",
    "\n",
    "    for w in in_vocab_words:\n",
    "        t_count, d_count = len(word_utt_count[\"train_utts\"][w]), len(word_utt_count[\"dev_utts\"][w])\n",
    "        if ((d_count >= min_dev_freq) and \n",
    "            (d_count <= max_dev_freq) and\n",
    "            (len(w) >= min_len) and\n",
    "            (t_count >= min_train_freq)):\n",
    "            details[\"words\"][w] = {\"train\": t_count, \"dev\": d_count}\n",
    "            details[\"train_utts\"].update(word_utt_count[\"train_utts\"][w])\n",
    "            details[\"dev_utts\"].update(word_utt_count[\"dev_utts\"][w])\n",
    "        # end meets criteria\n",
    "    # end for in-vocab word\n",
    "    return details\n",
    "# end function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration(utts, key):\n",
    "    dur = 0\n",
    "    utts_not_found = []\n",
    "    for u in utts:\n",
    "        if u not in info_dict[key]:\n",
    "            #print(\"argh!\", u)\n",
    "            utts_not_found.append(u)\n",
    "        else:\n",
    "            dur += (info_dict[key][u]['sp'] * 10)\n",
    "    dur = dur / 60 / 60 / 1000\n",
    "    print(\"-\"*80)\n",
    "    print(\"{0:d} total utts\".format(len(utts)))\n",
    "    print(\"{0:d} not found\".format(len(utts_not_found)))\n",
    "    print(\"selected utts from {0:s} -- duration = {1:.2f} hours\".format(key, dur))\n",
    "    return dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(words_list):\n",
    "    out = {\"w2i\":{}, \"i2w\":{}, \"freq\":{}, \"freq_dev\":{}}\n",
    "    START_VOCAB = [PAD, GO, EOS, UNK]\n",
    "    for w in START_VOCAB:\n",
    "        out['w2i'][w] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][w] = 1\n",
    "    #for w in words_list['words']:\n",
    "    sorted_w = sorted(words_list['words'].items(), reverse=True, key=lambda t: t[1]['train'])\n",
    "    for w in sorted_w:\n",
    "        encoded_word = w[0].encode()\n",
    "        out[\"w2i\"][encoded_word] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][encoded_word] = w[1][\"train\"]\n",
    "        out[\"freq_dev\"][encoded_word] = w[1][\"dev\"]\n",
    "\n",
    "    out[\"i2w\"] = {val:key for key, val in out[\"w2i\"].items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "138795 total utts\n",
      "89 not found\n",
      "selected utts from fisher_train -- duration = 161.62 hours\n",
      "--------------------------------------------------------------------------------\n",
      "3979 total utts\n",
      "2 not found\n",
      "selected utts from fisher_dev -- duration = 4.35 hours\n"
     ]
    }
   ],
   "source": [
    "train_dur, dev_dur = get_duration(en_train_utts, key=\"fisher_train\"), get_duration(en_dev_utts, key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=10\n",
    "max_dev_freq=10000\n",
    "min_train_freq=50 \n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 3824\n",
      "total words meeting criteria = 557\n"
     ]
    }
   ],
   "source": [
    "terms_of_interest = get_details_for_words(common_words, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "84953 total utts\n",
      "46 not found\n",
      "selected utts from fisher_train -- duration = 133.57 hours\n",
      "--------------------------------------------------------------------------------\n",
      "2759 total utts\n",
      "2 not found\n",
      "selected utts from fisher_dev -- duration = 3.82 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 500\n",
      "total words meeting criteria = 500\n"
     ]
    }
   ],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "82928 total utts\n",
      "43 not found\n",
      "selected utts from fisher_train -- duration = 131.58 hours\n",
      "--------------------------------------------------------------------------------\n",
      "2700 total utts\n",
      "2 not found\n",
      "selected utts from fisher_dev -- duration = 3.78 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neither',\n",
       " 'children',\n",
       " 'dancing',\n",
       " 'knows',\n",
       " 'immigration',\n",
       " 'found',\n",
       " 'belong',\n",
       " 'politics',\n",
       " 'computer',\n",
       " 'miles']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_top_500_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_top_500_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_top_500_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - randomly selected frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=10 \n",
    "max_dev_freq=100\n",
    "min_train_freq=100\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 3824\n",
      "total words meeting criteria = 372\n"
     ]
    }
   ],
   "source": [
    "terms_of_interest = get_details_for_words(common_words, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "62610 total utts\n",
      "35 not found\n",
      "selected utts from fisher_train -- duration = 109.48 hours\n",
      "--------------------------------------------------------------------------------\n",
      "2084 total utts\n",
      "1 not found\n",
      "selected utts from fisher_dev -- duration = 3.30 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 100\n",
      "total words meeting criteria = 100\n"
     ]
    }
   ],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "26354 total utts\n",
      "15 not found\n",
      "selected utts from fisher_train -- duration = 51.20 hours\n",
      "--------------------------------------------------------------------------------\n",
      "1026 total utts\n",
      "0 not found\n",
      "selected utts from fisher_dev -- duration = 1.83 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child',\n",
       " 'rican',\n",
       " 'lived',\n",
       " 'strange',\n",
       " 'thousand',\n",
       " 'alone',\n",
       " 'spend',\n",
       " 'whole',\n",
       " 'doing',\n",
       " 'married']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_top_100_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_top_100_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_top_100_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2 - topics as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_fname= \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\"\n",
    "topics_fname = \"../criseslex/fsp06_topics_in_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [ \"peace\", \"Music\", \"Marriage\", \"Religion\", \"Cell phones\", \n",
    "           \"Dating\", \"Telemarketing and SPAM\", \"Politics\", \"Travel\", \n",
    "           \"Technical devices\", \"Healthcare\", \"Advertisements\", \"Power\", \n",
    "           \"Occupations\", \"Movies\", \"Welfare\", \"Breaking up\", \"Location\", \n",
    "           \"Justice\", \"Memories\", \"Crime\", \"Violence against women\", \"Equality\", \n",
    "            \"Housing\", \"Immigration\",     \n",
    "            # new topics\n",
    "           \"Interracial\", \"Christians\", \"muslims\", \"jews\", \"e-mail\", \n",
    "           \"phone\", \"democracy\", \"Democratic\", \"Republican\", \"technology\", \n",
    "           \"leadership\", \"community\", \"jury\", \"police\", \"inequality\", \n",
    "           \"renting\", \"Violence\", \"immigrants\", \"immigrant\", \"skilled\", \n",
    "           \"Telemarketing\", \"SPAM\", \"skill\", \"job\", \"health\", \"mobile\", \n",
    "            \"ads\", \"physical\", \"emotional\", \"bubble\", \"rent\", \"economy\", \n",
    "            \"abuse\", \"women\", \"city\", \"country\", \"suburban\", \"dollar\", \n",
    "            \"united states\", \"laws\", \"phone\", \"race\", \"biracial\", \"interracial\", \n",
    "            \"marriage\", \"lyrics\", \"sexuality\", \"medicine\", \"television\", \"european\",\n",
    "            \"home\", \"protect\", \"spouse\", \"language\", \"cellphone\", \"money\",\n",
    "            \"doctor\", \"insurance\", \"cigarettes\", \"alcohol\", \"income\", \"salary\",\n",
    "            \"class\", \"censor\", \"rating\", \"programs\", \"government\",\n",
    "            \"relationship\", \"legal\", \"event\", \"life\", \"safe\", \"victim\", \"cops\",\n",
    "            \"wage\", \"illegal\"\n",
    "            ]\n",
    "topics = list(set(t.lower() for t in topics))\n",
    "topics_stem = [stem(t) for t in topics]\n",
    "\n",
    "# add similar topic words\n",
    "new_topics = []\n",
    "# for t in topics:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_topics.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "topics.extend(new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 60\n",
      "total words meeting criteria = 42\n"
     ]
    }
   ],
   "source": [
    "topics_details = get_details_for_words(topics, en_common_words, en_word_utt_count, \n",
    "                                       min_dev_freq=5, \n",
    "                                       max_dev_freq=10000, \n",
    "                                       min_train_freq=10, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(topics_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "13605 total utts\n",
      "8 not found\n",
      "selected utts from fisher_train -- duration = 28.43 hours\n",
      "--------------------------------------------------------------------------------\n",
      "518 total utts\n",
      "0 not found\n",
      "selected utts from fisher_dev -- duration = 0.99 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(topics_details[\"train_utts\"], key=\"fisher_train\"), get_duration(topics_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "life\n",
      "christians\n",
      "jury\n",
      "race\n",
      "alcohol\n",
      "welfare\n",
      "class\n",
      "home\n",
      "safe\n",
      "crime\n",
      "government\n",
      "religion\n",
      "doctor\n",
      "rent\n",
      "income\n",
      "medicine\n",
      "laws\n",
      "politics\n",
      "job\n",
      "justice\n",
      "television\n",
      "spam\n",
      "police\n",
      "movies\n",
      "immigration\n",
      "insurance\n",
      "money\n",
      "health\n",
      "language\n",
      "women\n",
      "illegal\n",
      "phone\n",
      "programs\n",
      "interracial\n",
      "travel\n",
      "music\n",
      "country\n",
      "lyrics\n",
      "marriage\n",
      "relationship\n",
      "city\n",
      "protect\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(list(topics_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alcohol': {'dev': 9, 'train': 38},\n",
       " 'christians': {'dev': 12, 'train': 86},\n",
       " 'city': {'dev': 144, 'train': 1045},\n",
       " 'class': {'dev': 31, 'train': 119},\n",
       " 'country': {'dev': 93, 'train': 1341},\n",
       " 'crime': {'dev': 35, 'train': 169},\n",
       " 'doctor': {'dev': 8, 'train': 195},\n",
       " 'government': {'dev': 33, 'train': 333},\n",
       " 'health': {'dev': 8, 'train': 243},\n",
       " 'home': {'dev': 73, 'train': 537},\n",
       " 'illegal': {'dev': 5, 'train': 85},\n",
       " 'immigration': {'dev': 12, 'train': 137},\n",
       " 'income': {'dev': 7, 'train': 32},\n",
       " 'insurance': {'dev': 7, 'train': 310},\n",
       " 'interracial': {'dev': 8, 'train': 66},\n",
       " 'job': {'dev': 33, 'train': 476},\n",
       " 'jury': {'dev': 25, 'train': 185},\n",
       " 'justice': {'dev': 8, 'train': 85},\n",
       " 'language': {'dev': 10, 'train': 230},\n",
       " 'laws': {'dev': 6, 'train': 170},\n",
       " 'life': {'dev': 93, 'train': 933},\n",
       " 'lyrics': {'dev': 8, 'train': 43},\n",
       " 'marriage': {'dev': 24, 'train': 281},\n",
       " 'medicine': {'dev': 15, 'train': 97},\n",
       " 'money': {'dev': 132, 'train': 1217},\n",
       " 'movies': {'dev': 10, 'train': 329},\n",
       " 'music': {'dev': 426, 'train': 1719},\n",
       " 'phone': {'dev': 42, 'train': 838},\n",
       " 'police': {'dev': 11, 'train': 213},\n",
       " 'politics': {'dev': 21, 'train': 180},\n",
       " 'programs': {'dev': 26, 'train': 133},\n",
       " 'protect': {'dev': 7, 'train': 39},\n",
       " 'race': {'dev': 22, 'train': 187},\n",
       " 'relationship': {'dev': 13, 'train': 380},\n",
       " 'religion': {'dev': 163, 'train': 885},\n",
       " 'rent': {'dev': 14, 'train': 213},\n",
       " 'safe': {'dev': 15, 'train': 95},\n",
       " 'spam': {'dev': 51, 'train': 80},\n",
       " 'television': {'dev': 35, 'train': 266},\n",
       " 'travel': {'dev': 71, 'train': 234},\n",
       " 'welfare': {'dev': 15, 'train': 34},\n",
       " 'women': {'dev': 28, 'train': 541}}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_details['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_topics_vocab = create_vocab(topics_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = pickle.load(open(os.path.join(m_cfg['data_path'], \"bow_topics_vocab.dict\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(haha['w2i']) & set(bow_topics_vocab['w2i'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_topics_vocab, open(os.path.join(m_cfg['data_path'], \"bow_topics_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - crises terms as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises = set()\n",
    "with open(crises_lex_fname, \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        crises.update(line.strip().split())\n",
    "crises = list(crises)\n",
    "crises_stem = [stem(w) for w in crises]\n",
    "\n",
    "# new_crises = []\n",
    "# for t in crises:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_crises.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "# crises.extend(new_crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 123\n",
      "total words meeting criteria = 40\n"
     ]
    }
   ],
   "source": [
    "crises_details = get_details_for_words(crises, en_common_words, en_word_utt_count,\n",
    "                                       min_dev_freq=10, \n",
    "                                       max_dev_freq=1000, \n",
    "                                       min_train_freq=100, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "28608 total utts\n",
      "11 not found\n",
      "selected utts from fisher_train -- duration = 55.51 hours\n",
      "--------------------------------------------------------------------------------\n",
      "1076 total utts\n",
      "0 not found\n",
      "selected utts from fisher_dev -- duration = 1.96 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'dev', 'train_utts', 'dev_utts'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "want\n",
      "life\n",
      "another\n",
      "people\n",
      "first\n",
      "stay\n",
      "need\n",
      "case\n",
      "city\n",
      "found\n",
      "watch\n",
      "remember\n",
      "town\n",
      "girl\n",
      "even\n",
      "coming\n",
      "gets\n",
      "high\n",
      "home\n",
      "women\n",
      "huge\n",
      "saying\n",
      "news\n",
      "make\n",
      "lives\n",
      "morning\n",
      "house\n",
      "change\n",
      "name\n",
      "give\n",
      "terrible\n",
      "love\n",
      "years\n",
      "someone\n",
      "waiting\n",
      "send\n",
      "time\n",
      "help\n",
      "leave\n",
      "live\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'another': {'dev': 45, 'train': 1215},\n",
       " 'case': {'dev': 14, 'train': 432},\n",
       " 'change': {'dev': 19, 'train': 425},\n",
       " 'city': {'dev': 39, 'train': 1045},\n",
       " 'coming': {'dev': 20, 'train': 253},\n",
       " 'even': {'dev': 73, 'train': 1582},\n",
       " 'first': {'dev': 20, 'train': 937},\n",
       " 'found': {'dev': 15, 'train': 290},\n",
       " 'gets': {'dev': 20, 'train': 370},\n",
       " 'girl': {'dev': 34, 'train': 598},\n",
       " 'give': {'dev': 37, 'train': 999},\n",
       " 'help': {'dev': 38, 'train': 529},\n",
       " 'high': {'dev': 20, 'train': 255},\n",
       " 'home': {'dev': 34, 'train': 537},\n",
       " 'house': {'dev': 42, 'train': 1134},\n",
       " 'huge': {'dev': 10, 'train': 119},\n",
       " 'leave': {'dev': 25, 'train': 404},\n",
       " 'life': {'dev': 25, 'train': 933},\n",
       " 'live': {'dev': 84, 'train': 1979},\n",
       " 'lives': {'dev': 24, 'train': 362},\n",
       " 'love': {'dev': 25, 'train': 712},\n",
       " 'make': {'dev': 62, 'train': 968},\n",
       " 'morning': {'dev': 11, 'train': 184},\n",
       " 'name': {'dev': 32, 'train': 1064},\n",
       " 'need': {'dev': 49, 'train': 770},\n",
       " 'news': {'dev': 11, 'train': 187},\n",
       " 'people': {'dev': 204, 'train': 5781},\n",
       " 'remember': {'dev': 21, 'train': 835},\n",
       " 'saying': {'dev': 23, 'train': 505},\n",
       " 'send': {'dev': 29, 'train': 368},\n",
       " 'someone': {'dev': 55, 'train': 1009},\n",
       " 'stay': {'dev': 19, 'train': 395},\n",
       " 'terrible': {'dev': 10, 'train': 263},\n",
       " 'time': {'dev': 142, 'train': 2665},\n",
       " 'town': {'dev': 10, 'train': 148},\n",
       " 'waiting': {'dev': 13, 'train': 115},\n",
       " 'want': {'dev': 97, 'train': 2145},\n",
       " 'watch': {'dev': 16, 'train': 361},\n",
       " 'women': {'dev': 10, 'train': 541},\n",
       " 'years': {'dev': 68, 'train': 2383}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \"bow_crises_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Spanish - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=18\n",
    "max_dev_freq=10000\n",
    "min_train_freq=200\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 3697\n",
      "total words meeting criteria = 100\n"
     ]
    }
   ],
   "source": [
    "terms_of_interest = get_details_for_words(es_common_words,\n",
    "                                          es_common_words,\n",
    "                                          es_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "74435 total utts\n",
      "37 not found\n",
      "selected utts from fisher_train -- duration = 122.08 hours\n",
      "--------------------------------------------------------------------------------\n",
      "2076 total utts\n",
      "0 not found\n",
      "selected utts from fisher_dev -- duration = 3.25 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"].keys()), len(terms_of_interest[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_terms = [\"bueno\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of in-vocab words = 100\n",
      "total words meeting criteria = 100\n"
     ]
    }
   ],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms,\n",
    "                                             es_common_words,\n",
    "                                             es_word_utt_count,\n",
    "                                              min_dev_freq=min_dev_freq, \n",
    "                                              max_dev_freq=max_dev_freq, \n",
    "                                              min_train_freq=min_train_freq, \n",
    "                                              min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "74435 total utts\n",
      "37 not found\n",
      "selected utts from fisher_train -- duration = 122.08 hours\n",
      "--------------------------------------------------------------------------------\n",
      "2076 total utts\n",
      "0 not found\n",
      "selected utts from fisher_dev -- duration = 3.25 hours\n"
     ]
    }
   ],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tanto',\n",
       " 'imagino',\n",
       " 'quiero',\n",
       " 'mejor',\n",
       " 'estados',\n",
       " 'gente',\n",
       " 'bastante',\n",
       " 'alguien',\n",
       " 'nosotros',\n",
       " 'tiempo',\n",
       " 'puedes',\n",
       " 'solamente',\n",
       " 'cinco',\n",
       " 'bonito',\n",
       " 'mucha',\n",
       " 'cuando',\n",
       " 'dicen',\n",
       " 'claro',\n",
       " 'muchas',\n",
       " 'mismo',\n",
       " 'cierto',\n",
       " 'ayuda',\n",
       " 'persona',\n",
       " 'tener',\n",
       " 'estar',\n",
       " 'español',\n",
       " 'quiere',\n",
       " 'estaba',\n",
       " 'religión',\n",
       " 'había',\n",
       " 'llama',\n",
       " 'estoy',\n",
       " 'cuatro',\n",
       " 'verdad',\n",
       " 'todos',\n",
       " 'gusta',\n",
       " 'ellos',\n",
       " 'estás',\n",
       " 'dónde',\n",
       " 'hacen',\n",
       " 'nueva',\n",
       " 'gustaría',\n",
       " 'tienes',\n",
       " 'hasta',\n",
       " 'puerto',\n",
       " 'chicago',\n",
       " 'ejemplo',\n",
       " 'familia',\n",
       " 'música',\n",
       " 'menos',\n",
       " 'nunca',\n",
       " 'sabes',\n",
       " 'ahora',\n",
       " 'decir',\n",
       " 'tampoco',\n",
       " 'mundo',\n",
       " 'todavía',\n",
       " '¿verdad',\n",
       " 'niños',\n",
       " 'buena',\n",
       " 'siempre',\n",
       " 'dinero',\n",
       " 'parece',\n",
       " 'están',\n",
       " 'personas',\n",
       " 'universidad',\n",
       " 'trabajo',\n",
       " 'tiene',\n",
       " 'antes',\n",
       " 'muchos',\n",
       " 'porque',\n",
       " 'entonces',\n",
       " 'bueno',\n",
       " 'después',\n",
       " 'esposo',\n",
       " 'españa',\n",
       " 'tengo',\n",
       " 'hablando',\n",
       " 'ciudad',\n",
       " 'mucho',\n",
       " 'unidos',\n",
       " 'estado',\n",
       " 'también',\n",
       " 'usted',\n",
       " 'difícil',\n",
       " 'tenía',\n",
       " 'poquito',\n",
       " 'tienen',\n",
       " 'buenas',\n",
       " 'puede',\n",
       " 'hacer',\n",
       " 'iglesia',\n",
       " 'tenemos',\n",
       " 'noches',\n",
       " 'méxico',\n",
       " '¿cómo',\n",
       " 'veces',\n",
       " 'cosas',\n",
       " 'todas',\n",
       " 'donde']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_terms_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['w2i', 'i2w', 'freq', 'freq_dev'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_es_top_words_vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['words', 'train_utts', 'dev_utts'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_terms_details.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3bc3f25da4a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_terms_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_utts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "sample_terms_details[\"train_utts\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ahora': {'dev': 65, 'train': 2955},\n",
       " 'alguien': {'dev': 23, 'train': 811},\n",
       " 'antes': {'dev': 28, 'train': 962},\n",
       " 'ayuda': {'dev': 19, 'train': 386},\n",
       " 'bastante': {'dev': 26, 'train': 955},\n",
       " 'bonito': {'dev': 19, 'train': 388},\n",
       " 'buena': {'dev': 23, 'train': 562},\n",
       " 'buenas': {'dev': 25, 'train': 627},\n",
       " 'bueno': {'dev': 158, 'train': 7215},\n",
       " 'chicago': {'dev': 20, 'train': 223},\n",
       " 'cierto': {'dev': 19, 'train': 868},\n",
       " 'cinco': {'dev': 20, 'train': 923},\n",
       " 'ciudad': {'dev': 32, 'train': 1028},\n",
       " 'claro': {'dev': 116, 'train': 5207},\n",
       " 'cosas': {'dev': 81, 'train': 2881},\n",
       " 'cuando': {'dev': 159, 'train': 4045},\n",
       " 'cuatro': {'dev': 18, 'train': 610},\n",
       " 'decir': {'dev': 23, 'train': 1239},\n",
       " 'después': {'dev': 35, 'train': 1715},\n",
       " 'dicen': {'dev': 21, 'train': 1016},\n",
       " 'difícil': {'dev': 20, 'train': 949},\n",
       " 'dinero': {'dev': 30, 'train': 990},\n",
       " 'donde': {'dev': 50, 'train': 1527},\n",
       " 'dónde': {'dev': 31, 'train': 1278},\n",
       " 'ejemplo': {'dev': 23, 'train': 1972},\n",
       " 'ellos': {'dev': 67, 'train': 2858},\n",
       " 'entonces': {'dev': 222, 'train': 6340},\n",
       " 'españa': {'dev': 18, 'train': 397},\n",
       " 'español': {'dev': 28, 'train': 844},\n",
       " 'esposo': {'dev': 32, 'train': 589},\n",
       " 'estaba': {'dev': 47, 'train': 1890},\n",
       " 'estado': {'dev': 21, 'train': 699},\n",
       " 'estados': {'dev': 28, 'train': 1661},\n",
       " 'estar': {'dev': 26, 'train': 1114},\n",
       " 'estoy': {'dev': 49, 'train': 2957},\n",
       " 'están': {'dev': 39, 'train': 2339},\n",
       " 'estás': {'dev': 33, 'train': 1497},\n",
       " 'familia': {'dev': 38, 'train': 891},\n",
       " 'gente': {'dev': 126, 'train': 4106},\n",
       " 'gusta': {'dev': 54, 'train': 1818},\n",
       " 'gustaría': {'dev': 22, 'train': 323},\n",
       " 'hablando': {'dev': 18, 'train': 794},\n",
       " 'había': {'dev': 20, 'train': 955},\n",
       " 'hacen': {'dev': 18, 'train': 813},\n",
       " 'hacer': {'dev': 51, 'train': 1847},\n",
       " 'hasta': {'dev': 38, 'train': 1252},\n",
       " 'iglesia': {'dev': 30, 'train': 405},\n",
       " 'imagino': {'dev': 18, 'train': 391},\n",
       " 'llama': {'dev': 29, 'train': 882},\n",
       " 'mejor': {'dev': 35, 'train': 1234},\n",
       " 'menos': {'dev': 35, 'train': 1535},\n",
       " 'mismo': {'dev': 26, 'train': 1429},\n",
       " 'mucha': {'dev': 37, 'train': 1347},\n",
       " 'muchas': {'dev': 39, 'train': 1360},\n",
       " 'mucho': {'dev': 123, 'train': 4017},\n",
       " 'muchos': {'dev': 30, 'train': 1379},\n",
       " 'mundo': {'dev': 30, 'train': 1055},\n",
       " 'méxico': {'dev': 30, 'train': 706},\n",
       " 'música': {'dev': 105, 'train': 1662},\n",
       " 'niños': {'dev': 25, 'train': 905},\n",
       " 'noches': {'dev': 22, 'train': 304},\n",
       " 'nosotros': {'dev': 50, 'train': 1200},\n",
       " 'nueva': {'dev': 25, 'train': 1124},\n",
       " 'nunca': {'dev': 47, 'train': 1299},\n",
       " 'parece': {'dev': 23, 'train': 1205},\n",
       " 'persona': {'dev': 23, 'train': 1726},\n",
       " 'personas': {'dev': 33, 'train': 1487},\n",
       " 'poquito': {'dev': 19, 'train': 648},\n",
       " 'porque': {'dev': 295, 'train': 10039},\n",
       " 'puede': {'dev': 47, 'train': 1688},\n",
       " 'puedes': {'dev': 23, 'train': 647},\n",
       " 'puerto': {'dev': 45, 'train': 454},\n",
       " 'quiere': {'dev': 21, 'train': 694},\n",
       " 'quiero': {'dev': 28, 'train': 584},\n",
       " 'religión': {'dev': 43, 'train': 891},\n",
       " 'sabes': {'dev': 93, 'train': 1604},\n",
       " 'siempre': {'dev': 75, 'train': 1993},\n",
       " 'solamente': {'dev': 26, 'train': 746},\n",
       " 'también': {'dev': 168, 'train': 5385},\n",
       " 'tampoco': {'dev': 20, 'train': 662},\n",
       " 'tanto': {'dev': 34, 'train': 948},\n",
       " 'tenemos': {'dev': 28, 'train': 796},\n",
       " 'tener': {'dev': 34, 'train': 1376},\n",
       " 'tengo': {'dev': 79, 'train': 3078},\n",
       " 'tenía': {'dev': 30, 'train': 998},\n",
       " 'tiempo': {'dev': 54, 'train': 1707},\n",
       " 'tiene': {'dev': 110, 'train': 3884},\n",
       " 'tienen': {'dev': 66, 'train': 2154},\n",
       " 'tienes': {'dev': 41, 'train': 1368},\n",
       " 'todas': {'dev': 23, 'train': 889},\n",
       " 'todavía': {'dev': 22, 'train': 856},\n",
       " 'todos': {'dev': 41, 'train': 1676},\n",
       " 'trabajo': {'dev': 33, 'train': 1342},\n",
       " 'unidos': {'dev': 26, 'train': 1577},\n",
       " 'universidad': {'dev': 27, 'train': 574},\n",
       " 'usted': {'dev': 31, 'train': 1536},\n",
       " 'veces': {'dev': 67, 'train': 2221},\n",
       " 'verdad': {'dev': 46, 'train': 2677},\n",
       " '¿cómo': {'dev': 48, 'train': 1732},\n",
       " '¿verdad': {'dev': 43, 'train': 678}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_terms_details['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev': 158, 'train': 7215}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_terms_details['words']['bueno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7215, 158)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_es_top_words_vocab['freq'][b'bueno'], bow_es_top_words_vocab['freq_dev'][b'bueno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_100word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_1word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password: "
     ]
    }
   ],
   "source": [
    "!su s1444673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'fbanks_80dim_nltk/bow_es_100word_vocab.dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-3923d20ac1a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_es_top_words_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bow_es_100word_vocab.dict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'fbanks_80dim_nltk/bow_es_100word_vocab.dict'"
     ]
    }
   ],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_100word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_top_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev': 11, 'train': 93}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_terms_details['words']['colorado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('porque', 10039),\n",
       " ('bueno', 7215),\n",
       " ('entonces', 6340),\n",
       " ('también', 5385),\n",
       " ('claro', 5207),\n",
       " ('gente', 4106),\n",
       " ('cuando', 4045),\n",
       " ('mucho', 4017),\n",
       " ('tiene', 3884),\n",
       " ('tengo', 3078),\n",
       " ('estoy', 2957),\n",
       " ('ahora', 2955),\n",
       " ('cosas', 2881),\n",
       " ('ellos', 2858),\n",
       " ('verdad', 2677),\n",
       " ('están', 2339),\n",
       " ('veces', 2221),\n",
       " ('tienen', 2154),\n",
       " ('siempre', 1993),\n",
       " ('ejemplo', 1972)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(w, sample_terms_details['words'][w]['train']) \n",
    "       for w in sample_terms_details['words']], reverse=True, key=lambda t: t[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('porque', 295),\n",
       " ('entonces', 222),\n",
       " ('también', 168),\n",
       " ('cuando', 159),\n",
       " ('bueno', 158),\n",
       " ('gente', 126),\n",
       " ('mucho', 123),\n",
       " ('claro', 116),\n",
       " ('tiene', 110),\n",
       " ('música', 105),\n",
       " ('sabes', 93),\n",
       " ('cosas', 81),\n",
       " ('tengo', 79),\n",
       " ('siempre', 75),\n",
       " ('ellos', 67),\n",
       " ('veces', 67),\n",
       " ('tienen', 66),\n",
       " ('ahora', 65),\n",
       " ('tiempo', 54),\n",
       " ('gusta', 54)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(w, sample_terms_details['words'][w]['dev']) \n",
    "       for w in sample_terms_details['words']], reverse=True, key=lambda t: t[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg['data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls fbanks_80dim_nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
