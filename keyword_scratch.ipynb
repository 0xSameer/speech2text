{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from basics import *\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"interspeech_new_vocab/sp_160hrs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fisher dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict = get_data_dicts(m_cfg)\n",
    "info_dict = pickle.load(open(\"fbanks_80dim_nltk/info.dict\", \"rb\"))\n",
    "sim_dict = pickle.load(open(\"./fbanks_80dim_nltk/mix_sim.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in map_dict[\"fisher_train\"]:\n",
    "    train_text.append(\" \".join([w.decode() for w in map_dict[\"fisher_train\"][u][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_to_dump = \"\\n\".join(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../subword-nmt/fisher_train.en\", \"w\") as out_f:\n",
    "    out_f.write(train_text_to_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./learn_joint_bpe_and_vocab.py --input {train_file}.L1 {train_file}.L2 -s {num_operations} -o {codes_file} --write-vocabulary {vocab_file}.L1 {vocab_file}.L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.Load(\"test/test_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_len = 1\n",
    "top_k = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_stop_words = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "len(es_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(m_dict, key=\"en_w\"):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u][key]) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u][key]])\n",
    "        else:\n",
    "            for ref in m_dict[u][key]:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3979"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_dict['fisher_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train word types   |      17688\n",
      "# train word tokens  |    1496533\n",
      "--------------------------------------------------------------------------------\n",
      "# dev word types     |       4798\n",
      "# dev word tokens    |     165047\n"
     ]
    }
   ],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))\n",
    "\n",
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train word types   |      32168\n",
      "# train word tokens  |    1494878\n",
      "--------------------------------------------------------------------------------\n",
      "# dev word types     |       4073\n",
      "# dev word tokens    |      41014\n"
     ]
    }
   ],
   "source": [
    "# words in train\n",
    "es_train_words = get_words(map_dict['fisher_train'], key=\"es_w\")\n",
    "es_train_words_top_k = [(w,f) for w, f in sorted(es_train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in es_stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "es_train_only_words = set(es_train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(es_train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(es_train_words.values())))\n",
    "\n",
    "es_dev_words = get_words(map_dict['fisher_dev'], key=\"es_w\")\n",
    "es_dev_words_top_k = [(w,f) for w, f in sorted(es_dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in es_stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "es_dev_only_words = set(es_dev_words.keys())\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(es_dev_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(es_dev_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('yes', 35085),\n",
       "  (\"'s\", 24162),\n",
       "  (\"n't\", 19184),\n",
       "  ('like', 14337),\n",
       "  ('well', 12354)],\n",
       " [('ah', 12598), ('eh', 11578), ('si', 9541), ('ajá', 7992), ('bueno', 7874)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words_top_k[:5], es_train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 24162), (\"n't\", 19184), (\"'m\", 5546), (\"'re\", 2832), (\"'ve\", 2392)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('yes', 3652), (\"n't\", 1999), (\"'s\", 1866), ('like', 1827), ('know', 1294)],\n",
       " [('ah', 349), ('ajá', 343), ('si', 251), ('entonces', 249), ('mhm', 236)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_words_top_k[:5], es_dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# oov word types     |        999\n",
      "# oov word tokens    |       1562\n"
     ]
    }
   ],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# oov word types     |        440\n",
      "# oov word tokens    |        517\n"
     ]
    }
   ],
   "source": [
    "es_oov_words = {w:f for w,f in es_dev_words.items() if w not in es_train_only_words}\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(es_oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(es_oov_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9%'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word level - get train, dev frequency, and utts in which they occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17688, 11869)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_only_words), len(set([stem(w) for w in train_only_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_level_details(word_key):\n",
    "    word_utt_count = {\"train\": {}, \"dev\": {}, \"train_utts\": {}, \"dev_utts\": {}}\n",
    "    for u in tqdm(map_dict[\"fisher_train\"].keys()):\n",
    "        for w in set(map_dict[\"fisher_train\"][u][word_key]):\n",
    "            curr_word = w.decode()\n",
    "            if curr_word not in word_utt_count[\"train\"]:\n",
    "                word_utt_count[\"train\"][curr_word] = 0\n",
    "                word_utt_count[\"train_utts\"][curr_word] = set()\n",
    "            word_utt_count[\"train\"][curr_word] += 1\n",
    "            word_utt_count[\"train_utts\"][curr_word].update({u})\n",
    "        # end for words in current utt\n",
    "    # end for all utts\n",
    "    for u in tqdm(map_dict[\"fisher_dev\"].keys()):\n",
    "        if word_key == \"en_w\":\n",
    "            for ref in map_dict[\"fisher_dev\"][u][word_key]:\n",
    "                for w in set(ref):\n",
    "                    curr_word = w.decode()\n",
    "                    if curr_word not in word_utt_count[\"dev\"]:\n",
    "                        word_utt_count[\"dev\"][curr_word] = 0\n",
    "                        word_utt_count[\"dev_utts\"][curr_word] = set()\n",
    "                    word_utt_count[\"dev\"][curr_word] += 1            \n",
    "                    word_utt_count[\"dev_utts\"][curr_word].update({u})\n",
    "                # end for words in current ref\n",
    "            # end for all references\n",
    "        # end if multiple references\n",
    "        else:\n",
    "            ref = map_dict[\"fisher_dev\"][u][word_key]\n",
    "            for w in set(ref):\n",
    "                curr_word = w.decode()\n",
    "                if curr_word not in word_utt_count[\"dev\"]:\n",
    "                    word_utt_count[\"dev\"][curr_word] = 0\n",
    "                    word_utt_count[\"dev_utts\"][curr_word] = set()\n",
    "                word_utt_count[\"dev\"][curr_word] += 1            \n",
    "                word_utt_count[\"dev_utts\"][curr_word].update({u})\n",
    "            \n",
    "    # end for all utts\n",
    "    all_train_utts = set()\n",
    "    for w in word_utt_count[\"train_utts\"]:\n",
    "        all_train_utts.update(word_utt_count[\"train_utts\"][w])\n",
    "    # end for\n",
    "\n",
    "    all_dev_utts = set()\n",
    "    for w in word_utt_count[\"dev_utts\"]:\n",
    "        all_dev_utts.update(word_utt_count[\"dev_utts\"][w])\n",
    "    # end for\n",
    "    \n",
    "    return word_utt_count, all_train_utts, all_dev_utts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138819/138819 [00:01<00:00, 90613.20it/s]\n",
      "100%|██████████| 3979/3979 [00:00<00:00, 26484.60it/s]\n"
     ]
    }
   ],
   "source": [
    "en_word_utt_count, en_train_utts, en_dev_utts = get_word_level_details(\"en_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138794, 3979)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_train_utts), len(en_dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_word_utt_count, es_train_utts, es_dev_utts = get_word_level_details(\"es_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_train_utts), len(es_dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word types\n",
      "17688 4798\n",
      "common word types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3799"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"word types\")\n",
    "print(len(en_word_utt_count['train']), len(en_word_utt_count['dev']))\n",
    "print(\"common word types\")\n",
    "en_common_words = set(en_word_utt_count['train'].keys()) & set(en_word_utt_count['dev'].keys())\n",
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word types\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'es_word_utt_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-518ebce0c80b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word types\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_word_utt_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_word_utt_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"common word types\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mes_common_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_word_utt_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_word_utt_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mes_common_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'es_word_utt_count' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"word types\")\n",
    "print(len(es_word_utt_count['train']), len(es_word_utt_count['dev']))\n",
    "print(\"common word types\")\n",
    "es_common_words = set(es_word_utt_count['train'].keys()) & set(es_word_utt_count['dev'].keys())\n",
    "len(es_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details_for_words(words, common_words, word_utt_count, \n",
    "                          min_dev_freq, max_dev_freq, min_train_freq, max_train_freq, min_len):\n",
    "    details = {\"words\": {}, \"train_utts\": set(), \"dev_utts\": set()}\n",
    "    \n",
    "    in_vocab_words = set(words) & set(common_words)\n",
    "    print(\"number of in-vocab words = {0:d}\".format(len(in_vocab_words)))\n",
    "\n",
    "    for w in in_vocab_words:\n",
    "        t_count, d_count = len(word_utt_count[\"train_utts\"][w]), len(word_utt_count[\"dev_utts\"][w])\n",
    "        if ((d_count >= min_dev_freq) and \n",
    "            (d_count <= max_dev_freq) and\n",
    "            (len(w) >= min_len) and\n",
    "            (t_count >= min_train_freq) and \n",
    "            (t_count <= max_train_freq)):\n",
    "            details[\"words\"][w] = {\"train\": t_count, \"dev\": d_count}\n",
    "            details[\"train_utts\"].update(word_utt_count[\"train_utts\"][w])\n",
    "            details[\"dev_utts\"].update(word_utt_count[\"dev_utts\"][w])\n",
    "        # end meets criteria\n",
    "    # end for in-vocab word\n",
    "    return details\n",
    "# end function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration(utts, key):\n",
    "    dur = 0\n",
    "    utts_not_found = []\n",
    "    for u in utts:\n",
    "        if u not in info_dict[key]:\n",
    "            #print(\"argh!\", u)\n",
    "            utts_not_found.append(u)\n",
    "        else:\n",
    "            dur += (info_dict[key][u]['sp'] * 10)\n",
    "    dur = dur / 60 / 60 / 1000\n",
    "    print(\"-\"*80)\n",
    "    print(\"{0:d} total utts\".format(len(utts)))\n",
    "    print(\"{0:d} not found\".format(len(utts_not_found)))\n",
    "    print(\"selected utts from {0:s} -- duration = {1:.2f} hours\".format(key, dur))\n",
    "    return dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(words_list):\n",
    "    out = {\"w2i\":{}, \"i2w\":{}, \"freq\":{}, \"freq_dev\":{}}\n",
    "    START_VOCAB = [PAD, GO, EOS, UNK]\n",
    "    for w in START_VOCAB:\n",
    "        out['w2i'][w] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][w] = 1\n",
    "    #for w in words_list['words']:\n",
    "    sorted_w = sorted(words_list['words'].items(), reverse=True, key=lambda t: t[1]['train'])\n",
    "    for w in sorted_w:\n",
    "        encoded_word = w[0].encode()\n",
    "        out[\"w2i\"][encoded_word] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][encoded_word] = w[1][\"train\"]\n",
    "        out[\"freq_dev\"][encoded_word] = w[1][\"dev\"]\n",
    "\n",
    "    out[\"i2w\"] = {val:key for key, val in out[\"w2i\"].items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "138794 total utts\n",
      "89 not found\n",
      "selected utts from fisher_train -- duration = 161.62 hours\n",
      "--------------------------------------------------------------------------------\n",
      "3979 total utts\n",
      "2 not found\n",
      "selected utts from fisher_dev -- duration = 4.35 hours\n"
     ]
    }
   ],
   "source": [
    "train_dur, dev_dur = get_duration(en_train_utts, key=\"fisher_train\"), get_duration(en_dev_utts, key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0 - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=10\n",
    "max_dev_freq=10000\n",
    "min_train_freq=50 \n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(common_words, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_top_500_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_top_500_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_top_500_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - randomly selected frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=10 \n",
    "max_dev_freq=100\n",
    "min_train_freq=100\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(common_words, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_top_100_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_top_100_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_top_100_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 2 - topics as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_fname= \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\"\n",
    "topics_fname = \"../criseslex/fsp06_topics_in_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [ \"peace\", \"Music\", \"Marriage\", \"Religion\", \"Cell phones\", \n",
    "           \"Dating\", \"Telemarketing and SPAM\", \"Politics\", \"Travel\", \n",
    "           \"Technical devices\", \"Healthcare\", \"Advertisements\", \"Power\", \n",
    "           \"Occupations\", \"Movies\", \"Welfare\", \"Breaking up\", \"Location\", \n",
    "           \"Justice\", \"Memories\", \"Crime\", \"Violence against women\", \"Equality\", \n",
    "            \"Housing\", \"Immigration\",     \n",
    "            # new topics\n",
    "           \"Interracial\", \"Christians\", \"muslims\", \"jews\", \"e-mail\", \n",
    "           \"phone\", \"democracy\", \"Democratic\", \"Republican\", \"technology\", \n",
    "           \"leadership\", \"community\", \"jury\", \"police\", \"inequality\", \n",
    "           \"renting\", \"Violence\", \"immigrants\", \"immigrant\", \"skilled\", \n",
    "           \"Telemarketing\", \"SPAM\", \"skill\", \"job\", \"health\", \"mobile\", \n",
    "            \"ads\", \"physical\", \"emotional\", \"bubble\", \"rent\", \"economy\", \n",
    "            \"abuse\", \"women\", \"city\", \"country\", \"suburban\", \"dollar\", \n",
    "            \"united states\", \"laws\", \"phone\", \"race\", \"biracial\", \"interracial\", \n",
    "            \"marriage\", \"lyrics\", \"sexuality\", \"medicine\", \"television\", \"european\",\n",
    "            \"home\", \"protect\", \"spouse\", \"language\", \"cellphone\", \"money\",\n",
    "            \"doctor\", \"insurance\", \"cigarettes\", \"alcohol\", \"income\", \"salary\",\n",
    "            \"class\", \"censor\", \"rating\", \"programs\", \"government\",\n",
    "            \"relationship\", \"legal\", \"event\", \"life\", \"safe\", \"victim\", \"cops\",\n",
    "            \"wage\", \"illegal\"\n",
    "            ]\n",
    "topics = list(set(t.lower() for t in topics))\n",
    "topics_stem = [stem(t) for t in topics]\n",
    "\n",
    "# add similar topic words\n",
    "new_topics = []\n",
    "# for t in topics:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_topics.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "topics.extend(new_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_details = get_details_for_words(topics, en_common_words, en_word_utt_count, \n",
    "                                       min_dev_freq=5, \n",
    "                                       max_dev_freq=10000, \n",
    "                                       min_train_freq=10, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(topics_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(topics_details[\"train_utts\"], key=\"fisher_train\"), get_duration(topics_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(list(topics_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_details['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_topics_vocab = create_vocab(topics_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = pickle.load(open(os.path.join(m_cfg['data_path'], \"bow_topics_vocab.dict\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(haha['w2i']) & set(bow_topics_vocab['w2i'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_topics_vocab, open(os.path.join(m_cfg['data_path'], \"bow_topics_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - crises terms as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises = set()\n",
    "with open(crises_lex_fname, \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        crises.update(line.strip().split())\n",
    "crises = list(crises)\n",
    "crises_stem = [stem(w) for w in crises]\n",
    "\n",
    "# new_crises = []\n",
    "# for t in crises:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_crises.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "# crises.extend(new_crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details = get_details_for_words(crises, en_common_words, en_word_utt_count,\n",
    "                                       min_dev_freq=10, \n",
    "                                       max_dev_freq=1000, \n",
    "                                       min_train_freq=100, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \"bow_crises_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details = get_details_for_words(crises, en_common_words, en_word_utt_count,\n",
    "                                       min_dev_freq=5, \n",
    "                                       max_dev_freq=1000, \n",
    "                                       min_train_freq=50, \n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \"bow_crises_vocab_more.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Spanish - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=18\n",
    "max_dev_freq=10000\n",
    "min_train_freq=200\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(es_common_words,\n",
    "                                          es_common_words,\n",
    "                                          es_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"].keys()), len(terms_of_interest[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_terms = [\"bueno\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms,\n",
    "                                             es_common_words,\n",
    "                                             es_word_utt_count,\n",
    "                                              min_dev_freq=min_dev_freq, \n",
    "                                              max_dev_freq=max_dev_freq, \n",
    "                                              min_train_freq=min_train_freq, \n",
    "                                              min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_terms_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details[\"train_utts\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details['words']['bueno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_es_top_words_vocab['freq'][b'bueno'], bow_es_top_words_vocab['freq_dev'][b'bueno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_100word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_1word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!su s1444673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_100word_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_es_top_words_vocab, open(os.path.join(m_cfg['data_path'], \"bow_es_top_words_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details['words']['colorado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(w, sample_terms_details['words'][w]['train']) \n",
    "       for w in sample_terms_details['words']], reverse=True, key=lambda t: t[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(w, sample_terms_details['words'][w]['dev']) \n",
    "       for w in sample_terms_details['words']], reverse=True, key=lambda t: t[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg['data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls fbanks_80dim_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg[\"sim_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'pre_trained_sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w'][b'sure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'mix_sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in mix_sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in [(w.decode(),[j.decode() for j in i]) for w, i in mix_sim_dict['w'].items() if len(i) > 2]:\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'pre_trained_sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in pre_sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in [(w.decode(),[j.decode() for j in i]) for w, i in pre_sim_dict['w'].items() if len(i) > 2]:\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sim_dict['w'][b'sure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = pickle.load(open(os.path.join(m_cfg['data_path'], 'sim.dict'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in sim_dict['w'].values() if len(i)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in [(w.decode(),[j.decode() for j in i]) for w, i in sim_dict['w'].items() if len(i) > 2]:\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_words = [w for w, i in pre_sim_dict['w'].items() if len(i)>1]\n",
    "fisher_words = [w for w, i in sim_dict['w'].items() if len(i)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_words), len(fisher_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_only = set(pre_words) - set(fisher_words)\n",
    "fisher_only = set(fisher_words) - set(pre_words)\n",
    "common_only = set(pre_words) & set(fisher_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_only), len(fisher_only),  len(common_only), (len(set(pre_words) | set(fisher_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in pre_sim_dict['w'].items() if len(i) > 2 and w in pre_only]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in sim_dict['w'].items() if len(i) > 2 and w in fisher_only]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in pre_sim_dict['w'].items() \n",
    "                    if (len(i) > 1 and w in common_only)]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, i in sorted([(w.decode(),[j.decode() for j in i]) for w, i in sim_dict['w'].items() \n",
    "                    if (len(i) > 1 and w in common_only)]):\n",
    "    print(w, \"  & \", \", \".join(set(i)-set([w])), \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word(curr_set, word_type, max_len=1):\n",
    "    found_count = 0\n",
    "    eng_tokens = []\n",
    "    for utt in map_dict[curr_set]:\n",
    "        if word_type.encode() in map_dict[curr_set][utt][\"es_w\"] and len(map_dict[curr_set][utt][\"es_w\"]) <= max_len:\n",
    "            found_count+=1\n",
    "            if curr_set == \"fisher_train\":\n",
    "                eng_tokens.append(\" \".join([w.decode() for w in map_dict[curr_set][utt][\"en_w\"]]))\n",
    "            else:\n",
    "                for r in map_dict[curr_set][utt][\"en_w\"]:\n",
    "                    eng_tokens.append(\" \".join([w.decode() for w in r]))\n",
    "    print(found_count, len(map_dict[curr_set]), \"{0:.2f}\".format(found_count / len(map_dict[curr_set]) * 100))\n",
    "    print(len(set(eng_tokens)))\n",
    "    return Counter(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = check_word(\"fisher_train\", \"si\", 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = check_word(\"fisher_dev\", \"mhm\", 1)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join(set([i[0] for i in t.most_common(10)]) and set([i[0] for i in d.most_common(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_count = 0\n",
    "eng_tokens = []\n",
    "curr_set= \"fisher_train\"\n",
    "for utt in map_dict[curr_set]:\n",
    "    if b\"claro\" in map_dict[curr_set][utt][\"es_w\"] and len(map_dict[curr_set][utt][\"es_w\"]) <= 1:\n",
    "        found_count+=1\n",
    "        eng_tokens.append(\" \".join([w.decode() for w in map_dict[curr_set][utt][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count, len(map_dict[curr_set]), found_count / len(map_dict[curr_set]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count = 0\n",
    "eng_tokens = []\n",
    "curr_set= \"fisher_dev\"\n",
    "for utt in map_dict[curr_set]:\n",
    "    if b\"claro\" in map_dict[curr_set][utt][\"es_w\"] and len(map_dict[curr_set][utt][\"es_w\"]) <= 1:\n",
    "        found_count+=1\n",
    "        eng_tokens.append(\" \".join([w.decode() for w in map_dict[curr_set][utt][\"en_w\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_count, len(map_dict[curr_set]), found_count / len(map_dict[curr_set]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(eng_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_names = [\"ref_min-0_max-300.en{0:d}\".format(i) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_names = []\n",
    "google_ref_names = []\n",
    "edin_ref_names = []\n",
    "for r in ref_names:\n",
    "    google_ref_names.append(os.path.join(\"google\", r))\n",
    "    edin_ref_names.append(os.path.join(\"./sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\", r))\n",
    "#     all_ref_names.append(os.path.join(\"sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\", r))\n",
    "    \n",
    "all_ref_names = google_ref_names + edin_ref_names    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_lines_in_file(fname):\n",
    "    all_lines = []\n",
    "    with open(fname, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            all_lines.append(set(line.strip().split()))\n",
    "    return all_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_in_file(fname):\n",
    "    all_words = []\n",
    "    with open(fname, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            all_words.extend(line.strip().split())\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_ref = get_all_words_in_file(all_ref_names[0])\n",
    "all_words_ref = get_all_words_in_file(all_ref_names[0])\n",
    "\n",
    "for r in all_ref_names[1:]:\n",
    "    common_words_ref &= get_all_words_in_file(r)\n",
    "    all_words_ref |= get_all_words_in_file(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_words_ref), len(all_words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_common_words_ref = get_all_words_in_file(google_ref_names[0])\n",
    "google_all_words_ref = get_all_words_in_file(google_ref_names[0])\n",
    "\n",
    "for r in google_ref_names[1:]:\n",
    "    google_common_words_ref &= get_all_words_in_file(r)\n",
    "    google_all_words_ref |= get_all_words_in_file(r)\n",
    "print(len(google_common_words_ref), len(google_all_words_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edin_common_words_ref = get_all_words_in_file(edin_ref_names[0])\n",
    "edin_all_words_ref = get_all_words_in_file(edin_ref_names[0])\n",
    "\n",
    "for r in edin_ref_names[1:]:\n",
    "    edin_common_words_ref &= get_all_words_in_file(r)\n",
    "    edin_all_words_ref |= get_all_words_in_file(r)\n",
    "print(len(edin_common_words_ref), len(edin_all_words_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(google_common_words_ref & edin_common_words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ref_words = google_common_words_ref & edin_common_words_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_ref_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(google_all_words_ref - edin_all_words_ref), len(edin_all_words_ref - google_all_words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_lines = {}\n",
    "for i,r in enumerate(edin_ref_names):\n",
    "    all_ref_lines[i] = read_all_lines_in_file(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_all_refs = []\n",
    "\n",
    "for i in range(len(all_ref_lines[0])):\n",
    "    words_ref = all_ref_lines[0][i]\n",
    "    for j in range(1,4):\n",
    "        words_ref &= all_ref_lines[j][i]\n",
    "    words_in_all_refs.append(words_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "\n",
    "for ref in words_in_all_refs:\n",
    "    now_words = ref & common_ref_words - stop_words - es_stop_words\n",
    "    for w in now_words:\n",
    "        if w not in word_freq:\n",
    "            word_freq[w] = 0\n",
    "        word_freq[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words = [(w,f) for w, f in word_freq.items() if f >= 2 and f <=5 and len(w) >= 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval 1 - 500 randomly selected frequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=25\n",
    "max_dev_freq=10000\n",
    "min_train_freq=25\n",
    "max_train_freq=10000\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_content_words = ((en_common_words & common_ref_words) - (es_stop_words | stop_words))\n",
    "en_content_words = {w for w in en_content_words if '¿' not in w and \"'\" not in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(en_content_words, en_content_words, en_word_utt_count, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq, \n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"]), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, en_common_words, en_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = (get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), \n",
    "       get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join(sample_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_freq_content = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_freq_content, \n",
    "            open(os.path.join(m_cfg['data_path'], \n",
    "                              \"eval_en_freq_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval 2 - 500 randomly selected infrequent words, minor filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=2\n",
    "max_dev_freq=10\n",
    "min_train_freq=2\n",
    "max_train_freq=25\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_content_words = (en_common_words - (es_stop_words | stop_words))\n",
    "# en_content_words = {w for w in en_content_words if '¿' not in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(en_content_words, en_content_words, en_word_utt_count, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"]), 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, en_common_words, en_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = (get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), \n",
    "       get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join(sample_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_content = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_content, \n",
    "            open(os.path.join(m_cfg['data_path'], \n",
    "                              \"eval_en_rare_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval 3 - common es, en words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dev_freq=2\n",
    "max_dev_freq=10000\n",
    "min_train_freq=2\n",
    "max_train_freq=100000\n",
    "min_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_content_words = (en_common_words - (es_stop_words | stop_words))\n",
    "# en_content_words = {w for w in en_content_words if '¿' not in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_content_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_common_words = (es_common_words & en_common_words & common_ref_words)  - (es_stop_words | stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = get_details_for_words(es_en_common_words, en_content_words, en_word_utt_count, \n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(terms_of_interest[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(terms_of_interest[\"train_utts\"], key=\"fisher_train\"), get_duration(terms_of_interest[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "sample_terms = random.sample(list(terms_of_interest[\"words\"].keys()), \n",
    "                             min(len(terms_of_interest[\"words\"]), 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_terms_details = get_details_for_words(sample_terms, en_common_words, en_word_utt_count,\n",
    "                                          min_dev_freq=min_dev_freq, \n",
    "                                          max_dev_freq=max_dev_freq, \n",
    "                                          min_train_freq=min_train_freq,\n",
    "                                          max_train_freq=max_train_freq,\n",
    "                                          min_len=min_len)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(sample_terms_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = (get_duration(sample_terms_details[\"train_utts\"], key=\"fisher_train\"), \n",
    "       get_duration(sample_terms_details[\"dev_utts\"], key=\"fisher_dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join(sample_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_content = create_vocab(sample_terms_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(eval_content, \n",
    "            open(os.path.join(m_cfg['data_path'], \n",
    "                              \"eval_en_es_common_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_common_words = [w for w in es_common_words & en_common_words if len(w) >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_en_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_en_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval crisis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises = set()\n",
    "with open(crises_lex_fname, \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        crises.update(line.strip().split())\n",
    "crises = list(crises)\n",
    "crises_stem = [stem(w) for w in crises]\n",
    "\n",
    "# new_crises = []\n",
    "# for t in crises:\n",
    "#     if t.encode() in sim_dict['w']:\n",
    "#         new_crises.extend([w.decode() for w in sim_dict['w'][t.encode()]])\n",
    "# crises.extend(new_crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_details = get_details_for_words(crises, (en_common_words & common_ref_words), en_word_utt_count,\n",
    "                                       min_dev_freq=2, \n",
    "                                       max_dev_freq=5000, \n",
    "                                       min_train_freq=2,\n",
    "                                       max_train_freq=5000,\n",
    "                                       min_len=1)\n",
    "print(\"total words meeting criteria = {0:d}\".format(len(crises_details[\"words\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = get_duration(crises_details[\"train_utts\"], key=\"fisher_train\"), get_duration(crises_details[\"dev_utts\"], key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_word_utt_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_word_utt_count['dev_utts']['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(crises_details[\"words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" -- \".join(list(crises_details[\"words\"].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# crises_details[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_crises_vocab = create_vocab(crises_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bow_crises_vocab, open(os.path.join(m_cfg['data_path'], \n",
    "                                                \"eval_en_crisis_vocab.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words = set(en_word_utt_count['train'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(en_words)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"howdy\".encode() in vocab_dict[\"en_w\"][\"w2i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = {}\n",
    "\n",
    "for w in tqdm(en_words):\n",
    "    s = []\n",
    "#     print(w)\n",
    "    w_syn = wn.synsets(w)\n",
    "    for item in w_syn:\n",
    "        for lm in item.lemma_names():\n",
    "            if lm in en_words:\n",
    "                s.append(lm)\n",
    "        # end for all lemmas\n",
    "    # end for syns\n",
    "    s.append(w)\n",
    "    syns[w] = list(set(s))\n",
    "# end for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_sim = {\"w\":{}, \"i\":{}}\n",
    "\n",
    "for w in tqdm(vocab_dict[\"en_w\"][\"w2i\"].keys()):\n",
    "    syn_sim[\"w\"][w] = [w.encode() for w in syns.get(w.decode(), [w.decode()])]\n",
    "    i = vocab_dict[\"en_w\"][\"w2i\"][w]\n",
    "    syn_sim[\"i\"][i] = [vocab_dict[\"en_w\"][\"w2i\"][j] for j in syn_sim[\"w\"][w]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(syn_sim[\"w\"].items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(syn_sim, open(os.path.join(m_cfg['data_path'], \"en_syns_train.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = {w:v for w, v in syns.items() if len(v) >= 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(haha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict[\"en_w\"][\"i2w\"][14261]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_keys = set([w for w in sim_dict['w'] if len(sim_dict[\"w\"][w]) >= 2])\n",
    "syn_keys = set([w for w in syn_sim['w'] if len(syn_sim[\"w\"][w]) >= 2])\n",
    "print(len(sim_keys), len(syn_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" -- \".join([w.decode() for w in syn_sim[\"w\"][b'run']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_keys - sim_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = \"hello\"\n",
    "w_syn = wn.synsets(w)\n",
    "for item in w_syn:\n",
    "    for lm in item.lemma_names():\n",
    "        if lm in en_words:\n",
    "            syns[w].append(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_set = []\n",
    "for synset in wn.synsets(\"hello\"):\n",
    "    for item in synset.lemma_names():\n",
    "        syn_set.append(item)\n",
    "print(syn_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns[1].lemmas()[3].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in syns:\n",
    "    print(s.lemmas()[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.lemmas[0] for s in syns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ES, EN common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_out_str(out_str):\n",
    "    out_str = out_str.replace(\"`\", \"\")\n",
    "    out_str = out_str.replace('\"', '')\n",
    "    out_str = out_str.replace('¿', '')\n",
    "    out_str = out_str.replace(\"''\", \"\")\n",
    "    out_str = out_str.strip()\n",
    "    return out_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_str(h):\n",
    "    out_str = \"\"\n",
    "    for w in [i.decode() for i in h]:\n",
    "        out_str += \"{0:s}\".format(w) if (w.startswith(\"'\") or w==\"n't\") else \" {0:s}\".format(w)\n",
    "\n",
    "    out_str = clean_out_str(out_str)\n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_es_en(es_words, en_words):\n",
    "    utt_es_en_stats = {\"common\":{}, \"es\": {}, \"en\": {}}\n",
    "    \n",
    "    utt_es_en_stats[\"es\"] = Counter(es_words)\n",
    "    utt_es_en_stats[\"en\"] = Counter(en_words)\n",
    "    \n",
    "    common_es_en = set(utt_es_en_stats[\"es\"].keys()) & set(utt_es_en_stats[\"en\"].keys())\n",
    "    \n",
    "    for w in common_es_en:\n",
    "        utt_es_en_stats[\"common\"][w] = min(utt_es_en_stats[\"es\"][w], utt_es_en_stats[\"en\"][w])\n",
    "    \n",
    "    return utt_es_en_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stop_words = (set(nltk.corpus.stopwords.words(\"english\")) | \n",
    "                    set(nltk.corpus.stopwords.words(\"spanish\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_details(common_results, show=20, min_len=5, max_freq=2, filter_stop=False):\n",
    "    for k in common_results:\n",
    "        print(\"{0:10s} = {1:>10d} types\".format(k,len(common_results[k])))\n",
    "        \n",
    "    tot = {}\n",
    "    print(\"-\"*60)\n",
    "    for k in common_results:\n",
    "        tot[k] = sum(common_results[k].values())\n",
    "        print(\"{0:10s} = {1:>10d} tokens\".format(k,tot[k]))\n",
    "        \n",
    "    print(\"-\"*60)\n",
    "    print(\"common / es = {0:.2f}%\".format(tot[\"common\"]/tot[\"es\"]*100))\n",
    "    print(\"common / en = {0:.2f}%\".format(tot[\"common\"]/tot[\"en\"]*100))\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    common_words = sorted(common_results[\"common\"].items(), reverse=True, key=lambda t: t[1])\n",
    "    \n",
    "    if filter_stop:\n",
    "        common_words = [(w,c) for w,c in common_words \n",
    "                        if w not in all_stop_words and \n",
    "                        len(w) >= min_len and \"'\" not in w and common_results[\"en\"][w] <= max_freq]\n",
    "        \n",
    "    non_cv_words = {}\n",
    "    set_common_words = set(common_results[\"common\"].keys())\n",
    "    for w,c in common_results[\"en\"].items():\n",
    "        if ((w not in set_common_words) and (w not in all_stop_words) and \n",
    "            (len(w) >= min_len) and (\"'\" not in w) and \n",
    "            (common_results[\"en\"][w] <= max_freq)):\n",
    "            non_cv_words[w] = common_results[\"en\"][w]\n",
    "    \n",
    "    print(\"total content word types: {0:d}\".format(len(common_words)))\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    tot_content = sum([c for w,c in common_words])\n",
    "    print(\"total content tokens: {0:d}\".format(tot_content))\n",
    "    print(\"common content / es = {0:.2f}%\".format(tot_content/tot[\"es\"]*100))\n",
    "    print(\"common content / en = {0:.2f}%\".format(tot_content/tot[\"en\"]*100))\n",
    "    print(\"-\"*60)\n",
    "    print(\"Top common words\")\n",
    "    for w, c in common_words[:show]:\n",
    "        print(\"{0:20s} || {1:10d}\".format(w,c))\n",
    "        \n",
    "    return {w:c for w,c in common_words}, non_cv_words\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_level_es_en(set_key, ref_num=0):\n",
    "    utts_with_common = []\n",
    "    es_en_stats = {\"common\":{}, \"es\": {}, \"en\": {}}\n",
    "\n",
    "    for u in tqdm(map_dict[set_key], ncols=80):\n",
    "        es_words = get_out_str(map_dict[set_key][u][\"es_w\"]).strip().split()\n",
    "        if set_key == \"fisher_dev\":\n",
    "            en_words = get_out_str(map_dict[set_key][u][\"en_w\"][ref_num]).strip().split()\n",
    "        else:\n",
    "            en_words = get_out_str(map_dict[set_key][u][\"en_w\"]).strip().split()\n",
    "        \n",
    "        utt_es_en_stats = match_es_en(es_words, en_words)\n",
    "    \n",
    "        common_words = [w for w,c in utt_es_en_stats[\"common\"].items() \n",
    "                            if w not in all_stop_words \n",
    "                            and len(w) >= 5 and \"'\" not in w]\n",
    "        \n",
    "        #print(common_words)\n",
    "        \n",
    "        if len(common_words) > 0:\n",
    "            utts_with_common.append(u)\n",
    "        \n",
    "        #print(utt_es_en_stats)\n",
    "        for k in utt_es_en_stats:\n",
    "            for w in utt_es_en_stats[k]:\n",
    "                if w not in es_en_stats[k]:\n",
    "                    es_en_stats[k][w] = 0\n",
    "                es_en_stats[k][w] += utt_es_en_stats[k][w]\n",
    "        # end for update counts\n",
    "    # end for all utts\n",
    "    \n",
    "    print(\"utts with common words: {0:d} / {1:d}\".format(len(utts_with_common), \n",
    "                                                         len(map_dict[set_key])))\n",
    "    return es_en_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_common = corpus_level_es_en(\"fisher_dev\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_comm_cont, dev_non_cv = show_details(dev_common, show=10, \n",
    "                                         min_len=5, \n",
    "                                         max_freq=1, \n",
    "                                         filter_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_comm_cont[\"annie\"], dev_common[\"en\"][\"annie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dev_comm_cont.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 138819/138819 [00:06<00:00, 20604.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utts with common words: 18491 / 138819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_common = corpus_level_es_en(\"fisher_train\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comm_cont, train_non_cv = show_details(train_common, show=10, \n",
    "                                             min_len=5, \n",
    "                                             max_freq=10, \n",
    "                                             filter_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_comm_cont), len(train_non_cv), len(dev_comm_cont), len(dev_non_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_comm_cont.values()), sum(train_non_cv.values()), sum(dev_comm_cont.values()), sum(dev_non_cv.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dev_non_cv.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_words = set(train_comm_cont.keys()) & all_dev_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cv_words = (set(train_non_cv.keys())-in_vocab_rare_words) & all_dev_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train_comm_cont.keys())), len(cv_words), len(non_cv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_noncv_words = {\"cv\": cv_words, \"noncv\": non_cv_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv_noncv_words, open(os.path.join(m_cfg[\"data_path\"],\"cv_noncv.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_common_words = {\"train\": train_comm_cont, \"dev\": dev_comm_cont, \n",
    "                        \"train_non_cv\": train_non_cv, \"dev_non_cv\": dev_non_cv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg[\"data_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(content_common_words, open(os.path.join(m_cfg[\"data_path\"],\"common_content.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_words = get_out_str(map_dict[\"fisher_dev\"][\"20051009_182032_217_fsp-B-2\"][\"es_w\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"chemistry\" in dev_common[\"common\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_non_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_comm_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = set(dev_common[\"en\"].keys()) - set(train_common[\"en\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words = []\n",
    "with open(\"interspeech/sp_160hrs/ref_min-0_max-300.en1\", \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        t_words.extend(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words_count = Counter(t_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = set(dev_comm_cont.keys())\n",
    "k2 = set(t_words_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(dev_comm_cont.keys()) & oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(dev_non_cv.keys()) & oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(k1), len(k2), len(k1 & k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in k1:\n",
    "    if dev_comm_cont[w] != t_words_count[w]:\n",
    "        print(w, dev_comm_cont[w], t_words_count[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words = []\n",
    "with open(\"interspeech/sp_160hrs/ref_min-0_max-300.en0\", \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        t_words.extend(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words_count = Counter(t_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dev_en = set(t_words_count.keys())\n",
    "all_train_en = set(train_common[\"en\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_dev_en), len(all_train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = all_dev_en - all_train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([v for w,v in t_words_count.items() if w in oov])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_counter = {w:v for w,v in t_words_count.items() if w in oov}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(oov_counter, open(os.path.join(m_cfg[\"data_path\"],\"oov.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words in google preds not in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_words = []\n",
    "with open(\"google/beam_min-0_max-300.en\", \"r\") as in_f:\n",
    "    for line in in_f:\n",
    "        g_words.extend(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_word_counter = Counter(g_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g_word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_new_words = set(g_word_counter.keys()) - all_train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g_new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([v for w,v in g_word_counter.items() if w in g_new_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"craigslists\" in all_train_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([v for w,v in g_word_counter.items() if w in (g_new_words & all_dev_en)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_new_words & oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_new_words & oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_new_words & oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([v for w,v in g_word_counter.items() if w in (g_new_words & oov)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_common[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_train_words(word_freq, show=5, min_len=5, min_freq=1, max_freq=10):\n",
    "    tot = sum(word_freq.values())\n",
    "    print(\"-\"*60)    \n",
    "    print(\"total types={0:>10d}, tokens = {1:>10d}\".format(len(word_freq), tot))\n",
    "    print(\"-\"*60)    \n",
    "    \n",
    "    common_words = sorted(word_freq.items(), reverse=True, key=lambda t: t[1])\n",
    "    \n",
    "    common_words = [(w,c) for w,c in common_words \n",
    "                    if w not in all_stop_words and \n",
    "                    len(w) >= min_len and \n",
    "                    \"'\" not in w and \n",
    "                    word_freq[w] >= min_freq and\n",
    "                    word_freq[w] <= max_freq]\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    print(\"total filter word types: {0:d}\".format(len(common_words)))\n",
    "    \n",
    "    tot_content = sum([c for w,c in common_words])\n",
    "    print(\"total filter tokens: {0:d}\".format(tot_content))\n",
    "    print(\"filter content / en = {0:.2f}%\".format(tot_content/tot*100))\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    print(\"-\"*60)    \n",
    "    in_dev = all_dev_en & set([w for w,c in common_words])\n",
    "    in_dev_tokens = sum([t_words_count[w] for w in in_dev])\n",
    "    print(\"total filter word types in dev: {0:d}, tokens: {1:d}\".format(len(in_dev), in_dev_tokens))\n",
    "    print(\"-\"*60)    \n",
    "    \n",
    "    \n",
    "    sel_words = {w:c for w,c in common_words if w in in_dev}\n",
    "    print(\"Top common words\")\n",
    "    for w, c in sorted(sel_words.items(), reverse=True, key=lambda t:t[1])[:show]:\n",
    "        print(\"{0:20s} || {1:10d} || {2:10d}\".format(w,c, t_words_count[w]))\n",
    "        \n",
    "    return sel_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_common' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-46d216b7b10a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_rare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_train_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_common\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_common' is not defined"
     ]
    }
   ],
   "source": [
    "train_rare = filter_train_words(train_common[\"en\"], min_len=5, min_freq=1, max_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_medium = filter_train_words(train_common[\"en\"], min_len=5, min_freq=25, max_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "total types=     18140, tokens =    1440914\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "total filter word types: 386\n",
      "total filter tokens: 199438\n",
      "filter content / en = 13.84%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "total filter word types in dev: 365, tokens: 5167\n",
      "------------------------------------------------------------\n",
      "Top common words\n",
      "right                ||       7249 ||        163\n",
      "think                ||       6676 ||        120\n",
      "people               ||       6522 ||        183\n",
      "really               ||       4265 ||         72\n",
      "things               ||       3058 ||         91\n"
     ]
    }
   ],
   "source": [
    "train_frequent = filter_train_words(train_common[\"en\"], min_len=5, min_freq=150, max_freq=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_tables = {\"rare\": train_rare, \"medium\": train_medium, \"frequent\": train_frequent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(freq_tables, open(os.path.join(m_cfg[\"data_path\"],\"word_freq.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = pickle.load(open(os.path.join(m_cfg[\"data_path\"],\"word_freq.p\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marry', 100),\n",
       " ('needed', 100),\n",
       " ('building', 100),\n",
       " ('idaho', 100),\n",
       " ('hardly', 99),\n",
       " ('knowing', 99),\n",
       " ('train', 99),\n",
       " ('light', 99),\n",
       " ('bigger', 98),\n",
       " ('supposed', 98),\n",
       " ('colorado', 98),\n",
       " ('georgia', 98),\n",
       " ('students', 98),\n",
       " ('cheap', 98),\n",
       " ('talks', 97),\n",
       " ('treat', 97),\n",
       " ('weird', 97),\n",
       " ('loved', 97),\n",
       " ('guilty', 97),\n",
       " ('student', 97),\n",
       " ('studies', 96),\n",
       " ('behind', 96),\n",
       " ('ricans', 96),\n",
       " ('shows', 96),\n",
       " ('spoken', 95),\n",
       " ('eighteen', 95),\n",
       " ('connecticut', 95),\n",
       " ('involved', 95),\n",
       " ('economy', 95),\n",
       " ('teacher', 95),\n",
       " ('jewish', 95),\n",
       " ('justice', 94),\n",
       " ('community', 93),\n",
       " ('looked', 93),\n",
       " ('thinks', 93),\n",
       " ('reggaeton', 93),\n",
       " ('supposedly', 93),\n",
       " ('fighting', 92),\n",
       " ('nebraska', 92),\n",
       " ('killed', 92),\n",
       " ('relation', 92),\n",
       " ('illegal', 92),\n",
       " ('related', 91),\n",
       " ('communicate', 91),\n",
       " ('putting', 91),\n",
       " ('stopped', 91),\n",
       " ('twice', 91),\n",
       " ('order', 91),\n",
       " ('explain', 90),\n",
       " ('leaving', 90),\n",
       " ('computers', 90),\n",
       " ('types', 90),\n",
       " ('emergency', 90),\n",
       " ('judge', 90),\n",
       " ('situations', 89),\n",
       " ('longer', 89),\n",
       " ('traffic', 89),\n",
       " ('laugh', 89),\n",
       " ('careful', 89),\n",
       " ('station', 89),\n",
       " ('afraid', 89),\n",
       " ('cause', 88),\n",
       " ('sleep', 88),\n",
       " ('christians', 88),\n",
       " ('feels', 88),\n",
       " ('feeling', 88),\n",
       " ('services', 88),\n",
       " ('selling', 88),\n",
       " ('understood', 87),\n",
       " ('sunday', 87),\n",
       " ('please', 87),\n",
       " ('grandmother', 87),\n",
       " ('channel', 87),\n",
       " ('asking', 87),\n",
       " ('mostly', 86),\n",
       " ('account', 86),\n",
       " ('border', 86),\n",
       " ('special', 86),\n",
       " ('popular', 86),\n",
       " ('returned', 85),\n",
       " ('changing', 85),\n",
       " ('contact', 85),\n",
       " ('impossible', 85),\n",
       " ('trust', 84),\n",
       " ('helped', 84),\n",
       " ('pleasure', 84),\n",
       " ('papers', 84),\n",
       " ('positive', 84),\n",
       " ('drugs', 84),\n",
       " ('seemed', 84),\n",
       " ('races', 84),\n",
       " ('vacations', 84),\n",
       " ('cultural', 83),\n",
       " ('quickly', 83),\n",
       " ('worst', 83),\n",
       " ('plays', 83),\n",
       " ('classical', 83),\n",
       " ('december', 83),\n",
       " ('kinds', 82),\n",
       " ('force', 82),\n",
       " ('practice', 82),\n",
       " ('enjoy', 81),\n",
       " ('capital', 81),\n",
       " ('stand', 81),\n",
       " ('hands', 81),\n",
       " ('reach', 80),\n",
       " ('merengue', 80),\n",
       " ('mountains', 79),\n",
       " ('necessarily', 79),\n",
       " ('count', 79),\n",
       " ('reasons', 79),\n",
       " ('streets', 79),\n",
       " ('engineering', 79),\n",
       " ('market', 79),\n",
       " ('moving', 78),\n",
       " ('differences', 78),\n",
       " ('towards', 78),\n",
       " ('clear', 77),\n",
       " ('growing', 77),\n",
       " ('complain', 77),\n",
       " ('meeting', 77),\n",
       " ('seriously', 77),\n",
       " ('famous', 77),\n",
       " ('generally', 77),\n",
       " ('virginia', 76),\n",
       " ('goodness', 76),\n",
       " ('separated', 76),\n",
       " ('career', 76),\n",
       " ('depending', 76),\n",
       " ('compared', 76),\n",
       " ('didn´t', 76),\n",
       " ('benefits', 76),\n",
       " ('cover', 76),\n",
       " ('share', 76),\n",
       " ('suffer', 75),\n",
       " ('languages', 75),\n",
       " ('legal', 75),\n",
       " ('story', 75),\n",
       " ('early', 74),\n",
       " ('among', 74),\n",
       " ('bored', 74),\n",
       " ('minute', 74),\n",
       " ('begin', 74),\n",
       " ('offer', 74),\n",
       " ('helping', 73),\n",
       " ('clean', 73),\n",
       " ('regarding', 73),\n",
       " ('lately', 73),\n",
       " ('loves', 73),\n",
       " ('blacks', 73),\n",
       " ('adults', 72),\n",
       " ('vacation', 72),\n",
       " ('peruvian', 72),\n",
       " ('wherever', 72),\n",
       " ('sound', 72),\n",
       " ('within', 72),\n",
       " ('barely', 72),\n",
       " ('anywhere', 71),\n",
       " ('touch', 71),\n",
       " ('throw', 71),\n",
       " ('brothers', 71),\n",
       " ('bothers', 71),\n",
       " ('immediately', 71),\n",
       " ('traveling', 71),\n",
       " ('baltimore', 71),\n",
       " ('plans', 70),\n",
       " ('games', 70),\n",
       " ('traveled', 70),\n",
       " ('forgot', 70),\n",
       " ('consider', 70),\n",
       " ('somebody', 70),\n",
       " ('arrive', 70),\n",
       " ('cultures', 70),\n",
       " ('manuel', 70),\n",
       " ('temperature', 69),\n",
       " ('opera', 69),\n",
       " ('closer', 69),\n",
       " ('romantic', 69),\n",
       " ('directly', 69),\n",
       " ('easily', 67),\n",
       " ('raise', 67),\n",
       " ('rules', 67),\n",
       " ('international', 67),\n",
       " ('paper', 67),\n",
       " ('interracial', 67),\n",
       " ('boring', 66),\n",
       " ('natural', 66),\n",
       " ('received', 66),\n",
       " ('store', 66),\n",
       " ('catholics', 66),\n",
       " ('played', 66),\n",
       " ('topics', 66),\n",
       " ('opposite', 66),\n",
       " ('emails', 66),\n",
       " ('coast', 65),\n",
       " ('taught', 65),\n",
       " ('messages', 65),\n",
       " ('beaches', 64),\n",
       " ('stupid', 64),\n",
       " ('belong', 64),\n",
       " ('dress', 64),\n",
       " ('dominican', 64),\n",
       " ('miles', 64),\n",
       " ('ideas', 63),\n",
       " ('video', 63),\n",
       " ('citizen', 63),\n",
       " ('european', 63),\n",
       " ('apart', 62),\n",
       " ('numbers', 62),\n",
       " ('large', 62),\n",
       " ('independent', 62),\n",
       " ('everyday', 62),\n",
       " ('peaceful', 61),\n",
       " ('january', 61),\n",
       " ('regular', 61),\n",
       " ('salvador', 61),\n",
       " ('exists', 61),\n",
       " ('freedom', 61),\n",
       " ('record', 60),\n",
       " ('style', 60),\n",
       " ('blocks', 60),\n",
       " ('letters', 60),\n",
       " ('simple', 60),\n",
       " ('saturday', 59),\n",
       " ('surprised', 59),\n",
       " ('finally', 59),\n",
       " ('wonderful', 58),\n",
       " ('based', 58),\n",
       " ('costs', 58),\n",
       " ('italy', 58),\n",
       " ('biggest', 58),\n",
       " ('dream', 58),\n",
       " ('effort', 57),\n",
       " ('visited', 57),\n",
       " ('distance', 57),\n",
       " ('muslim', 57),\n",
       " ('singer', 57),\n",
       " ('season', 57),\n",
       " ('sending', 57),\n",
       " ('newspaper', 57),\n",
       " ('trouble', 56),\n",
       " ('favorite', 56),\n",
       " ('listened', 56),\n",
       " ('shopping', 56),\n",
       " ('green', 56),\n",
       " ('apartments', 56),\n",
       " ('block', 56),\n",
       " ('fixed', 55),\n",
       " ('willing', 55),\n",
       " ('economical', 55),\n",
       " ('including', 55),\n",
       " ('animals', 55),\n",
       " ('heart', 55),\n",
       " ('liberal', 55),\n",
       " ('drink', 54),\n",
       " ('apply', 54),\n",
       " ('angry', 54),\n",
       " ('normally', 54),\n",
       " ('thousands', 54),\n",
       " ('leaves', 53),\n",
       " ('graduated', 53),\n",
       " ('killing', 53),\n",
       " ('canadian', 53),\n",
       " ('affect', 53),\n",
       " ('danger', 53),\n",
       " ('plane', 53),\n",
       " ('science', 53),\n",
       " ('millions', 53),\n",
       " ('transportation', 53),\n",
       " ('hotel', 52),\n",
       " ('dinner', 52),\n",
       " ('confused', 52),\n",
       " ('friday', 52),\n",
       " ('separate', 52),\n",
       " ('became', 52),\n",
       " ('except', 52),\n",
       " ('favor', 52),\n",
       " ('showing', 52),\n",
       " ('blood', 52),\n",
       " ('ridiculous', 52),\n",
       " ('assistance', 52),\n",
       " ('faster', 51),\n",
       " ('badly', 51),\n",
       " ('india', 51),\n",
       " ('military', 51),\n",
       " ('union', 51),\n",
       " ('covered', 51),\n",
       " ('teaching', 50),\n",
       " ('cousin', 50),\n",
       " ('november', 50),\n",
       " ('rooms', 50),\n",
       " ('latino', 50),\n",
       " ('spiritual', 50),\n",
       " ('keeps', 50),\n",
       " ('connected', 50),\n",
       " ('whites', 50),\n",
       " ('court', 50),\n",
       " ('salary', 50),\n",
       " ('search', 49),\n",
       " ('weekends', 49),\n",
       " ('beliefs', 49),\n",
       " ('negative', 49),\n",
       " ('daily', 49),\n",
       " ('stations', 49),\n",
       " ('names', 49),\n",
       " ('miguel', 49),\n",
       " ('dollar', 49),\n",
       " ('caribbean', 49),\n",
       " ('graduate', 49),\n",
       " ('japanese', 49),\n",
       " ('address', 49),\n",
       " ('cellphones', 49),\n",
       " ('local', 49),\n",
       " ('books', 49),\n",
       " ('report', 49),\n",
       " ('equal', 49),\n",
       " ('cristina', 49),\n",
       " ('familiar', 48),\n",
       " ('snows', 48),\n",
       " ('chemistry', 48),\n",
       " ('pesos', 48),\n",
       " ('poverty', 48),\n",
       " ('daughters', 48),\n",
       " ('forward', 47),\n",
       " ('lyrics', 47),\n",
       " ('steal', 47),\n",
       " ('professional', 47),\n",
       " ('faith', 46),\n",
       " ('recording', 46),\n",
       " ('national', 46),\n",
       " ('march', 46),\n",
       " ('adult', 46),\n",
       " ('benefit', 46),\n",
       " ('buildings', 46),\n",
       " ('avoid', 46),\n",
       " ('israel', 46),\n",
       " ('persons', 45),\n",
       " ('cross', 45),\n",
       " ('sisters', 45),\n",
       " ('snowed', 45),\n",
       " ('france', 45),\n",
       " ('swear', 45),\n",
       " ('youngest', 44),\n",
       " ('truly', 44),\n",
       " ('death', 44),\n",
       " ('dominicans', 44),\n",
       " ('catch', 44),\n",
       " ('citizens', 44),\n",
       " ('electronic', 44),\n",
       " ('believes', 43),\n",
       " ('carmen', 43),\n",
       " ('coffee', 43),\n",
       " ('belief', 43),\n",
       " ('fault', 43),\n",
       " ('breaks', 43),\n",
       " ('quality', 43),\n",
       " ('website', 43),\n",
       " ('attitude', 43),\n",
       " ('beauty', 43),\n",
       " ('ahead', 42),\n",
       " ('nineteen', 42),\n",
       " ('lasted', 42),\n",
       " ('construction', 42),\n",
       " ('nations', 42),\n",
       " ('piano', 42),\n",
       " ('honduras', 42),\n",
       " ('elections', 42),\n",
       " ('protect', 42),\n",
       " ('paris', 42),\n",
       " ('kitchen', 41),\n",
       " ('activities', 41),\n",
       " ('answered', 41),\n",
       " ('responsible', 41),\n",
       " ('falls', 41),\n",
       " ('accepted', 41),\n",
       " ('press', 41),\n",
       " ('broken', 41),\n",
       " ('aside', 41),\n",
       " ('subway', 41),\n",
       " ('considered', 40),\n",
       " ('understanding', 40),\n",
       " ('boyfriends', 40),\n",
       " ('healthy', 40),\n",
       " ('suffering', 40),\n",
       " ('alcohol', 40),\n",
       " ('knowledge', 40),\n",
       " ('ecuador', 40),\n",
       " ('missing', 40),\n",
       " ('columbia', 40),\n",
       " ('republicans', 40),\n",
       " ('agreement', 40),\n",
       " ('image', 40),\n",
       " ('circumstances', 40),\n",
       " ('expenses', 39),\n",
       " ('outskirts', 39),\n",
       " ('unite', 39),\n",
       " ('clubs', 39),\n",
       " ('stays', 39),\n",
       " ('affects', 39),\n",
       " ('annoying', 39),\n",
       " ('treated', 39),\n",
       " ('arizona', 39),\n",
       " ('whoever', 39),\n",
       " ('feelings', 39),\n",
       " ('exercise', 39),\n",
       " ('whenever', 38),\n",
       " ('cousins', 38),\n",
       " ('create', 38),\n",
       " ('concerts', 38),\n",
       " ('ticket', 38),\n",
       " ('jenny', 38),\n",
       " ('masters', 38),\n",
       " ('limit', 38),\n",
       " ('theory', 38),\n",
       " ('build', 38),\n",
       " ('welfare', 38),\n",
       " ('mature', 37),\n",
       " ('maintain', 37),\n",
       " ('drinking', 37),\n",
       " ('attack', 37),\n",
       " ('leader', 37),\n",
       " ('holidays', 37),\n",
       " ('grown', 37),\n",
       " ('technical', 37),\n",
       " ('angel', 37),\n",
       " ('details', 36),\n",
       " ('universities', 36),\n",
       " ('educated', 36),\n",
       " ('cards', 36),\n",
       " ('opened', 36),\n",
       " ('attend', 36),\n",
       " ('seattle', 36),\n",
       " ('title', 36),\n",
       " ('anybody', 36),\n",
       " ('organized', 36),\n",
       " ('dependent', 36),\n",
       " ('income', 36),\n",
       " ('impressive', 36),\n",
       " ('racial', 36),\n",
       " ('santiago', 36),\n",
       " ('floor', 36),\n",
       " ('montreal', 36),\n",
       " ('participated', 35),\n",
       " ('development', 35),\n",
       " ('sleeping', 35),\n",
       " ('david', 35),\n",
       " ('systems', 35),\n",
       " ('youth', 35),\n",
       " ('brown', 35),\n",
       " ('dressed', 35),\n",
       " ('becoming', 35),\n",
       " ('neighbors', 35),\n",
       " ('moments', 35),\n",
       " ('neighborhoods', 35),\n",
       " ('product', 35),\n",
       " ('humidity', 35),\n",
       " ('include', 35),\n",
       " ('improve', 35),\n",
       " ('aspect', 35),\n",
       " ('relax', 34),\n",
       " ('present', 34),\n",
       " ('hearing', 34),\n",
       " ('mercedes', 34),\n",
       " ('definitively', 34),\n",
       " ('global', 34),\n",
       " ('picked', 34),\n",
       " ('marketing', 34),\n",
       " ('shoes', 34),\n",
       " ('mortgage', 34),\n",
       " ('weight', 33),\n",
       " ('jesus', 33),\n",
       " ('curious', 33),\n",
       " ('survive', 33),\n",
       " ('sundays', 33),\n",
       " ('believed', 33),\n",
       " ('division', 33),\n",
       " ('variety', 33),\n",
       " ('convince', 33),\n",
       " ('courses', 33),\n",
       " ('september', 33),\n",
       " ('flight', 33),\n",
       " ('stable', 33),\n",
       " ('missed', 33),\n",
       " ('wallet', 33),\n",
       " ('therefore', 33),\n",
       " ('doors', 33),\n",
       " ('civil', 33),\n",
       " ('oldest', 32),\n",
       " ('christ', 32),\n",
       " ('finds', 32),\n",
       " ('opinions', 32),\n",
       " ('domingo', 32),\n",
       " ('semester', 32),\n",
       " ('scary', 32),\n",
       " ('suffered', 32),\n",
       " ('written', 32),\n",
       " ('disconnected', 32),\n",
       " ('arturo', 32),\n",
       " ('british', 32),\n",
       " ('chemical', 31),\n",
       " ('express', 31),\n",
       " ('josefina', 31),\n",
       " ('tennessee', 31),\n",
       " ('concert', 31),\n",
       " ('smoking', 31),\n",
       " ('strict', 31),\n",
       " ('cents', 31),\n",
       " ('linguistics', 31),\n",
       " ('previous', 31),\n",
       " ('training', 31),\n",
       " ('exact', 31),\n",
       " ('divided', 31),\n",
       " ('contrary', 31),\n",
       " ('forgive', 31),\n",
       " ('nearby', 30),\n",
       " ('tickets', 30),\n",
       " ('dirty', 30),\n",
       " ('sitting', 30),\n",
       " ('guitar', 30),\n",
       " ('santo', 30),\n",
       " ('windows', 30),\n",
       " ('atlantic', 30),\n",
       " ('republic', 30),\n",
       " ('corner', 30),\n",
       " ('someday', 30),\n",
       " ('holiday', 30),\n",
       " ('nephews', 30),\n",
       " ('tonight', 30),\n",
       " ('crimes', 30),\n",
       " ('spirit', 30),\n",
       " ('documents', 30),\n",
       " ('grand', 29),\n",
       " ('grows', 29),\n",
       " ('behave', 29),\n",
       " ('pleasant', 29),\n",
       " ('following', 29),\n",
       " ('bachata', 29),\n",
       " ('sudden', 29),\n",
       " ('argentinean', 29),\n",
       " ('nurse', 29),\n",
       " ('trips', 29),\n",
       " ('mainly', 29),\n",
       " ('developed', 29),\n",
       " ('asleep', 29),\n",
       " ('letter', 29),\n",
       " ('citizenship', 29),\n",
       " ('wednesday', 29),\n",
       " ('siblings', 29),\n",
       " ('piece', 29),\n",
       " ('annie', 28),\n",
       " ('discuss', 28),\n",
       " ('bedrooms', 28),\n",
       " ('norma', 28),\n",
       " ('reaction', 28),\n",
       " ('dances', 28),\n",
       " ('surprise', 28),\n",
       " ('tropical', 28),\n",
       " ('table', 28),\n",
       " ('costa', 28),\n",
       " ('sometime', 28),\n",
       " ('abusing', 28),\n",
       " ('theme', 28),\n",
       " ('fantastic', 28),\n",
       " ('friendship', 28),\n",
       " ('serve', 28),\n",
       " ('durango', 28),\n",
       " ('animal', 28),\n",
       " ('field', 28),\n",
       " ('teenager', 27),\n",
       " ('identify', 27),\n",
       " ('eventually', 27),\n",
       " ('pacific', 27),\n",
       " ('rains', 27),\n",
       " ('accidents', 27),\n",
       " ('satellite', 27),\n",
       " ('pants', 27),\n",
       " ('damaged', 27),\n",
       " ('brain', 27),\n",
       " ('scholarship', 27),\n",
       " ('points', 27),\n",
       " ('arrives', 27),\n",
       " ('taste', 27),\n",
       " ('stories', 26),\n",
       " ('grade', 26),\n",
       " ('karma', 26),\n",
       " ('enjoying', 26),\n",
       " ('window', 26),\n",
       " ('daddy', 26),\n",
       " ('result', 26),\n",
       " ('entertaining', 26),\n",
       " ('balance', 25),\n",
       " ('awful', 25),\n",
       " ('uncomfortable', 25),\n",
       " ('relaxed', 25),\n",
       " ('members', 25),\n",
       " ('meant', 25),\n",
       " ('engineer', 25),\n",
       " ('understands', 25),\n",
       " ('instrument', 25),\n",
       " ('grandfather', 25),\n",
       " ('tuesday', 25),\n",
       " ('controlled', 25),\n",
       " ('origin', 25),\n",
       " ('ramon', 25),\n",
       " ('began', 25),\n",
       " ('appears', 25),\n",
       " ('travelling', 25)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(haha[\"medium\"].items(), reverse=True, key=lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'haha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b62b76a95800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhaha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frequent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'haha' is not defined"
     ]
    }
   ],
   "source": [
    "haha[\"frequent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(haha[\"frequent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hehe = pickle.load(open(\"./fbanks_80dim_nltk/bow_top_100_words_vocab.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1word = {'music': 2191}\n",
    "train_fewwords = {'music': 2191, 'person': 1637, 'religion': 1005, 'school': 675, 'church': 468}\n",
    "train_church = {'church': 468}\n",
    "train_rare_suffer = {'boring': 66}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'music': 2191}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_1word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hehe[\"w2i\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_vocab(words_list):\n",
    "    out = {\"w2i\":{}, \"i2w\":{}, \"freq\":{}}\n",
    "    START_VOCAB = [PAD, GO, EOS, UNK]\n",
    "    for w in START_VOCAB:\n",
    "        out['w2i'][w] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][w] = 1\n",
    "    #for w in words_list['words']:\n",
    "    sorted_w = sorted(words_list.items(), reverse=True, key=lambda t: t[1])\n",
    "    for w in sorted_w:\n",
    "        encoded_word = w[0].encode()\n",
    "        out[\"w2i\"][encoded_word] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][encoded_word] = w[1]\n",
    "\n",
    "    out[\"i2w\"] = {val:key for key, val in out[\"w2i\"].items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'haha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-44c17b73d2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfrequent_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_new_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhaha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frequent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'haha' is not defined"
     ]
    }
   ],
   "source": [
    "frequent_bow = create_new_vocab(haha[\"frequent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(frequent_bow, open(os.path.join(m_cfg['data_path'], \"bow_frequent.en\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_bow = create_new_vocab(haha[\"medium\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(medium_bow, open(os.path.join(m_cfg['data_path'], \"bow_medium.en\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_bow = create_new_vocab(haha[\"rare\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rare_bow, open(os.path.join(m_cfg['data_path'], \"bow_rare.en\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneword_bow = create_new_vocab(train_1word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(oneword_bow, open(os.path.join(m_cfg['data_path'], \"bow_oneword.en\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewwords_bow = create_new_vocab(train_fewwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fewwords_bow, open(os.path.join(m_cfg['data_path'], \"bow_fewwords.en\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "church_bow = create_new_vocab(train_church)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(church_bow, open(os.path.join(m_cfg['data_path'], \"bow_church.en\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffer_bow = create_new_vocab(train_rare_suffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(suffer_bow, open(os.path.join(m_cfg['data_path'], \"bow_suffer.en\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
