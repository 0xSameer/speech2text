{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bow_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def count_match(list1, list2):\n",
    "#     # each list can have repeated elements. The count should account for this.\n",
    "#     count1 = Counter(list1)\n",
    "#     count2 = Counter(list2)\n",
    "#     count2_keys = count2.keys()\n",
    "#     common_w = set(count1.keys()) & set(count2_keys)\n",
    "#     print(common_w)\n",
    "#     matches = sum([min(count1[w], count2[w]) for w in common_w])\n",
    "#     return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_bow_batch(m_dict, x_key, y_key, utt_list, vocab_dict, bow_dict,\n",
    "#                   max_enc, max_dec, input_path=''):\n",
    "#     batch_data = {'X':[], 't':[], 'y':[], 'r':[]}\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     # loop through each utterance in utt list\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     for i, u in enumerate(utt_list):\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         #  add X data\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         if x_key == 'sp':\n",
    "#             # -----------------------------------------------------------------\n",
    "#             # for speech data\n",
    "#             # -----------------------------------------------------------------\n",
    "#             # get path to speech file\n",
    "#             utt_sp_path = os.path.join(input_path, \"{0:s}.npy\".format(u))\n",
    "#             if not os.path.exists(utt_sp_path):\n",
    "#                 # for training data, there are sub-folders\n",
    "#                 utt_sp_path = os.path.join(input_path,\n",
    "#                                            u.split('_',1)[0],\n",
    "#                                            \"{0:s}.npy\".format(u))\n",
    "#             if os.path.exists(utt_sp_path):\n",
    "#                 x_data = xp.load(utt_sp_path)[:max_enc]\n",
    "#             else:\n",
    "#                 # -------------------------------------------------------------\n",
    "#                 # exception if file not found\n",
    "#                 # -------------------------------------------------------------\n",
    "#                 raise FileNotFoundError(\"ERROR!! file not found: {0:s}\".format(utt_sp_path))\n",
    "#                 # -------------------------------------------------------------\n",
    "#         else:\n",
    "#             # -----------------------------------------------------------------\n",
    "#             # for text data\n",
    "#             # -----------------------------------------------------------------\n",
    "#             x_ids = [vocab_dict[x_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][x_key]]\n",
    "#             x_data = xp.asarray(x_ids, dtype=xp.int32)[:max_enc]\n",
    "#             # -----------------------------------------------------------------\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         #  add labels\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         if type(m_dict[u][y_key]) == list:\n",
    "#             en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key]])-set(range(4)))\n",
    "#             r_data = [en_ids[:max_dec]]\n",
    "\n",
    "#         else:\n",
    "#             # dev and test data have multiple translations\n",
    "#             # choose the first one for computing perplexity\n",
    "#             en_ids = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key][0]])-set(range(4)))\n",
    "#             r_data = []\n",
    "#             for r in m_dict[u][y_key]:\n",
    "#                 r_list = list(set([bow_dict['w2i'].get(w, UNK_ID) for w in r])-set(range(4)))\n",
    "#                 r_data.append(r_list[:max_dec])\n",
    "\n",
    "#         y_ids = en_ids[:max_dec]\n",
    "#         # ---------------------------------------------------------------------\n",
    "#         if len(x_data) > 0:\n",
    "#             #  and len(y_ids) > 0\n",
    "#             batch_data['X'].append(x_data)\n",
    "#             batch_data['t'].append([y_ids])\n",
    "#             y_data = xp.zeros(len(bow_dict['w2i']), dtype=xp.int32)\n",
    "#             y_data[y_ids] = 1\n",
    "#             y_data[list(range(4))] = -1\n",
    "#             batch_data['y'].append(y_data)\n",
    "#             batch_data['r'].append(r_data)\n",
    "\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     # end for all utterances in batch\n",
    "#     # -------------------------------------------------------------------------\n",
    "#     if len(batch_data['X']) > 0 and len(batch_data['y']) > 0:\n",
    "#         batch_data['X'] = F.pad_sequence(batch_data['X'], padding=PAD_ID)\n",
    "#         batch_data['y'] = F.pad_sequence(batch_data['y'], padding=PAD_ID)\n",
    "#     return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def basic_precision_recall(r, h, display=False):\n",
    "#     p_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "#     p_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "#     r_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "#     r_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "\n",
    "#     print(\"total utts={0:d}\".format(len(r)))\n",
    "\n",
    "#     i=1\n",
    "\n",
    "#     for references, hypothesis in zip(r, h):\n",
    "#         p_i = modified_precision(references, hypothesis, i)\n",
    "#         p_numerators[i] += p_i.numerator\n",
    "#         p_denominators[i] += p_i.denominator\n",
    "\n",
    "#         tot_match = 0\n",
    "#         tot_count = 0\n",
    "        \n",
    "#         max_recall_match = 0\n",
    "#         max_recall_count = 0\n",
    "#         max_recall = 0\n",
    "        \n",
    "#         for curr_ref in references:\n",
    "#             print(curr_ref, hypothesis)\n",
    "#             curr_match = count_match(curr_ref, hypothesis)\n",
    "#             print(\"curr_match\", curr_match)\n",
    "            \n",
    "#             curr_count = len(curr_ref)\n",
    "#             curr_recall = curr_match / curr_count if curr_count > 0 else 0\n",
    "#             print(curr_match, curr_count, curr_recall)\n",
    "#             if curr_recall > max_recall:\n",
    "#                 max_recall_match = curr_match\n",
    "#                 max_recall_count = curr_count\n",
    "#                 max_recall = curr_recall\n",
    "                \n",
    "#         r_numerators[i] += max_recall_match\n",
    "#         r_denominators[i] += max_recall_count\n",
    "\n",
    "#     prec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(p_numerators.values(), p_denominators.values())]\n",
    "#     rec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(r_numerators.values(), r_denominators.values())]\n",
    "\n",
    "#     if display:\n",
    "#         print(\"{0:10s} | {1:>8s}\".format(\"metric\", \"1-gram\"))\n",
    "#         print(\"-\"*54)\n",
    "#         print(\"{0:10s} | {1:8.2f}\".format(\"precision\", *prec))\n",
    "#         print(\"{0:10s} | {1:8.2f}\".format(\"recall\", *rec))\n",
    "\n",
    "#     return prec[0], rec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfg_path = \"./sp2bagwords/sp_1.0_h-256_e-128_rnn-2_hwy-2_cnn-32-2-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_out_dim = rnn_in_units =  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sameer/anaconda2/envs/chainer3/lib/python3.6/site-packages/chainer/utils/experimental.py:104: FutureWarning: chainer.links.normalization.layer_normalization.py is experimental. The interface can change in the future.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using SGD optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "model found = \n",
      "./sp2bagwords/sp_1.0_h-256_e-128_rnn-2_hwy-2_cnn-32-2-5/seq2seq_35.model\n",
      "finished loading ..\n",
      "optimizer found = ./sp2bagwords/sp_1.0_h-256_e-128_rnn-2_hwy-2_cnn-32-2-5/train.opt\n",
      "finished loading optimizer ...\n"
     ]
    }
   ],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fraction(0, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_precision([[0],[0]], [1], n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total utts=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100.0, 100.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_precision_recall([[[2],[1]]], [[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./sp2bagwords/sp_1.0_h-256_e-128_rnn-2_hwy-2_cnn-32-2-5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_cfg['model_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict, bow_dict = get_data_dicts(m_cfg)\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\")):\n",
    "    dev_utts = pickle.load(open(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\"), \"rb\"))\n",
    "\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"mean_pos_scores.dict\")):\n",
    "    mean_pos_scores = pickle.load(open(os.path.join(m_cfg['model_dir'], \"mean_pos_scores.dict\"), \"rb\"))\n",
    "    mean_neg_scores = pickle.load(open(os.path.join(m_cfg['model_dir'], \"mean_neg_scores.dict\"), \"rb\"))\n",
    "# batch_size = {'max': 128, 'med': 128, 'min': 128, 'scale': 1}\n",
    "batch_size = {'max': 64, 'med': 64, 'min': 64, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer.hyperparam.lr = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.34183857,  0.20332989,  0.16533113,  0.13396151,  0.12785606,\n",
       "         0.17088211],\n",
       "       [ 0.14858542,  0.08471115,  0.07898171,  0.07449739,  0.05836316,\n",
       "         0.06727904]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.vstack((mean_pos_scores, mean_neg_scores))[:,4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(0.04837069660425186, dtype=float32),\n",
       " array(0.019010351970791817, dtype=float32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.mean(mean_pos_scores), xp.mean(mean_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(0.05061212554574013, dtype=float32),\n",
       " array(0.020977046340703964, dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.std(mean_pos_scores), xp.std(mean_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b=30,l=0.09,avg=0.09: 100%|█████████████████| 1305/1305 [01:48<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08796076363981215\n",
      "using threshold of 0.1\n",
      "total utts=1305\n",
      "(19.01918976545842, 56.74300254452926)\n",
      "using mean positive prediction threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/1305 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total utts=1305\n",
      "(8.627168785480132, 65.47330097087378)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b=116,l=0.09,avg=0.09: 100%|████████████████| 1305/1305 [01:49<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08703548703412178\n",
      "using threshold of 0.1\n",
      "total utts=1305\n",
      "(19.43500424808836, 57.1875)\n",
      "using mean positive prediction threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/1305 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total utts=1305\n",
      "(8.214884466797871, 66.2551440329218)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b=82,l=0.14,avg=0.09: 100%|█████████████████| 1305/1305 [01:47<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08594100912963387\n",
      "using threshold of 0.1\n",
      "total utts=1305\n",
      "(19.41978783286426, 57.46316463805253)\n",
      "using mean positive prediction threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b=7,l=0.03,avg=0.03:   2%|▎                  | 25/1305 [00:00<00:07, 169.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total utts=1305\n",
      "(8.272978421563504, 67.31470230862698)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b=122,l=0.12,avg=0.08: 100%|████████████████| 1305/1305 [01:47<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08465433786553678\n",
      "using threshold of 0.1\n",
      "total utts=1305\n",
      "(19.953742640874687, 56.42092746730083)\n",
      "using mean positive prediction threshold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/1305 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total utts=1305\n",
      "(9.127631476734251, 67.80150028851702)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b=61,l=0.10,avg=0.08: 100%|█████████████████| 1305/1305 [01:48<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08406977717583972\n",
      "using threshold of 0.1\n",
      "total utts=1305\n",
      "(19.453781512605044, 59.28297055057619)\n",
      "using mean positive prediction threshold\n",
      "total utts=1305\n",
      "(8.573497465604635, 68.32083092902481)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    train_utts, train_loss = feed_model(model,\n",
    "                                optimizer=optimizer,\n",
    "                                m_dict=map_dict[m_cfg['train_set']],\n",
    "                                b_dict=bucket_dict[m_cfg['train_set']],\n",
    "                                vocab_dict=vocab_dict,\n",
    "                                bow_dict=bow_dict,\n",
    "                                batch_size=batch_size,\n",
    "                                x_key=enc_key,\n",
    "                                y_key=dec_key,\n",
    "                                train=True,\n",
    "                                input_path=os.path.join(m_cfg['data_path'], m_cfg['train_set']),\n",
    "                                max_dec=m_cfg['max_en_pred'],\n",
    "                                t_cfg=t_cfg,\n",
    "                                use_y=True)\n",
    "    print(train_loss)\n",
    "    print(\"using threshold of 0.1\")\n",
    "    train_pred_words = get_pred_words_from_probs(train_utts[\"probs\"],\n",
    "                                           0.1,\n",
    "                                           m_cfg['max_en_pred'])\n",
    "    print(basic_precision_recall(train_utts[\"refs\"], train_pred_words))\n",
    "    print(\"using mean positive prediction threshold\")\n",
    "    train_pred_words = get_pred_words_from_probs(train_utts[\"probs\"],\n",
    "                                               mean_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "    print(basic_precision_recall(train_utts[\"refs\"], train_pred_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_fil = m_cfg['model_fname']\n",
    "# print(\"Saving model\")\n",
    "# serializers.save_npz(model_fil.replace(\".model\", \"_{0:d}.model\".format(20)), model)\n",
    "# print(\"Finished saving model\")\n",
    "# print(\"Saving optimizer\")\n",
    "# serializers.save_npz(m_cfg['opt_fname'], optimizer)\n",
    "# print(\"Finished saving optimizer\")\n",
    "# print(\"Saving utterance predictions\")\n",
    "\n",
    "# pickle.dump(utts, open(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\"), \"wb\"))\n",
    "# pickle.dump(mean_pos_scores, open(os.path.join(m_cfg['model_dir'], \"mean_pos_scores.dict\"), \"wb\"))\n",
    "# pickle.dump(mean_neg_scores, open(os.path.join(m_cfg['model_dir'], \"mean_neg_scores.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.59993172, -2.21785092], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_utts[\"probs\"][:2,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.06181449, -0.40225312,  0.15357459,  0.31289765, -2.59993172,\n",
       "         -1.86838281, -1.92051077, -1.57338321, -2.54046893, -3.31746721],\n",
       "        [ 0.08252125, -0.15521246, -0.17448367, -0.22369032, -2.21785092,\n",
       "         -2.01475859, -1.14290965, -2.17804217, -3.90053344, -5.28072119]], dtype=float32),\n",
       " [[[66, 51, 85, 55]], [[41, 27, 23]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_utts[\"probs\"][:2,:10], train_utts[\"refs\"][:2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_pos_scores = xp.array([0.0 for i in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "mean_neg_scores = xp.array([0.0 for i in bow_dict[\"i2w\"]], dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'yes' 35054\n",
      "0.32988789677619934 (251,)\n",
      "0.12474673986434937 (1054,)\n",
      "b'like' 14334\n",
      "0.2018955796957016 (98,)\n",
      "0.08374379575252533 (1207,)\n",
      "b'well' 12354\n",
      "0.1673634946346283 (90,)\n",
      "0.08193064481019974 (1215,)\n",
      "b'know' 10619\n",
      "0.14862149953842163 (89,)\n",
      "0.08002760261297226 (1216,)\n",
      "b'ah' 9590\n",
      "0.11550240218639374 (82,)\n",
      "0.054315224289894104 (1223,)\n",
      "b'oh' 9183\n",
      "0.14873015880584717 (91,)\n",
      "0.05575650930404663 (1214,)\n",
      "b'eh' 7164\n",
      "0.13792714476585388 (62,)\n",
      "0.048028115183115005 (1243,)\n",
      "b'think' 6677\n",
      "0.10452914983034134 (49,)\n",
      "0.03986036032438278 (1256,)\n",
      "b'people' 6560\n",
      "0.14330370724201202 (60,)\n",
      "0.05451585352420807 (1245,)\n",
      "b'right' 6505\n",
      "0.043389104306697845 (45,)\n",
      "0.031220421195030212 (1260,)\n",
      "b'one' 6340\n",
      "0.10720238834619522 (62,)\n",
      "0.042309608310461044 (1243,)\n",
      "b'aha' 6291\n",
      "0.13751831650733948 (60,)\n",
      "0.03354032710194588 (1245,)\n",
      "b'uh' 5595\n",
      "0.05088953301310539 (35,)\n",
      "0.029370009899139404 (1270,)\n",
      "b'sure' 4842\n",
      "0.07231127470731735 (48,)\n",
      "0.033011987805366516 (1257,)\n",
      "b'say' 4652\n",
      "0.13842333853244781 (44,)\n",
      "0.0487004779279232 (1261,)\n",
      "b'mm' 4639\n",
      "0.10060326009988785 (31,)\n",
      "0.02213430218398571 (1274,)\n",
      "b'good' 4628\n",
      "0.06826526671648026 (43,)\n",
      "0.040688712149858475 (1262,)\n",
      "b'yeah' 4283\n",
      "0.04684169590473175 (36,)\n",
      "0.032725170254707336 (1269,)\n",
      "b'really' 4241\n",
      "0.023580612614750862 (24,)\n",
      "0.01477877702564001 (1281,)\n",
      "b'lot' 3986\n",
      "0.057007595896720886 (31,)\n",
      "0.029468625783920288 (1274,)\n",
      "b'also' 3927\n",
      "0.08824498951435089 (29,)\n",
      "0.025189843028783798 (1276,)\n",
      "b'go' 3901\n",
      "0.08654986321926117 (28,)\n",
      "0.032950058579444885 (1277,)\n",
      "b'see' 3702\n",
      "0.08986271172761917 (25,)\n",
      "0.02366618812084198 (1280,)\n",
      "b'many' 3518\n",
      "0.09239834547042847 (35,)\n",
      "0.03679598122835159 (1270,)\n",
      "b'mean' 3177\n",
      "0.05839257687330246 (25,)\n",
      "0.028050497174263 (1280,)\n",
      "b'things' 3058\n",
      "0.06981927156448364 (20,)\n",
      "0.02612888067960739 (1285,)\n",
      "b'something' 3009\n",
      "0.08266514539718628 (23,)\n",
      "0.022804033011198044 (1282,)\n",
      "b'get' 2855\n",
      "0.058836083859205246 (28,)\n",
      "0.027038583531975746 (1277,)\n",
      "b'going' 2842\n",
      "0.050279706716537476 (24,)\n",
      "0.0232061967253685 (1281,)\n",
      "b'time' 2841\n",
      "0.057894267141819 (37,)\n",
      "0.02811868116259575 (1268,)\n",
      "b'would' 2802\n",
      "0.062128644436597824 (19,)\n",
      "0.02252786234021187 (1286,)\n",
      "b'years' 2574\n",
      "0.023727504536509514 (15,)\n",
      "0.011815877631306648 (1290,)\n",
      "b'want' 2442\n",
      "0.05891424044966698 (25,)\n",
      "0.02264205552637577 (1280,)\n",
      "b'much' 2440\n",
      "0.07090042531490326 (29,)\n",
      "0.025248320773243904 (1276,)\n",
      "b'hmm' 2332\n",
      "0.03577870875597 (20,)\n",
      "0.008215735666453838 (1285,)\n",
      "b'okay' 2246\n",
      "0.017741894349455833 (14,)\n",
      "0.008307457901537418 (1291,)\n",
      "b'two' 2208\n",
      "0.023787319660186768 (15,)\n",
      "0.010291346348822117 (1290,)\n",
      "b'music' 2190\n",
      "0.04091176390647888 (20,)\n",
      "0.01741836778819561 (1285,)\n",
      "b'little' 2174\n",
      "0.03895500674843788 (20,)\n",
      "0.01667899638414383 (1285,)\n",
      "b'look' 2145\n",
      "0.016911648213863373 (12,)\n",
      "0.008963131345808506 (1293,)\n",
      "b'live' 2128\n",
      "0.03091040998697281 (19,)\n",
      "0.014756675809621811 (1286,)\n",
      "b'exactly' 2094\n",
      "0.008363893255591393 (15,)\n",
      "0.007485545706003904 (1290,)\n",
      "b'always' 2016\n",
      "0.026368718594312668 (13,)\n",
      "0.01278598140925169 (1292,)\n",
      "b'um' 1952\n",
      "0.03642695024609566 (18,)\n",
      "0.014792029745876789 (1287,)\n",
      "b'already' 1941\n",
      "0.017668312415480614 (12,)\n",
      "0.007313420996069908 (1293,)\n",
      "b'work' 1929\n",
      "0.017200540751218796 (16,)\n",
      "0.010574853979051113 (1289,)\n",
      "b'everything' 1863\n",
      "0.05118345469236374 (8,)\n",
      "0.008541754446923733 (1297,)\n",
      "b'example' 1825\n",
      "0.04379287734627724 (13,)\n",
      "0.013246744871139526 (1292,)\n",
      "b'thing' 1799\n",
      "0.05061730369925499 (16,)\n",
      "0.012646119110286236 (1289,)\n",
      "b'new' 1767\n",
      "0.036600448191165924 (14,)\n",
      "0.015026609413325787 (1291,)\n",
      "b'states' 1676\n",
      "0.043188948184251785 (17,)\n",
      "0.014194704592227936 (1288,)\n",
      "b'tell' 1672\n",
      "0.03158298507332802 (18,)\n",
      "0.013380581513047218 (1287,)\n",
      "b'even' 1665\n",
      "0.07534589618444443 (20,)\n",
      "0.021196609362959862 (1285,)\n",
      "b'person' 1661\n",
      "0.07463160902261734 (15,)\n",
      "0.020202383399009705 (1290,)\n",
      "b'mhm' 1645\n",
      "0.03832358121871948 (7,)\n",
      "0.0036149444058537483 (1298,)\n",
      "b'united' 1612\n",
      "0.036348577588796616 (16,)\n",
      "0.013253243640065193 (1289,)\n",
      "b'nice' 1586\n",
      "0.0124178696423769 (13,)\n",
      "0.007199015934020281 (1292,)\n",
      "b'sometimes' 1559\n",
      "0.060227759182453156 (8,)\n",
      "0.009331943467259407 (1297,)\n",
      "b'said' 1550\n",
      "0.0822213664650917 (12,)\n",
      "0.016903474926948547 (1293,)\n",
      "b'never' 1539\n",
      "0.027510207146406174 (7,)\n",
      "0.00850861519575119 (1298,)\n",
      "b'call' 1501\n",
      "0.028767969459295273 (16,)\n",
      "0.012682205066084862 (1289,)\n",
      "b'true' 1484\n",
      "0.04262369126081467 (8,)\n",
      "0.012960276566445827 (1297,)\n",
      "b'let' 1479\n",
      "0.059262715280056 (16,)\n",
      "0.01619618572294712 (1289,)\n",
      "b'different' 1473\n",
      "0.011594068259000778 (8,)\n",
      "0.005840265657752752 (1297,)\n",
      "b'country' 1462\n",
      "0.03956180438399315 (16,)\n",
      "0.015101170167326927 (1289,)\n",
      "b'way' 1459\n",
      "0.02414901927113533 (10,)\n",
      "0.009984809905290604 (1295,)\n",
      "b'understand' 1436\n",
      "0.02010919526219368 (13,)\n",
      "0.010854765772819519 (1292,)\n",
      "b'course' 1432\n",
      "0.013556218706071377 (9,)\n",
      "0.0063714273273944855 (1296,)\n",
      "b'ca' 1423\n",
      "0.011102297343313694 (8,)\n",
      "0.00506993243470788 (1297,)\n",
      "b'hello' 1412\n",
      "0.017530646175146103 (16,)\n",
      "0.009531138464808464 (1289,)\n",
      "b'talk' 1395\n",
      "0.009523548185825348 (5,)\n",
      "0.0032262068707495928 (1300,)\n",
      "b'money' 1343\n",
      "0.031034715473651886 (15,)\n",
      "0.010194503702223301 (1290,)\n",
      "b'another' 1314\n",
      "0.04022328928112984 (14,)\n",
      "0.014588777907192707 (1291,)\n",
      "b'since' 1269\n",
      "0.030999112874269485 (10,)\n",
      "0.011405506171286106 (1295,)\n",
      "b'day' 1262\n",
      "0.030068030580878258 (13,)\n",
      "0.010301702655851841 (1292,)\n",
      "b'three' 1247\n",
      "0.028377145528793335 (10,)\n",
      "0.008450902067124844 (1295,)\n",
      "b'house' 1247\n",
      "0.026560775935649872 (13,)\n",
      "0.008830740116536617 (1292,)\n",
      "b'come' 1242\n",
      "0.010716026648879051 (7,)\n",
      "0.004539621062576771 (1298,)\n",
      "b'york' 1194\n",
      "0.010964254848659039 (9,)\n",
      "0.0049475617706775665 (1296,)\n",
      "b'city' 1179\n",
      "0.007107052020728588 (5,)\n",
      "0.0030134774278849363 (1300,)\n",
      "b'could' 1170\n",
      "0.02262197434902191 (8,)\n",
      "0.0095556965097785 (1297,)\n",
      "b'god' 1157\n",
      "0.026302311569452286 (14,)\n",
      "0.011211423203349113 (1291,)\n",
      "b'bad' 1153\n",
      "0.012694336473941803 (5,)\n",
      "0.0039967698976397514 (1300,)\n",
      "b'went' 1153\n",
      "0.036766599863767624 (10,)\n",
      "0.011244396679103374 (1295,)\n",
      "b'take' 1153\n",
      "0.024076100438833237 (13,)\n",
      "0.010362665168941021 (1292,)\n",
      "b'told' 1150\n",
      "0.002236235188320279 (1,)\n",
      "0.001566226128488779 (1304,)\n",
      "b'anything' 1149\n",
      "0.0151488883420825 (9,)\n",
      "0.006025901064276695 (1296,)\n",
      "b'name' 1122\n",
      "0.01766958087682724 (8,)\n",
      "0.006671759765595198 (1297,)\n",
      "b'year' 1114\n",
      "0.036422643810510635 (11,)\n",
      "0.011938827112317085 (1294,)\n",
      "b'give' 1103\n",
      "0.024605650454759598 (8,)\n",
      "0.008473226800560951 (1297,)\n",
      "b'long' 1085\n",
      "0.01152742374688387 (9,)\n",
      "0.006385623011738062 (1296,)\n",
      "b'spanish' 1080\n",
      "0.037887632846832275 (6,)\n",
      "0.006914718076586723 (1299,)\n",
      "b'someone' 1075\n",
      "0.017315000295639038 (8,)\n",
      "0.00581287732347846 (1297,)\n",
      "b'old' 1073\n",
      "0.005860029719769955 (5,)\n",
      "0.003733148565515876 (1300,)\n",
      "b'believe' 1073\n",
      "0.005586783401668072 (5,)\n",
      "0.004506423603743315 (1300,)\n",
      "b'five' 1062\n",
      "0.008694554679095745 (9,)\n",
      "0.00543363019824028 (1296,)\n",
      "b'got' 1054\n",
      "0.12298906594514847 (8,)\n",
      "0.009107254445552826 (1297,)\n",
      "b'kids' 1030\n",
      "0.02504446916282177 (9,)\n",
      "0.007962331175804138 (1296,)\n",
      "b'find' 1030\n",
      "0.031114740297198296 (9,)\n",
      "0.011440504342317581 (1296,)\n",
      "b'make' 1021\n",
      "0.009910216554999352 (8,)\n",
      "0.00564992893487215 (1297,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, len(bow_dict[\"i2w\"])):\n",
    "    this_word = bow_dict[\"i2w\"][i]\n",
    "    print(this_word, bow_dict[\"freq\"][this_word])\n",
    "    pos_indx = [i in r[0] for r in train_utts[\"refs\"]]\n",
    "    neg_indx = [i not in r[0] for r in train_utts[\"refs\"]]\n",
    "    mean_pos_scores[i] = np.mean(F.sigmoid(train_utts[\"probs\"][:,i][pos_indx]).data)\n",
    "    mean_neg_scores[i] = np.mean(F.sigmoid(train_utts[\"probs\"][:,i][neg_indx]).data)\n",
    "    print(mean_pos_scores[i], train_utts[\"probs\"][:,i][pos_indx].shape)\n",
    "    print(mean_neg_scores[i], train_utts[\"probs\"][:,i][neg_indx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.12474674,\n",
       "         0.0837438 ,  0.08193064,  0.0800276 ,  0.05431522,  0.05575651,\n",
       "         0.04802812,  0.03986036,  0.05451585,  0.03122042,  0.04230961,\n",
       "         0.03354033,  0.02937001,  0.03301199,  0.04870048,  0.0221343 ,\n",
       "         0.04068871,  0.03272517,  0.01477878,  0.02946863,  0.02518984,\n",
       "         0.03295006,  0.02366619,  0.03679598,  0.0280505 ,  0.02612888,\n",
       "         0.02280403,  0.02703858,  0.0232062 ,  0.02811868,  0.02252786,\n",
       "         0.01181588,  0.02264206,  0.02524832,  0.00821574,  0.00830746,\n",
       "         0.01029135,  0.01741837,  0.016679  ,  0.00896313,  0.01475668,\n",
       "         0.00748555,  0.01278598,  0.01479203,  0.00731342,  0.01057485,\n",
       "         0.00854175,  0.01324674,  0.01264612,  0.01502661,  0.0141947 ,\n",
       "         0.01338058,  0.02119661,  0.02020238,  0.00361494,  0.01325324,\n",
       "         0.00719902,  0.00933194,  0.01690347,  0.00850862,  0.01268221,\n",
       "         0.01296028,  0.01619619,  0.00584027,  0.01510117,  0.00998481,\n",
       "         0.01085477,  0.00637143,  0.00506993,  0.00953114,  0.00322621,\n",
       "         0.0101945 ,  0.01458878,  0.01140551,  0.0103017 ,  0.0084509 ,\n",
       "         0.00883074,  0.00453962,  0.00494756,  0.00301348,  0.0095557 ,\n",
       "         0.01121142,  0.00399677,  0.0112444 ,  0.01036267,  0.00156623,\n",
       "         0.0060259 ,  0.00667176,  0.01193883,  0.00847323,  0.00638562,\n",
       "         0.00691472,  0.00581288,  0.00373315,  0.00450642,  0.00543363,\n",
       "         0.00910725,  0.00796233,  0.0114405 ,  0.00564993], dtype=float32),\n",
       " array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.3298879 ,\n",
       "         0.20189558,  0.16736349,  0.1486215 ,  0.1155024 ,  0.14873016,\n",
       "         0.13792714,  0.10452915,  0.14330371,  0.0433891 ,  0.10720239,\n",
       "         0.13751832,  0.05088953,  0.07231127,  0.13842334,  0.10060326,\n",
       "         0.06826527,  0.0468417 ,  0.02358061,  0.0570076 ,  0.08824499,\n",
       "         0.08654986,  0.08986271,  0.09239835,  0.05839258,  0.06981927,\n",
       "         0.08266515,  0.05883608,  0.05027971,  0.05789427,  0.06212864,\n",
       "         0.0237275 ,  0.05891424,  0.07090043,  0.03577871,  0.01774189,\n",
       "         0.02378732,  0.04091176,  0.03895501,  0.01691165,  0.03091041,\n",
       "         0.00836389,  0.02636872,  0.03642695,  0.01766831,  0.01720054,\n",
       "         0.05118345,  0.04379288,  0.0506173 ,  0.03660045,  0.04318895,\n",
       "         0.03158299,  0.0753459 ,  0.07463161,  0.03832358,  0.03634858,\n",
       "         0.01241787,  0.06022776,  0.08222137,  0.02751021,  0.02876797,\n",
       "         0.04262369,  0.05926272,  0.01159407,  0.0395618 ,  0.02414902,\n",
       "         0.0201092 ,  0.01355622,  0.0111023 ,  0.01753065,  0.00952355,\n",
       "         0.03103472,  0.04022329,  0.03099911,  0.03006803,  0.02837715,\n",
       "         0.02656078,  0.01071603,  0.01096425,  0.00710705,  0.02262197,\n",
       "         0.02630231,  0.01269434,  0.0367666 ,  0.0240761 ,  0.00223624,\n",
       "         0.01514889,  0.01766958,  0.03642264,  0.02460565,  0.01152742,\n",
       "         0.03788763,  0.017315  ,  0.00586003,  0.00558678,  0.00869455,\n",
       "         0.12298907,  0.02504447,  0.03111474,  0.00991022], dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_neg_scores, mean_pos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'yes' 35054\n",
      "0.32988789677619934 (251,)\n",
      "0.12474673986434937 (1054,)\n"
     ]
    }
   ],
   "source": [
    "check_index = 4\n",
    "this_word = bow_dict[\"i2w\"][check_index]\n",
    "print(this_word, bow_dict[\"freq\"][this_word])\n",
    "pos_indx = [check_index in r[0] for r in train_utts[\"refs\"]]\n",
    "neg_indx = [check_index not in r[0] for r in train_utts[\"refs\"]]\n",
    "print(np.mean(F.sigmoid(train_utts[\"probs\"][:,check_index][pos_indx]).data), train_utts[\"probs\"][:,check_index][pos_indx].shape)\n",
    "print(np.mean(F.sigmoid(train_utts[\"probs\"][:,check_index][neg_indx]).data), train_utts[\"probs\"][:,check_index][neg_indx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(False, dtype=bool), False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp.all(train_utts[\"probs\"][:10] == train_utts[\"probs\"][10:20]), train_utts[\"preds\"][:10] == train_utts[\"preds\"][10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total utts=1305\n",
      "4.954268292682927 58.03571428571429\n"
     ]
    }
   ],
   "source": [
    "train_prec, train_rec = basic_precision_recall(train_utts[\"refs\"], train_utts[\"preds\"])\n",
    "print(train_prec, train_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51544869  0.40077111  0.53831834  0.57759237  0.06914282  0.13372895\n",
      "  0.12780462  0.17173462  0.07306941  0.0349768   0.11242215  0.03428413\n",
      "  0.19470397  0.01632826  0.07281633  0.00395539  0.0192567   0.01851128\n",
      "  0.07345533  0.00187328  0.02366112  0.02286559  0.03503358  0.0583662\n",
      "  0.02522211  0.08731006  0.03314728  0.15134153  0.05322033  0.08109252\n",
      "  0.05765366  0.10448772  0.03016962  0.03833305  0.05536424  0.02219497\n",
      "  0.04001034  0.08084626  0.00168835  0.00555753  0.01220204  0.03535946\n",
      "  0.04005387  0.01693087  0.02857115  0.01161144  0.02318759  0.00666831\n",
      "  0.01684841  0.01795754  0.01862131  0.04898268  0.0231012   0.01412547\n",
      "  0.04483757  0.03984403  0.02511185  0.02505676  0.000625    0.0297839\n",
      "  0.00900214  0.01389109  0.03104129  0.008212    0.02086379  0.01783746\n",
      "  0.04733364  0.01644694  0.03997594  0.03147162  0.03201365  0.00521437\n",
      "  0.01047211  0.00311684  0.00436865  0.01912993  0.0278133   0.02110873\n",
      "  0.01631512  0.02018163  0.01519018  0.00709303  0.0072712   0.00668228\n",
      "  0.01866021  0.02470019  0.00727811  0.01436733  0.02177863  0.00365427\n",
      "  0.00722803  0.01166455  0.01247929  0.01622083  0.0091634   0.00589579\n",
      "  0.01092333  0.00787478  0.00950551  0.00968334  0.00599611  0.01720218\n",
      "  0.02030555  0.01344758]\n",
      "[ 0.          0.          0.          0.          0.3298879   0.20189558\n",
      "  0.16736349  0.1486215   0.1155024   0.14873016  0.13792714  0.10452915\n",
      "  0.14330371  0.0433891   0.10720239  0.13751832  0.05088953  0.07231127\n",
      "  0.13842334  0.10060326  0.06826527  0.0468417   0.02358061  0.0570076\n",
      "  0.08824499  0.08654986  0.08986271  0.09239835  0.05839258  0.06981927\n",
      "  0.08266515  0.05883608  0.05027971  0.05789427  0.06212864  0.0237275\n",
      "  0.05891424  0.07090043  0.03577871  0.01774189  0.02378732  0.04091176\n",
      "  0.03895501  0.01691165  0.03091041  0.00836389  0.02636872  0.03642695\n",
      "  0.01766831  0.01720054  0.05118345  0.04379288  0.0506173   0.03660045\n",
      "  0.04318895  0.03158299  0.0753459   0.07463161  0.03832358  0.03634858\n",
      "  0.01241787  0.06022776  0.08222137  0.02751021  0.02876797  0.04262369\n",
      "  0.05926272  0.01159407  0.0395618   0.02414902  0.0201092   0.01355622\n",
      "  0.0111023   0.01753065  0.00952355  0.03103472  0.04022329  0.03099911\n",
      "  0.03006803  0.02837715  0.02656078  0.01071603  0.01096425  0.00710705\n",
      "  0.02262197  0.02630231  0.01269434  0.0367666   0.0240761   0.00223624\n",
      "  0.01514889  0.01766958  0.03642264  0.02460565  0.01152742  0.03788763\n",
      "  0.017315    0.00586003  0.00558678  0.00869455  0.12298907  0.02504447\n",
      "  0.03111474  0.00991022]\n",
      "[ 0  1  2  3  5  6  7 10 12 27 31]\n",
      "total utts=1305\n",
      "0.0 0\n"
     ]
    }
   ],
   "source": [
    "PRED_THRESH = 0.1\n",
    "pred_words = []\n",
    "pred_limit = m_cfg['max_en_pred']\n",
    "for row in F.sigmoid(train_utts[\"probs\"]).data:\n",
    "    pred_inds = xp.where(row >=PRED_THRESH)[0]\n",
    "    if len(pred_inds) > pred_limit:\n",
    "        pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    pred_words.append([i for i in pred_inds.tolist() if i > 3])\n",
    "    print(row)\n",
    "    print(mean_pos_scores)\n",
    "    print(pred_inds)\n",
    "    break\n",
    "\n",
    "train_prec, train_rec = basic_precision_recall(train_utts[\"refs\"], pred_words)\n",
    "print(train_prec, train_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51544869  0.40077111  0.53831834  0.57759237  0.06914282  0.13372895\n",
      "  0.12780462  0.17173462  0.07306941  0.0349768   0.11242215  0.03428413\n",
      "  0.19470397  0.01632826  0.07281633  0.00395539  0.0192567   0.01851128\n",
      "  0.07345533  0.00187328  0.02366112  0.02286559  0.03503358  0.0583662\n",
      "  0.02522211  0.08731006  0.03314728  0.15134153  0.05322033  0.08109252\n",
      "  0.05765366  0.10448772  0.03016962  0.03833305  0.05536424  0.02219497\n",
      "  0.04001034  0.08084626  0.00168835  0.00555753  0.01220204  0.03535946\n",
      "  0.04005387  0.01693087  0.02857115  0.01161144  0.02318759  0.00666831\n",
      "  0.01684841  0.01795754  0.01862131  0.04898268  0.0231012   0.01412547\n",
      "  0.04483757  0.03984403  0.02511185  0.02505676  0.000625    0.0297839\n",
      "  0.00900214  0.01389109  0.03104129  0.008212    0.02086379  0.01783746\n",
      "  0.04733364  0.01644694  0.03997594  0.03147162  0.03201365  0.00521437\n",
      "  0.01047211  0.00311684  0.00436865  0.01912993  0.0278133   0.02110873\n",
      "  0.01631512  0.02018163  0.01519018  0.00709303  0.0072712   0.00668228\n",
      "  0.01866021  0.02470019  0.00727811  0.01436733  0.02177863  0.00365427\n",
      "  0.00722803  0.01166455  0.01247929  0.01622083  0.0091634   0.00589579\n",
      "  0.01092333  0.00787478  0.00950551  0.00968334  0.00599611  0.01720218\n",
      "  0.02030555  0.01344758]\n",
      "[ 0.          0.          0.          0.          0.3298879   0.20189558\n",
      "  0.16736349  0.1486215   0.1155024   0.14873016  0.13792714  0.10452915\n",
      "  0.14330371  0.0433891   0.10720239  0.13751832  0.05088953  0.07231127\n",
      "  0.13842334  0.10060326  0.06826527  0.0468417   0.02358061  0.0570076\n",
      "  0.08824499  0.08654986  0.08986271  0.09239835  0.05839258  0.06981927\n",
      "  0.08266515  0.05883608  0.05027971  0.05789427  0.06212864  0.0237275\n",
      "  0.05891424  0.07090043  0.03577871  0.01774189  0.02378732  0.04091176\n",
      "  0.03895501  0.01691165  0.03091041  0.00836389  0.02636872  0.03642695\n",
      "  0.01766831  0.01720054  0.05118345  0.04379288  0.0506173   0.03660045\n",
      "  0.04318895  0.03158299  0.0753459   0.07463161  0.03832358  0.03634858\n",
      "  0.01241787  0.06022776  0.08222137  0.02751021  0.02876797  0.04262369\n",
      "  0.05926272  0.01159407  0.0395618   0.02414902  0.0201092   0.01355622\n",
      "  0.0111023   0.01753065  0.00952355  0.03103472  0.04022329  0.03099911\n",
      "  0.03006803  0.02837715  0.02656078  0.01071603  0.01096425  0.00710705\n",
      "  0.02262197  0.02630231  0.01269434  0.0367666   0.0240761   0.00223624\n",
      "  0.01514889  0.01766958  0.03642264  0.02460565  0.01152742  0.03788763\n",
      "  0.017315    0.00586003  0.00558678  0.00869455  0.12298907  0.02504447\n",
      "  0.03111474  0.00991022]\n",
      "[  0   1   2   3   7  12  22  23  25  27  29  31  37  42  43  45  49  51\n",
      "  54  55  67  68  69  70  89  97  98  99 103]\n",
      "total utts=1305\n",
      "8.0 50.0\n"
     ]
    }
   ],
   "source": [
    "pred_words = []\n",
    "pred_limit = m_cfg['max_en_pred']\n",
    "for row in F.sigmoid(train_utts[\"probs\"]).data:\n",
    "    pred_inds = xp.where(row >= mean_pos_scores)[0]\n",
    "    if len(pred_inds) > pred_limit:\n",
    "        pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    pred_words.append([i for i in pred_inds.tolist() if i > 3])\n",
    "    print(row)\n",
    "    print(mean_pos_scores)\n",
    "    print(pred_inds)\n",
    "    break\n",
    "\n",
    "train_prec, train_rec = basic_precision_recall(train_utts[\"refs\"], pred_words)\n",
    "print(train_prec, train_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_words[:5], train_utts[\"refs\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum([1 if (check_index in p) else 0 for p in pred_words]), len(pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utts, dev_loss = feed_model(model,\n",
    "                            optimizer=optimizer,\n",
    "                            m_dict=map_dict[dev_key],\n",
    "                            b_dict=bucket_dict[dev_key],\n",
    "                            vocab_dict=vocab_dict,\n",
    "                            bow_dict=bow_dict,\n",
    "                            batch_size=batch_size,\n",
    "                            x_key=enc_key,\n",
    "                            y_key=dec_key,\n",
    "                            train=False,\n",
    "                            input_path=input_path,\n",
    "                            max_dec=m_cfg['max_en_pred'],\n",
    "                            t_cfg=t_cfg,\n",
    "                            use_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.1\n",
    "dev_pred_words = []\n",
    "pred_limit = m_cfg['max_en_pred']\n",
    "for row in F.sigmoid(utts[\"probs\"]).data:\n",
    "    dev_pred_inds = xp.where(row >= PRED_THRESH)[0]\n",
    "    if len(dev_pred_inds) > pred_limit:\n",
    "        dev_pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    dev_pred_words.append([i for i in dev_pred_inds.tolist() if i > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_precision_recall(utts[\"refs\"], dev_pred_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           0.1,\n",
    "                                           m_cfg['max_en_pred'])\n",
    "print(basic_precision_recall(dev_utts[\"refs\"], dev_pred_words))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           mean_pos_scores,\n",
    "                                           m_cfg['max_en_pred'])\n",
    "print(basic_precision_recall(dev_utts[\"refs\"], dev_pred_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_pred_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2a50e467a0e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdev_prec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasic_precision_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_utts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"refs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_pred_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_pred_words' is not defined"
     ]
    }
   ],
   "source": [
    "dev_prec, dev_rec = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_prec, dev_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_words = []\n",
    "pred_limit = m_cfg['max_en_pred']\n",
    "for row in F.sigmoid(train_utts[\"probs\"]).data:\n",
    "    pred_inds = xp.where(row >= mean_pos_scores)[0]\n",
    "    if len(pred_inds) > pred_limit:\n",
    "        pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    pred_words.append([i for i in pred_inds.tolist() if i > 3])\n",
    "\n",
    "train_prec, train_rec = basic_precision_recall(train_utts[\"refs\"], pred_words)\n",
    "print(train_prec, train_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp.all(utts[\"probs\"][:10] == utts[\"probs\"][10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utts[\"probs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.sigmoid(utts[\"probs\"][:2,100:115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utts[\"probs\"].T[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utts[\"refs\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len([p for p in utts[\"preds\"] if len(p) > 0]), len([p for p in utts[\"refs\"] if len(p) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_precision_recall(utts[\"refs\"], utts[\"preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.binary_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_precision_recall([[[]], [[]], [[]], [[1]]], [[1], [], [], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utts[\"refs\"][:2], utts[\"preds\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\".join([vocab_dict['en_c']['i2w'][i].decode() for i in pred_sents[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, 200\n",
    "# min_len, max_len = 0, 1\n",
    "displayN = 50\n",
    "m_dict=map_dict[dev_key]\n",
    "# wavs_path = os.path.join(m_cfg['data_path'], \"wavs\")\n",
    "wavs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"wavs\")\n",
    "v_dict = vocab_dict[dec_key]\n",
    "key = m_cfg['dev_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(m_dict['20051009_182032_217_fsp-B-1'][dec_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsh_filt_pred, fsh_filt_utts = zip(*sorted([(p,u) for p, u in zip(pred_sents, utts) if (len(m_dict[u]['es_w']) >= min_len) and \n",
    "                                        (len(m_dict[u]['es_w']) <= max_len)], key=lambda t:t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"length filtered utterances = {0:d}\".format(len(fsh_filt_utts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "813 / 3977, (50+115+15+171) / 3977, (50+115+15+171) / 813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_words(m_dict, v_dict, \n",
    "              fsh_filt_pred, \n",
    "              fsh_filt_utts, dec_key, \n",
    "              key, \n",
    "              play_audio=True, \n",
    "              displayN=displayN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_ref = []\n",
    "en_ref = []\n",
    "for u in fsh_filt_utts:\n",
    "    es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "    if type(m_dict[u][dec_key]) == list:\n",
    "        en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "    else:\n",
    "        en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "\n",
    "en_pred = []\n",
    "join_str = ' ' if dec_key.endswith('_w') else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"/afs/inf.ed.ac.uk/group/project/lowres/work/speech2text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(fsh_filt_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b, chrf, h, r = calc_bleu(m_dict, \n",
    "                          v_dict, \n",
    "                          fsh_filt_pred, \n",
    "                          fsh_filt_utts, \n",
    "                          dec_key, \n",
    "                          ref_index=ref_index)\n",
    "\n",
    "print(\"BLEU score on: {0:s} = {1:.2f}\".format(key, b * 100))\n",
    "print(\"-\"*60)\n",
    "\n",
    "model_refs = {u: mr for u, mr in zip(fsh_filt_utts, r)}\n",
    "model_hyps = {u: mh for u, mh in zip(fsh_filt_utts, h)}\n",
    "\n",
    "all_weights=[(1.,0.,0.,0.),\n",
    "             (0.,1.,0.,0.),\n",
    "             (0.,0.,1.,0.),\n",
    "             (0.,0.,0.,1.),\n",
    "             (1./2,1./2,0.,0.),\n",
    "             (1./3,1./3,1./3,0.),\n",
    "             (.25,.25,.25,.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_bleu(r, h, smoothing_function=smooth_fun.method2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_ref, en_hyp = write_predictions_to_file(m_dict, v_dict, fsh_filt_pred, fsh_filt_utts, \n",
    "                                           dec_key, key, stemmify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_refs_dict = {u:v[0].strip().split() for u, v in zip(fsh_filt_utts, en_ref)}\n",
    "model_hyps_dict = {u:v.strip().split() for u, v in zip(fsh_filt_utts, en_hyp)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_refs_dict, open(os.path.join(m_cfg['model_dir'], \"model_s2t_refs.dict\"), \"wb\"))\n",
    "pickle.dump(model_hyps_dict, open(os.path.join(m_cfg['model_dir'], \"model_s2t_hyps.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(en_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, _ = corpus_precision_recall(r, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_prec_recall = {'precision' : {}, 'recall' : {}, \"tp\": 0, \"tc\": 0, \"tr\": 0}\n",
    "\n",
    "for utt_id, ref, hyp in zip(fsh_filt_utts, r, h):\n",
    "    es_ref = [w.decode() for w in m_dict[utt_id]['es_w']]\n",
    "    \n",
    "    pval, rval = modified_precision_recall(ref, hyp, n=1)\n",
    "    model_prec_recall['tc'] += pval.numerator\n",
    "    model_prec_recall['tp'] += pval.denominator\n",
    "    model_prec_recall['tr'] += rval.denominator\n",
    "\n",
    "    model_prec_recall['precision'][utt_id] = float(pval)\n",
    "    model_prec_recall['recall'][utt_id] = float(rval)\n",
    "# end for\n",
    "    \n",
    "model_prec_recall['total_precision'] = model_prec_recall['tc'] / model_prec_recall['tp']\n",
    "model_prec_recall['total_recall'] = model_prec_recall['tc'] / model_prec_recall['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tModel metrics\")\n",
    "print(\"-\"*60)\n",
    "print(\"{0:10s} = {1:0.3f}\\n{2:10s} = {3:0.3f}\".format(\"precision\",\n",
    "                                                      model_prec_recall['total_precision'],\n",
    "                                                      \"recall\",\n",
    "                                                      model_prec_recall['total_recall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"model-s2t BLEU score:\")\n",
    "\"{0:0.3f}\".format(100.0 * corpus_bleu(model_refs.values(), model_hyps.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "_, _ = corpus_precision_recall(model_refs.values(), model_hyps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "metrics = {\"p\": {1: [], 2: [], 3: [], 4: []},\n",
    "           \"r\": {1: [], 2: [], 3: [], 4: []}}\n",
    "\n",
    "for ix in range(len(list(model_refs.values())[0])):\n",
    "    temp_refs = [[i[ix]] for i in model_refs.values()]\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\t\\tUsing reference = {0:d}\".format(ix+1))\n",
    "    print(\"-\"*60)\n",
    "    ps, rs = corpus_precision_recall(temp_refs, model_hyps.values())\n",
    "    for i, (p, r) in enumerate(zip(ps, rs)):\n",
    "        metrics['p'][i+1].append(p)\n",
    "        metrics['r'][i+1].append(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_std(vals_dict):\n",
    "#     print(\"estimated mean\")\n",
    "    k = []\n",
    "    u = []\n",
    "    s = []\n",
    "    for m, vals in vals_dict.items():\n",
    "        k.append(m)\n",
    "        u.append(np.mean(vals))\n",
    "        s.append(np.std(vals)/np.sqrt(len(vals)))\n",
    "        print(\"{0:10d}-gram = {1:.2f} ± {2:.2f}\".format(m, np.mean(vals), np.std(vals)/np.sqrt(len(vals))))\n",
    "        \n",
    "    print(\",\".join(map(lambda x : \"{0:.2f}\".format(x), u)))\n",
    "    print(\",\".join(map(lambda x : \"{0:.2f}\".format(x), s)))\n",
    "    return k, u, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s2w_p_r = {}\n",
    "\n",
    "print(\"Precision:\")\n",
    "_, s2w_p_r['p'], s2w_p_r['p_std'] = get_mean_std(metrics['p'])\n",
    "\n",
    "print(\"Recall:\")\n",
    "_, s2w_p_r['r'], s2w_p_r['r_std'] = get_mean_std(metrics['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(m_cfg['model_dir'], \"s2w_p_r.json\"), \"w\") as out_f:\n",
    "    json.dump(s2w_p_r, out_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_predictions = {}\n",
    "for u, hyp in zip(fsh_filt_utts, h):\n",
    "    model_predictions[u] = hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "min_word_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(m_dict):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u]['en_w']) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u]['en_w']])\n",
    "        else:\n",
    "            for ref in m_dict[u]['en_w']:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_only_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at utterances which our model is doing better at, and compare to Google.\n",
    "\n",
    "Are we doing better on a few calls only? Is there any particular speaker or call messing up our results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Query task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\"\n",
    "train_text_fname= \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\"\n",
    "topics_fname = \"../criseslex/fsp06_topics_in_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = [ \"peace\", \"Music\", \"Marriage\", \"Religion\", \"Cell phones\", \n",
    "           \"Dating\", \"Telemarketing and SPAM\", \"Politics\", \"Travel\", \n",
    "           \"Technical devices\", \"Healthcare\", \"Advertisements\", \"Power\", \n",
    "           \"Occupations\", \"Movies\", \"Welfare\", \"Breaking up\", \"Location\", \n",
    "           \"Justice\", \"Memories\", \"Crime\", \"Violence against women\", \"Equality\", \n",
    "            \"Housing\", \"Immigration\",     \n",
    "            # new topics\n",
    "           \"Interracial\", \"Christians\", \"muslims\", \"jews\", \"e-mail\", \n",
    "           \"phone\", \"democracy\", \"Democratic\", \"Republican\", \"technology\", \n",
    "           \"leadership\", \"community\", \"jury\", \"police\", \"inequality\", \n",
    "           \"renting\", \"Violence\", \"immigrants\", \"immigrant\", \"skilled\", \n",
    "           \"Telemarketing\", \"SPAM\", \"skill\", \"job\", \"health\", \"mobile\", \n",
    "            \"ads\", \"physical\", \"emotional\", \"bubble\", \"rent\", \"economy\", \n",
    "            \"abuse\", \"women\", \"city\", \"country\", \"suburban\", \"dollar\", \n",
    "            \"united states\", \"laws\", \"phone\", \"race\", \"biracial\", \"interracial\", \n",
    "            \"marriage\", \"lyrics\", \"sexuality\", \"medicine\", \"television\", \"european\",\n",
    "            \"home\", \"protect\", \"spouse\", \"language\", \"cellphone\", \"money\",\n",
    "            \"doctor\", \"insurance\", \"cigarettes\", \"alcohol\", \"income\", \"salary\",\n",
    "            \"class\", \"censor\", \"rating\", \"programs\", \"government\",\n",
    "            \"relationship\", \"legal\", \"event\", \"life\", \"safe\", \"victim\", \"cops\",\n",
    "            \"wage\", \"illegal\"\n",
    "            ]\n",
    "topics = list(set(t.lower() for t in topics))\n",
    "topics_stem = [stem(t) for t in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_words_stem = {}\n",
    "for w in dev_words:\n",
    "    stem_w = stem(w)\n",
    "    if stem_w not in dev_words_stem:\n",
    "        dev_words_stem[stem_w] = 0\n",
    "    dev_words_stem[stem_w] += dev_words[w]\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE this checks for # of occurrences of the word in the specified text\n",
    "# We later use the # of utterrances, which will be lower or equal to this number\n",
    "prune_topics = [t for t in topics if dev_words_stem.get(stem(t),0) > 10 and dev_words_stem.get(stem(t),0) < 100]\n",
    "len(prune_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(\"selec_query_terms\")\n",
    "sel_topics = random.sample(prune_topics, min(len(prune_topics), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(sel_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_all_terms_references(utt_text, terms, text_key=\"en_w\"):\n",
    "    terms_set = set([stem(i) for i in terms])\n",
    "    terms_search_res = {}\n",
    "    full_words = {}\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        l = (utt_text[u][text_key] if type(utt_text[u]) == dict \n",
    "             else utt_text[u])\n",
    "        for r in l:\n",
    "            for w in set(r):\n",
    "                decoded_w = w.decode() if type(w) != str else w\n",
    "                w_to_search = stem(decoded_w)\n",
    "                if w_to_search in terms_set:\n",
    "                    if w_to_search not in terms_search_res:\n",
    "                        terms_search_res[w_to_search] = set()\n",
    "                        full_words[w_to_search] = set()\n",
    "                    terms_search_res[w_to_search].add(u)\n",
    "                    full_words[w_to_search].add(decoded_w)\n",
    "                # end if found\n",
    "            # end for current reference\n",
    "        # end for all references\n",
    "    # end for all utterances\n",
    "    return terms_search_res, full_words\n",
    "\n",
    "\n",
    "def find_all_terms_predictions(utt_text, terms):\n",
    "    terms_set = set([stem(i) for i in terms])\n",
    "    terms_search_res = {}\n",
    "    full_words = {}\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        r = utt_text[u]\n",
    "        for w in set(r):\n",
    "            w_to_search = stem(w)\n",
    "            if w_to_search in terms_set:\n",
    "                if w_to_search not in terms_search_res:\n",
    "                    terms_search_res[w_to_search] = set()\n",
    "                    full_words[w_to_search] = set()\n",
    "                terms_search_res[w_to_search].add(u)\n",
    "                full_words[w_to_search].add(u)\n",
    "                # end if found\n",
    "            # end for current reference\n",
    "    # end for all utterances\n",
    "    return terms_search_res, full_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def terms_prec_recall(preds, refs):\n",
    "    prec_recall = {}\n",
    "    prec_recall = {'t':0, 'tp':0, 'tc':0, 'terms':{}}\n",
    "    for term in refs.keys():\n",
    "        prec_recall['terms'][term] = {}\n",
    "        prec_recall['terms'][term]['t'] = len(refs[term])\n",
    "        preds_occ = preds.get(term, set())\n",
    "        prec_recall['terms'][term]['tp'] = len(preds_occ)\n",
    "        prec_recall['terms'][term]['tc'] = len(refs[term] & preds_occ)\n",
    "        prec_recall['t'] += prec_recall['terms'][term]['t']\n",
    "        prec_recall['tp'] += prec_recall['terms'][term]['tp']\n",
    "        prec_recall['tc'] += prec_recall['terms'][term]['tc']\n",
    "    # end for each term\n",
    "    return prec_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_topics, ref_topic_labels = find_all_terms_references(map_dict['fisher_dev'], sel_topics)\n",
    "pred_topics, pred_topics_topics_labels = find_all_terms_predictions(model_predictions, sel_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(ref_topics), len(pred_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_topics_in_utts = {k:v for k, v in ref_topics.items() if len(v) >=3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(prune_topics_in_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(6,10),nrows=1, ncols=1)\n",
    "\n",
    "\n",
    "sorted_counts = [t[0] for t in sorted(ref_topics.items(), reverse=True, key=lambda t:len(t[1]))]\n",
    "topic_count = [len(ref_topics[t]) for t in sorted_counts]\n",
    "\n",
    "topic_labels = [\"-\".join(list(ref_topic_labels[t])[:3]) for t in sorted_counts]\n",
    "\n",
    "ax = sns.barplot(x=topic_count, \n",
    "                 y=topic_labels, \n",
    "                 color=tableau20[6], alpha=.7)\n",
    "                 #**{\"label\":\"topic counts\", \"alpha\":0.5}, ax=ax)\n",
    "\n",
    "# ax.set_xlabel(\"# of utts in which topic occurs\", size=20)\n",
    "\n",
    "max_count_val = max([len(v) for v in ref_topics.values()])\n",
    "min_count_val = max([len(v) for v in ref_topics.values()])\n",
    "plt.xticks(list(range(0,max_count_val, 5))+[min_count_val, max_count_val])\n",
    "\n",
    "# ax.legend(loc='upper right', bbox_to_anchor=(1.1, 0.9),\n",
    "#                       ncol=1, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "sns.despine(left=True, bottom=True, top=True, right=False)\n",
    "\n",
    "for t in ax.get_yticklabels():\n",
    "    t.set_fontsize(15) \n",
    "    \n",
    "for i, t in enumerate(ax.get_xticklabels()):\n",
    "    if i > 0 and i < (len(ax.get_xticklabels())-1):\n",
    "        t.set_visible(False)\n",
    "    else:\n",
    "        t.set_fontsize(16)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig.savefig(\"../criseslex/sel_topics_new.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_topics_p_r = terms_prec_recall(preds = pred_topics, refs = ref_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_topics_p_r['precision'] = model_topics_p_r['tc'] / model_topics_p_r['tp']\n",
    "model_topics_p_r['recall'] = model_topics_p_r['tc'] / model_topics_p_r['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"-------------- Query by Text\")\n",
    "print(\"-\"*40)\n",
    "print(\"{0:20s} | {1:5.2f}%\".format(\"precision\", model_topics_p_r['precision']*100.0))\n",
    "print(\"{0:20s} | {1:5.2f}%\".format(\"recall\", model_topics_p_r['recall']*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(m_cfg['model_dir'], \"topics_p_r.json\"), \"w\") as out_f:\n",
    "    json.dump(model_topics_p_r, out_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_all_terms_es_references(utt_text, term):\n",
    "    terms_search_res = set()\n",
    "    total_word_count = 0\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        l = (utt_text[u][\"es_w\"] if type(utt_text[u]) == dict \n",
    "             else utt_text[u])\n",
    "        #if len(l) == 1 and l[0].decode() == term:\n",
    "        if set(l) == set(term.split()):\n",
    "            terms_search_res.add(u)\n",
    "            total_word_count += 1\n",
    "            # end if found\n",
    "        # end for current reference\n",
    "        # end for all references\n",
    "    # end for all utterances\n",
    "    return terms_search_res, total_word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_key = \"fisher_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hmm, tot_count = find_all_terms_es_references(map_dict[set_key], b'claro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ref_words = {}\n",
    "all_match_count = 0\n",
    "no_match_count = 0\n",
    "\n",
    "utt_refs_set = {}\n",
    "for utt in hmm:\n",
    "    #print(\"-\"*60)\n",
    "    #print(utt)\n",
    "    curr_ref = {k : \"\" for k in range(4)}\n",
    "    if \"train\" in set_key:\n",
    "        r = map_dict[set_key][utt][\"en_w\"]\n",
    "        en_ref_words = \" \".join(w.decode() for w in r)\n",
    "        curr_ref[0] = en_ref_words\n",
    "        if en_ref_words not in all_ref_words:\n",
    "            all_ref_words[en_ref_words] = 0\n",
    "        all_ref_words[en_ref_words] += 1\n",
    "    else:\n",
    "        for i, r in enumerate(map_dict[set_key][utt][\"en_w\"]):\n",
    "            en_ref_words = \" \".join(w.decode() for w in r)\n",
    "            curr_ref[i] = en_ref_words\n",
    "            #print(\" \".join(w.decode() for w in r), end=\"----\")\n",
    "            if en_ref_words not in all_ref_words:\n",
    "                all_ref_words[en_ref_words] = 0\n",
    "            all_ref_words[en_ref_words] += 1\n",
    "    #print(curr_ref.values())\n",
    "\n",
    "    \n",
    "    utt_refs_set[utt] = set(curr_ref.values())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(utt_refs_set['20051023_232057_325_fsp-B-77']) if \"dev\" in set_key else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " if \"dev\" in set_key:\n",
    "    start_set = utt_refs_set['20051023_232057_325_fsp-B-77']\n",
    "    for utt in utt_refs_set:\n",
    "        start_set = start_set & utt_refs_set[utt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_match_count = sum([len(utt_refs_set[utt]) == 1 for utt in utt_refs_set])\n",
    "all_dont_match_count = sum([len(utt_refs_set[utt]) == 4  for utt in utt_refs_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_match_count, all_dont_match_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ref_words, len(all_ref_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utt_refs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_dict = map_dict[set_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel_utts = [utt for utt in utt_refs_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data = get_batch(m_dict, enc_key, dec_key, sel_utts[:101], vocab_dict, 200, 10, input_path=input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = xp.asnumpy(batch_data['X'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "distance, path = fastdtw(A[1], A[2], dist=euclidean)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtw_out = np.zeros((len(A), len(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(A))):\n",
    "    for j in range(i,len(A)):\n",
    "        dtw_out[i,j], _ = fastdtw(A[i], A[j], dist=euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_matrix = dtw_out + dtw_out.T - - np.diag(np.diag(dtw_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(dist_matrix), np.std(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(dist_matrix, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = np.zeros_like(dist_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(dist_matrix, mask=mask, square=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time as time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "ward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X)\n",
    "elapsed_time = time.time() - st\n",
    "label = ward.labels_\n",
    "print(\"Elapsed time: %.2fs\" % elapsed_time)\n",
    "print(\"Number of points: %i\" % label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = A.reshape(len(A),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "B_r = pca.fit(B).transform(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n",
    "               color=plt.cm.jet(np.float(l) / np.max(label + 1)),\n",
    "               s=20, edgecolor='k')\n",
    "plt.title('Without connectivity constraints (time %.2fs)' % elapsed_time)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Define the structure A of the data. Here a 10 nearest neighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "connectivity = kneighbors_graph(X, n_neighbors=25, include_self=False)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute clustering\n",
    "print(\"Compute structured hierarchical clustering...\")\n",
    "st = time.time()\n",
    "ward = AgglomerativeClustering(n_clusters=10, connectivity=connectivity,\n",
    "                               linkage='ward').fit(X)\n",
    "elapsed_time = time.time() - st\n",
    "label = ward.labels_\n",
    "print(\"Elapsed time: %.2fs\" % elapsed_time)\n",
    "print(\"Number of points: %i\" % label.size)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n",
    "               color=plt.cm.jet(float(l) / np.max(label + 1)),\n",
    "               s=20, edgecolor='k')\n",
    "plt.title('With connectivity constraints (time %.2fs)' % elapsed_time)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ward = AgglomerativeClustering(n_clusters=5, connectivity=connectivity,\n",
    "                               linkage='ward').fit(X)\n",
    "\n",
    "labels = ward.labels_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for color, l in zip(tableau20, np.unique(labels)):\n",
    "    ax.scatter(B_r[labels == l, 0], B_r[labels == l, 1], s=100,\n",
    "               color=plt.cm.jet(float(l) / np.max(labels + 1)), edgecolor='k')\n",
    "plt.yticks(visible=False)\n",
    "plt.xticks(visible=False)\n",
    "fig.savefig(\"agglo_dtw_claro.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "est = KMeans(n_clusters=5)\n",
    "est.fit(B)\n",
    "labels = est.labels_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for color, l in zip(tableau20, np.unique(labels)):\n",
    "    ax.scatter(B_r[labels == l, 0], B_r[labels == l, 1], s=100,\n",
    "               color=plt.cm.jet(float(l) / np.max(labels + 1)), edgecolor='k')\n",
    "# done\n",
    "plt.yticks(visible=False)\n",
    "plt.xticks(visible=False)\n",
    "fig.savefig(\"k_means_claro.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(list(ward.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = ward.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.unique(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X[label == l, 0], X[label == l, 1], X[label == l, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the results on an image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(X, cmap=plt.cm.gray)\n",
    "for l in range(n_clusters):\n",
    "    plt.contour(label == l, contours=1,\n",
    "                colors=[plt.cm.spectral(l / float(n_clusters)), ])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(A)\n",
    "print('pairwise dense output:\\n {}\\n'.format(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"20051021_222225_307_fsp-B-109\" in map_dict['fisher_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(m_dict, x_key, y_key, utt_list, vocab_dict,\n",
    "              max_enc, max_dec, input_path=''):\n",
    "    batch_data = {'X':[], 'y':[]}\n",
    "    # -------------------------------------------------------------------------\n",
    "    # loop through each utterance in utt list\n",
    "    # -------------------------------------------------------------------------\n",
    "    for u in utt_list:\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add X data\n",
    "        # ---------------------------------------------------------------------\n",
    "        if x_key == 'sp':\n",
    "            # -----------------------------------------------------------------\n",
    "            # for speech data\n",
    "            # -----------------------------------------------------------------\n",
    "            # get path to speech file\n",
    "            utt_sp_path = os.path.join(input_path, \"{0:s}.npy\".format(u))\n",
    "            if not os.path.exists(utt_sp_path):\n",
    "                # for training data, there are sub-folders\n",
    "                utt_sp_path = os.path.join(input_path,\n",
    "                                           u.split('_',1)[0],\n",
    "                                           \"{0:s}.npy\".format(u))\n",
    "            if os.path.exists(utt_sp_path):\n",
    "                x_data = xp.load(utt_sp_path)\n",
    "                # truncate max length\n",
    "                batch_data['X'].append(x_data[:max_enc])\n",
    "            else:\n",
    "                # -------------------------------------------------------------\n",
    "                # exception if file not found\n",
    "                # -------------------------------------------------------------\n",
    "                print(\"ERROR!! file not found: {0:s}\".format(utt_sp_path))\n",
    "                # -------------------------------------------------------------\n",
    "        else:\n",
    "            # -----------------------------------------------------------------\n",
    "            # for text data\n",
    "            # -----------------------------------------------------------------\n",
    "            x_ids = [vocab_dict[x_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][x_key]]\n",
    "            x_ids = xp.asarray(x_ids, dtype=xp.int32)\n",
    "            batch_data['X'].append(x_ids[:max_enc])\n",
    "            # -----------------------------------------------------------------\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add labels\n",
    "        # ---------------------------------------------------------------------\n",
    "        if type(m_dict[u][y_key]) == list:\n",
    "            en_ids = [vocab_dict[y_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key]]\n",
    "        else:\n",
    "            # dev and test data have multiple translations\n",
    "            # choose the first one for computing perplexity\n",
    "            en_ids = [vocab_dict[y_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key][0]]\n",
    "        y_ids = [GO_ID] + en_ids[:max_dec-2] + [EOS_ID]\n",
    "        batch_data['y'].append(xp.asarray(y_ids, dtype=xp.int32))\n",
    "        # ---------------------------------------------------------------------\n",
    "    # -------------------------------------------------------------------------\n",
    "    # end for all utterances in batch\n",
    "    # -------------------------------------------------------------------------\n",
    "    if len(batch_data['X']) > 0 and len(batch_data['y']) > 0:\n",
    "        batch_data['X'] = F.pad_sequence(batch_data['X'], padding=PAD_ID)\n",
    "        batch_data['y'] = F.pad_sequence(batch_data['y'], padding=PAD_ID)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# puerto rico utterance\n",
    "# \"i 'm from puerto rico but i live here in denver colorado\"\n",
    "\" \".join(w.decode() for w in map_dict['fisher_dev']['20051023_232057_325_fsp-A-3']['en_w'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
