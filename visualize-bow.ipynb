{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bow_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_basic_precision_recall(r, h, display=False):\n",
    "    p_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    p_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    r_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    r_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    metrics = {\"rc\": 0, \"rt\": 0, \"tp\": 0, \"tc\": 0, \"word\": {}}\n",
    "\n",
    "    if display:\n",
    "        print(\"total utts={0:d}\".format(len(r)))\n",
    "\n",
    "    i=1\n",
    "\n",
    "    for references, hypothesis in zip(r, h):\n",
    "        if min([len(any_ref) for any_ref in references]) > 0:\n",
    "            if len(hypothesis) > 0:\n",
    "                p_i = modified_precision(references, hypothesis, i)\n",
    "                p_numerators[i] += p_i.numerator\n",
    "                p_denominators[i] += p_i.denominator\n",
    "\n",
    "                metrics[\"tc\"] += p_i.numerator\n",
    "                metrics[\"tp\"] += p_i.denominator\n",
    "            else:\n",
    "                p_numerators[i] += 0\n",
    "                p_denominators[i] += 0\n",
    "\n",
    "                metrics[\"tc\"] += 0\n",
    "                metrics[\"tp\"] += 0\n",
    "\n",
    "            #print(p_i.numerator, p_i.denominator)\n",
    "\n",
    "            tot_match = 0\n",
    "            tot_count = 0\n",
    "\n",
    "            common_ref_words = set(references[0])\n",
    "            for curr_ref in references[1:]:\n",
    "                common_ref_words &= set(curr_ref)\n",
    "            \n",
    "            common_words = common_ref_words & set(hypothesis)\n",
    "            for w in common_ref_words:\n",
    "                if w not in metrics[\"word\"]:\n",
    "                    metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "                metrics[\"word\"][w][\"t\"] += 1\n",
    "            \n",
    "            for w in set(hypothesis):\n",
    "                if w not in metrics[\"word\"]:\n",
    "                    metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "                metrics[\"word\"][w][\"tp\"] += 1\n",
    "                \n",
    "            for w in common_words:\n",
    "                if w not in metrics[\"word\"]:\n",
    "                    metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "                metrics[\"word\"][w][\"tc\"] += 1\n",
    "            \n",
    "            r_numerators[i] += len(common_words)\n",
    "            r_denominators[i] += len(common_ref_words)\n",
    "            metrics[\"rc\"] += len(common_words)\n",
    "            metrics[\"rt\"] += len(common_ref_words)\n",
    "            \n",
    "\n",
    "#             max_recall_match, max_tp, max_t, max_word_level_details = count_match(list(common_ref_words), list(set(hypothesis)))\n",
    "#             max_recall = max_recall_match / max_t if max_t > 0 else 0\n",
    "\n",
    "            # max_recall_match, max_tp, max_t, max_word_level_details = count_match(references[0], hypothesis)\n",
    "            # max_recall = max_recall_match / max_t if max_t > 0 else 0\n",
    "\n",
    "            # for curr_ref in references:\n",
    "            #     curr_match, curr_tp, curr_t, curr_word_level_details = count_match(curr_ref, hypothesis)\n",
    "            #     curr_recall = curr_match / curr_t if curr_t > 0 else 0\n",
    "\n",
    "            #     if curr_recall > max_recall:\n",
    "            #         max_recall_match = curr_match\n",
    "            #         max_t = curr_t\n",
    "            #         max_recall = curr_recall\n",
    "            #         max_word_level_details = curr_word_level_details\n",
    "            \n",
    "\n",
    "#             r_numerators[i] += max_recall_match\n",
    "#             r_denominators[i] += max_t\n",
    "#             metrics[\"rc\"] += max_recall_match\n",
    "#             metrics[\"rt\"] += max_t\n",
    "#             for key in {\"t\",\"tp\",\"tc\"}:\n",
    "#                 for w in max_word_level_details[key]:\n",
    "#                     if w not in metrics[\"word\"]:\n",
    "#                         metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "#                     metrics[\"word\"][w][key] += max_word_level_details[key][w]\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    prec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(p_numerators.values(), p_denominators.values())]\n",
    "    rec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(r_numerators.values(), r_denominators.values())]\n",
    "\n",
    "    if display:\n",
    "        print(\"{0:10s} | {1:>8s}\".format(\"metric\", \"1-gram\"))\n",
    "        print(\"-\"*54)\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"precision\", *prec))\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"recall\", *rec))\n",
    "\n",
    "    return prec[0], rec[0], metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmt_basic_precision_recall(r, h, display=False):\n",
    "    p_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    p_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    r_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    r_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    metrics = {\"rc\": 0, \"rt\": 0, \"tp\": 0, \"tc\": 0, \"word\": {}}\n",
    "\n",
    "    if display:\n",
    "        print(\"total utts={0:d}\".format(len(r)))\n",
    "\n",
    "    i=1\n",
    "\n",
    "    for references, hypothesis in zip(r, h):\n",
    "        if min([len(any_ref) for any_ref in references]) > 0:\n",
    "            if len(hypothesis) > 0:\n",
    "                p_i = modified_precision(references, hypothesis, i)\n",
    "                p_numerators[i] += p_i.numerator\n",
    "                p_denominators[i] += p_i.denominator\n",
    "\n",
    "                metrics[\"tc\"] += p_i.numerator\n",
    "                metrics[\"tp\"] += p_i.denominator\n",
    "            else:\n",
    "                p_numerators[i] += 0\n",
    "                p_denominators[i] += 0\n",
    "\n",
    "                metrics[\"tc\"] += 0\n",
    "                metrics[\"tp\"] += 0\n",
    "\n",
    "            #print(p_i.numerator, p_i.denominator)\n",
    "\n",
    "            tot_match = 0\n",
    "            tot_count = 0\n",
    "\n",
    "            max_recall_match, max_tp, max_t, max_word_level_details = count_match(references[0], hypothesis)\n",
    "            max_recall = max_recall_match / max_t if max_t > 0 else 0\n",
    "\n",
    "            for curr_ref in references:\n",
    "                curr_match, curr_tp, curr_t, curr_word_level_details = count_match(curr_ref, hypothesis)\n",
    "                curr_recall = curr_match / curr_t if curr_t > 0 else 0\n",
    "\n",
    "                if curr_recall > max_recall:\n",
    "                    max_recall_match = curr_match\n",
    "                    max_t = curr_t\n",
    "                    max_recall = curr_recall\n",
    "                    max_word_level_details = curr_word_level_details\n",
    "\n",
    "            r_numerators[i] += max_recall_match\n",
    "            r_denominators[i] += max_t\n",
    "            metrics[\"rc\"] += max_recall_match\n",
    "            metrics[\"rt\"] += max_t\n",
    "            for key in {\"t\",\"tp\",\"tc\"}:\n",
    "                for w in max_word_level_details[key]:\n",
    "                    if w not in metrics[\"word\"]:\n",
    "                        metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "                    metrics[\"word\"][w][key] += max_word_level_details[key][w]\n",
    "\n",
    "    prec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(p_numerators.values(), p_denominators.values())]\n",
    "    rec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(r_numerators.values(), r_denominators.values())]\n",
    "\n",
    "    if display:\n",
    "        print(\"{0:10s} | {1:>8s}\".format(\"metric\", \"1-gram\"))\n",
    "        print(\"-\"*54)\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"precision\", *prec))\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"recall\", *rec))\n",
    "\n",
    "    return prec[0], rec[0], metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(nmt_path, use_google=False):\n",
    "    if use_google:\n",
    "        google_s2t_hyps, google_s2t_refs, nmt_4refs = get_google_data()\n",
    "        nmt_hyps = google_s2t_hyps['fisher_dev_r0']\n",
    "        nmt_refs = google_s2t_refs['fisher_dev_ref_0']\n",
    "    else:\n",
    "        nmt_refs = pickle.load(open(os.path.join(nmt_path, \n",
    "                                                 \"model_s2t_refs.dict\"), \"rb\"))\n",
    "        nmt_hyps = pickle.load(open(os.path.join(nmt_path, \n",
    "                                                 \"model_s2t_hyps.dict\"), \"rb\"))\n",
    "        nmt_4refs = pickle.load(open(os.path.join(nmt_path,\n",
    "                                                  \"model_s2t_refs_for_eval.dict\"), \"rb\"))\n",
    "    \n",
    "    return nmt_refs, nmt_hyps, nmt_4refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nmt_model(nmt_path, use_google=False, min_len=10):\n",
    "    smooth_fun = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    \n",
    "    nmt_refs, nmt_hyps, nmt_4refs = get_model_data(nmt_path, use_google=use_google)\n",
    "\n",
    "    nmt_preds_bow = {}\n",
    "    nmt_1_ref = {}\n",
    "    nmt_refs_bow = {}\n",
    "\n",
    "    dev_utt_ids = nmt_hyps.keys()\n",
    "\n",
    "    for u in dev_utt_ids:\n",
    "        nmt_preds_bow[u] = list(get_words_in_bow_vocab(nmt_hyps[u], bow_dict))\n",
    "        nmt_refs_bow[u] = []\n",
    "        nmt_1_ref[u] = [list(get_words_in_bow_vocab(nmt_refs[u], bow_dict))]\n",
    "        for r in nmt_4refs[u]:\n",
    "            nmt_refs_bow[u].append(list(get_words_in_bow_vocab(r, bow_dict)))\n",
    "    \n",
    "    \n",
    "    p_bow, r_bow, metrics_1_bow = bow_basic_precision_recall(nmt_1_ref.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    print(\"-\"*80)\n",
    "    print(\"BOW - using 1 reference\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_bow, r_bow))\n",
    "    num_1correct = len([item for item in metrics_1_bow[\"word\"].items() \n",
    "                        if item[1]['tc'] > 0])\n",
    "    \n",
    "    num_all = len([item for item in metrics_1_bow[\"word\"].items() \n",
    "                        if item[1]['t'] > 0])\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"{0:d} out of {1:d} retrieved with atleast 1 correct instance\".format(num_1correct, num_all))\n",
    "    \n",
    "\n",
    "    p_bow, r_bow, metrics_bow = bow_basic_precision_recall(nmt_refs_bow.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(\"BOW - using all 4 references\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_bow, r_bow))\n",
    "    \n",
    "    num_1correct = len([item for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['tc'] > 0])\n",
    "    \n",
    "    num_all = len([item for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['t'] > 0])\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"{0:d} out of {1:d} retrieved with atleast 1 correct instance\".format(num_1correct, num_all))\n",
    "    \n",
    "    # MT PRECISION RECALL - NOOOT BOW\n",
    "    p_nmt, r_nmt, metrics_nmt = nmt_basic_precision_recall(nmt_4refs.values(), \n",
    "                                                       nmt_hyps.values())\n",
    "    print(\"-\"*80)\n",
    "    print(\"MT task - using all 4 references\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_nmt, r_nmt))\n",
    "\n",
    "    nmt_bleu = corpus_bleu(nmt_4refs.values(), \n",
    "                           nmt_hyps.values(),\n",
    "                           smoothing_function=smooth_fun.method2)\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    print(\"4 references bleu={0:2f}\".format(nmt_bleu*100))\n",
    "    \n",
    "    one_ref_list = []\n",
    "    one_hyp_list = []\n",
    "    \n",
    "#     for u in nmt_refs:\n",
    "#         one_ref_list.append([nmt_refs[u]])\n",
    "#         one_hyp_list.append(nmt_hyps[u])\n",
    "        \n",
    "#     p_nmt_one, r_nmt_one, metrics_nmt_one = nmt_basic_precision_recall(one_ref_list, \n",
    "#                                                            one_hyp_list)\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"MT task - using single references\")\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"precision={0:.2f}, recall={1:.2f}\".format(p_nmt_one, r_nmt_one))\n",
    "    \n",
    "#     nmt_bleu = corpus_bleu(one_ref_list, \n",
    "#                            one_hyp_list,\n",
    "#                            smoothing_function=smooth_fun.method2)\n",
    "\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"single reference bleu={0:2f}\".format(nmt_bleu*100))\n",
    "    print(\"-\"*80)\n",
    "    print(\"using min len filter\")\n",
    "    print(\"-\"*20)\n",
    "    check_bleu_with_len_filter(nmt_4refs, nmt_hyps, min_len=min_len)\n",
    "    \n",
    "    return metrics_1_bow, metrics_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prec_recall_for_words(nmt_path, bow_dict, use_google=False):\n",
    "    smooth_fun = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    \n",
    "    nmt_refs, nmt_hyps, nmt_4refs = get_model_data(nmt_path, use_google=use_google)\n",
    "\n",
    "    nmt_preds_bow = {}\n",
    "    nmt_1_ref = {}\n",
    "    nmt_refs_bow = {}\n",
    "\n",
    "    dev_utt_ids = nmt_hyps.keys()\n",
    "\n",
    "    for u in dev_utt_ids:\n",
    "        nmt_preds_bow[u] = list(get_words_in_bow_vocab(nmt_hyps[u], bow_dict))\n",
    "        nmt_refs_bow[u] = []\n",
    "        nmt_1_ref[u] = [list(get_words_in_bow_vocab(nmt_refs[u], bow_dict))]\n",
    "        for r in nmt_4refs[u]:\n",
    "            nmt_refs_bow[u].append(list(get_words_in_bow_vocab(r, bow_dict)))\n",
    "    \n",
    "    \n",
    "    p_bow, r_bow, metrics_1_bow = bow_basic_precision_recall(nmt_1_ref.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    \n",
    "#     print(\"-\"*80)\n",
    "#     print(\"Using word list: \\n{0:s}\".format(\" -- \".join([w.decode() for w in bow_dict[\"w2i\"].keys()])))\n",
    "#     print(\"number of words: {0:d}\".format(len(bow_dict[\"w2i\"])))\n",
    "    print(\"-\"*80)\n",
    "    print(\"-\"*20)\n",
    "    p_bow, r_bow, metrics_bow = bow_basic_precision_recall(nmt_refs_bow.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(\"BOW - using all 4 references\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_bow, r_bow))\n",
    "    \n",
    "    num_1correct = len([item for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['tc'] > 0])\n",
    "    \n",
    "    words_present = [item[0] for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['t'] > 0]\n",
    "    num_all = len(words_present)\n",
    "    print(\"-\"*80)\n",
    "    print(\"Using word list: \\n{0:s}\".format(\" -- \".join(words_present)))\n",
    "    print(\"number of words: {0:d}\".format(num_all))\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"{0:d} out of {1:d} retrieved with atleast 1 correct instance\".format(num_1correct, num_all))\n",
    "    \n",
    "    \n",
    "    return metrics_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_word_lists(nmt_path, use_google=False):\n",
    "    metrics = {}\n",
    "    eval_word_lists = {\"en_freq\": os.path.join(m_cfg['data_path'], \"eval_en_freq_vocab.dict\"),\n",
    "                       \"en_rare\": os.path.join(m_cfg['data_path'], \"eval_en_rare_vocab.dict\"),\n",
    "                       \"en_es_common\": os.path.join(m_cfg['data_path'], \n",
    "                                                    \"eval_en_es_common_vocab.dict\"),\n",
    "                       \"crisis\": os.path.join(m_cfg['data_path'], \n",
    "                                                    \"bow_crises_vocab.dict\")}\n",
    "    for key, word_list in eval_word_lists.items():\n",
    "        words = pickle.load(open(word_list, \"rb\"))\n",
    "        metrics[key] = eval_prec_recall_for_words(nmt_path, words, use_google=use_google)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bleu_with_len_filter(refs, hyps, min_len):\n",
    "    sel_refs, sel_hyps = [], []\n",
    "    for u in refs:\n",
    "        len_ref = min([len(r) for r in refs[u]])\n",
    "        if len_ref >= min_len:\n",
    "            sel_refs.append(refs[u])\n",
    "            sel_hyps.append(hyps[u])\n",
    "    print(\"{0:d} out of {1:d} have len >= {2:d}\".format(len(sel_refs), len(refs), min_len))\n",
    "    bleu_score = corpus_bleu(sel_refs, sel_hyps, smoothing_function=smooth_fun.method2)*100\n",
    "    print(\"BLEU={0:.2f}\".format(bleu_score))\n",
    "    sel_p, sel_r, _ = nmt_basic_precision_recall(sel_refs, sel_hyps)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(sel_p, sel_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"sp2bagwords/sp_0.50_trial-A/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_out_dim = rnn_in_units =  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/project/lowres/work/miniconda3/envs/chainer3/lib/python3.6/site-packages/chainer/utils/experimental.py:104: FutureWarning: chainer.links.normalization.layer_normalization.py is experimental. The interface can change in the future.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ADAM optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "model not found\n"
     ]
    }
   ],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict, bow_dict = get_data_dicts(m_cfg)\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\")):\n",
    "    dev_utts = pickle.load(open(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\"), \"rb\"))\n",
    "\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"model_s2t_train_out.dict\")):\n",
    "    train_utts = pickle.load(open(os.path.join(m_cfg['model_dir'], \"model_s2t_train_out.dict\"), \"rb\"))\n",
    "# batch_size = {'max': 128, 'med': 128, 'min': 128, 'scale': 1}\n",
    "batch_size = {'max': 64, 'med': 64, 'min': 64, 'scale': 1}\n",
    "batch_size = t_cfg['batch_size']\n",
    "\n",
    "edin_s2t_refs_for_eval_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \n",
    "                                           \"edin_s2t_refs_for_eval.dict\")\n",
    "edin_s2t_refs_for_eval = pickle.load(open(edin_s2t_refs_for_eval_path, \"rb\"))\n",
    "single_dev_ref = [[i[0]] for i in dev_utts[\"refs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict[\"fisher_dev\"]['20051009_182032_217_fsp-B-1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'],\n",
    "                                      m_cfg['train_set'])\n",
    "train_utts, train_loss = feed_model(model,\n",
    "                              optimizer=optimizer,\n",
    "                              m_dict=map_dict[train_key],\n",
    "                              b_dict=bucket_dict[train_key],\n",
    "                              vocab_dict=vocab_dict,\n",
    "                              bow_dict=bow_dict,\n",
    "                              batch_size=batch_size,\n",
    "                              x_key=enc_key,\n",
    "                              y_key=dec_key,\n",
    "                              train=False,\n",
    "                              input_path=input_path,\n",
    "                              max_dec=m_cfg['max_en_pred'],\n",
    "                              t_cfg=t_cfg,\n",
    "                              use_y=True,\n",
    "                              get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(train_utts, open(os.path.join(m_cfg['model_dir'], \"model_s2t_train_out.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pos_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "mean_neg_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "\n",
    "\n",
    "for i_w in range(4, len(bow_dict[\"i2w\"])):\n",
    "    this_word = bow_dict[\"i2w\"][i_w]\n",
    "    pos_indx = [i_w in r[0] for r in train_utts[\"refs\"]]\n",
    "    neg_indx = [i_w not in r[0] for r in train_utts[\"refs\"]]\n",
    "    mean_pos_scores[i_w] = np.mean(train_utts[\"probs\"][:,i_w][pos_indx])\n",
    "    mean_neg_scores[i_w] = np.mean(train_utts[\"probs\"][:,i_w][neg_indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.mean(mean_pos_scores), xp.mean(mean_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_p, _ = compute_avg_precision(train_utts[\"probs\"],\n",
    "                                                     0.0, 1.0, 5,\n",
    "                                                     m_cfg['max_en_pred'],\n",
    "                                                     train_utts[\"refs\"])\n",
    "train_avg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = m_cfg[\"pred_thresh\"]\n",
    "train_pred_words = get_pred_words_from_probs(train_utts[\"probs\"],\n",
    "#                                              mean_pos_scores,\n",
    "                                               0.5,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "\n",
    "train_prec, train_rec, _ = basic_precision_recall(train_utts[\"refs\"], train_pred_words)\n",
    "train_prec, train_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "\n",
    "dev_utts, dev_loss = feed_model(model,\n",
    "                                optimizer=optimizer,\n",
    "                                m_dict=map_dict[dev_key],\n",
    "                                b_dict=bucket_dict[dev_key],\n",
    "                                vocab_dict=vocab_dict,\n",
    "                                bow_dict=bow_dict,\n",
    "                                batch_size=batch_size,\n",
    "                                x_key=enc_key,\n",
    "                                y_key=dec_key,\n",
    "                                train=False,\n",
    "                                input_path=input_path,\n",
    "                                max_dec=m_cfg['max_en_pred'],\n",
    "                                t_cfg=t_cfg,\n",
    "                                use_y=True,\n",
    "                                get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dev_ref = [[i[0]] for i in dev_utts[\"refs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(dev_utts[\"probs\"]), np.max(dev_utts[\"probs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dev_pos_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "mean_dev_neg_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "\n",
    "\n",
    "for i_w in range(4, len(bow_dict[\"i2w\"])):\n",
    "    this_word = bow_dict[\"i2w\"][i_w]\n",
    "    pos_indx = [i_w in r[0] for r in dev_utts[\"refs\"]]\n",
    "    neg_indx = [i_w not in r[0] for r in dev_utts[\"refs\"]]\n",
    "    mean_dev_pos_scores[i_w] = np.mean(dev_utts[\"probs\"][:,i_w][pos_indx])\n",
    "    mean_dev_neg_scores[i_w] = np.mean(dev_utts[\"probs\"][:,i_w][neg_indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.2\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, haha = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                               mean_dev_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "p, r, _ = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.1\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                               mean_dev_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_prob, max_prob = float(xp.min(dev_utts[\"probs\"])), float(xp.max(dev_utts[\"probs\"]))\n",
    "min_prob, max_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Plot - word level threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-0.5, 0.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pos_scores[4:14]*1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_deltas = np.asarray([0.7,0.8,0.9,1,1.1,1.2,1.3], dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_thresh = {}\n",
    "thresh_delta = 0.05\n",
    "for thresh in tqdm(np.arange(-0.5, 0.5+thresh_delta, thresh_delta)):\n",
    "# for thresh in tqdm(thresh_deltas):\n",
    "# for thresh in tqdm(np.linspace(min_prob, max_prob,num=20,endpoint=True)):\n",
    "    p_r_thresh[thresh] = {}\n",
    "    dev_pred_words_at_thresh = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                                           mean_pos_scores + thresh,\n",
    "                                                           len(bow_dict['i2w']))\n",
    "    p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(dev_utts[\"refs\"], \n",
    "                                                                              dev_pred_words_at_thresh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels, p_vals, r_vals = [], [], []\n",
    "for l in p_r_thresh:\n",
    "    p_vals.append(p_r_thresh[l][\"p\"])\n",
    "    r_vals.append(p_r_thresh[l][\"r\"])\n",
    "    thresh_labels.append(\"{0:.2f}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(r_vals, p_vals, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(r_vals, p_vals, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Plot - fixed threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_utts[\"probs\"][0]), len(bow_dict['i2w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_precision(probs, min_prob, max_prob, num_points, max_words, refs):\n",
    "    p_r_thresh = {}\n",
    "    for thresh in tqdm(np.linspace(min_prob, max_prob, num=num_points, endpoint=True)):\n",
    "        p_r_thresh[thresh] = {}\n",
    "        words_at_thresh = get_pred_words_from_probs(probs, thresh, max_words)\n",
    "        p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(refs, words_at_thresh)\n",
    "    \n",
    "    precision_array = np.array([p_r_thresh[i]['p']/100 for i in p_r_thresh], dtype=\"f\")\n",
    "    recall_array = np.array([p_r_thresh[i]['r']/100 for i in p_r_thresh], dtype=\"f\")\n",
    "    avg_p = np.trapz(precision_array[::-1], recall_array[::-1])\n",
    "    return avg_p, p_r_thresh\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_p, p_r_thresh = compute_avg_precision(dev_utts[\"probs\"], 0.0, 1.0, 50, 104, dev_utts[\"refs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_r_thresh = {}\n",
    "# thresh_delta = 0.01\n",
    "# # for thresh in tqdm(np.arange(min_prob, max_prob+thresh_delta,thresh_delta)):\n",
    "# for thresh in tqdm(np.linspace(min_prob, max_prob, num=30,endpoint=True)):\n",
    "#     p_r_thresh[thresh] = {}\n",
    "#     dev_pred_words_at_thresh = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "#                                                            thresh,\n",
    "#                                                            len(bow_dict['i2w']))\n",
    "#     p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(dev_utts[\"refs\"], \n",
    "#                                                                               dev_pred_words_at_thresh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels, p_vals, r_vals = [], [], []\n",
    "for l in p_r_thresh:\n",
    "    p_vals.append(p_r_thresh[l][\"p\"])\n",
    "    r_vals.append(p_r_thresh[l][\"r\"])\n",
    "    thresh_labels.append(\"{0:.2f}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(r_vals, p_vals, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(r_vals, p_vals, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.15\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)\n",
    "# print(\"using mean positive prediction threshold\")\n",
    "# dev_mean_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "#                                            mean_pos_scores,\n",
    "#                                            m_cfg['max_en_pred'])\n",
    "# p, r, _ = basic_precision_recall(single_dev_ref, dev_mean_pred_words)\n",
    "# print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.01\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)\n",
    "# print(\"using mean positive prediction threshold\")\n",
    "# dev_mean_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "#                                            mean_pos_scores,\n",
    "#                                            m_cfg['max_en_pred'])\n",
    "# p, r, _ = basic_precision_recall(single_dev_ref, dev_mean_pred_words)\n",
    "# print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_array = np.array([p_r_thresh[i]['p']/100 for i in p_r_thresh], dtype=\"f\")\n",
    "recall_array = np.array([p_r_thresh[i]['r']/100 for i in p_r_thresh], dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_array[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(precision_array[::-1], recall_array[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get preds and refs in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(train_pred_words, train_utts[\"refs\"]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_utt_preds_words = {}\n",
    "dev_utt_refs_words = {}\n",
    "for u, p, refs in zip(dev_utts['ids'], dev_pred_words, dev_utts[\"refs\"]):\n",
    "    dev_utt_preds_words[u] = list(set([bow_dict['i2w'][i].decode() for i in p]))\n",
    "    dev_utt_refs_words[u] = []\n",
    "    for r in refs:\n",
    "        #print(r)\n",
    "        dev_utt_refs_words[u].append([bow_dict['i2w'][i].decode() for i in set(r)])\n",
    "single_dev_ref_words = {u: [dev_utt_refs_words[u][0]] for u in dev_utt_refs_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, metric = basic_precision_recall(list(dev_utt_refs_words.values()), list(dev_utt_preds_words.values()))\n",
    "p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, rs, _ = basic_precision_recall(single_dev_ref_words.values(), dev_utt_preds_words.values())\n",
    "ps, rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, metric[k]) for k in ['rc', 'rt', 'tp', 'tc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_correctly_predicted = [item for item in metric[\"word\"].items() if item[1]['tc'] > 0]\n",
    "print(len(words_correctly_predicted))\n",
    "display(words_correctly_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common train words\n",
    "[w.decode() for w, f in sorted(bow_dict['freq'].items(), reverse=True, key=lambda t: t[1])][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(single_dev_ref_words.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_bow_words(single_dev_ref_words, \n",
    "                  dev_utt_preds_words, \n",
    "                  bow_dict, \n",
    "                  map_dict[\"fisher_dev\"], display_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval refs found, loading\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=84.85, recall=69.54\n",
      "--------------------\n",
      "39 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=92.82, recall=87.38\n",
      "--------------------\n",
      "36 out of 39 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=76.60, recall=67.74\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=45.204494\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "3977 out of 3979 have len >= 1\n",
      "BLEU=45.21\n",
      "precision=76.60, recall=67.74\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"\", use_google=True, min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval refs found, loading\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=84.30, recall=84.01\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "afternoon -- little -- since -- around -- would -- family -- church -- believe -- always -- chicago -- thank -- puerto -- change -- wants -- every -- religion -- catholic -- women -- think -- something -- colombia -- problem -- beautiful -- going -- exactly -- people -- sometimes -- thinking -- close -- children -- remember -- first -- waiting -- thing -- american -- topic -- talking -- thought -- coming -- right -- nothing -- anything -- woman -- almost -- states -- years -- united -- married -- twenty -- example -- things -- better -- spanish -- working -- school -- seeing -- brothers -- stayed -- still -- living -- quiet -- florida -- small -- heard -- house -- already -- getting -- brother -- morning -- everything -- night -- father -- university -- called -- college -- hours -- problems -- husband -- another -- visit -- friends -- comes -- speak -- general -- important -- happen -- attention -- moment -- words -- happy -- person -- happened -- never -- really -- world -- interesting -- truth -- needs -- least -- someone -- understand -- course -- child -- maybe -- imagine -- whatever -- calls -- asked -- leave -- crazy -- matter -- different -- either -- daughter -- saying -- looking -- listening -- philadelphia -- everybody -- wanted -- program -- without -- talked -- study -- times -- start -- happens -- mexican -- helps -- mexico -- lives -- makes -- center -- everyone -- usually -- brought -- possible -- sorry -- interested -- bring -- hello -- three -- email -- bought -- terrible -- classes -- class -- neither -- enough -- money -- student -- security -- giving -- studying -- places -- history -- months -- lived -- watch -- worry -- spain -- super -- reason -- spend -- state -- phone -- north -- today -- calling -- eight -- difference -- horrible -- especially -- travel -- parents -- country -- english -- parts -- started -- sleep -- music -- month -- hundred -- listen -- knows -- canada -- place -- outside -- found -- could -- difficult -- spoke -- telling -- neighborhood -- relax -- alone -- anywhere -- salsa -- dance -- dollars -- fifteen -- great -- listened -- rican -- idaho -- check -- reggaeton -- finish -- moved -- careful -- forty -- dominican -- mother -- dominicans -- ricans -- starts -- traveling -- countries -- white -- dangerous -- colorado -- making -- crime -- philly\n",
      "number of words: 224\n",
      "--------------------\n",
      "209 out of 224 retrieved with atleast 1 correct instance\n",
      "eval refs found, loading\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=91.03, recall=69.43\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "uncle -- pants -- sisters -- showing -- carmen -- november -- husbands -- dances -- earrings -- nieces -- fanatic -- cousin -- organized -- miracle -- baptized -- strict -- belief -- negative -- discuss -- cousins -- occasions -- celebrate -- supreme -- moments -- philosophy -- suffering -- circumstances -- karma -- reaction -- action -- activities -- actions -- serve -- develop -- krishna -- daily -- temple -- sundays -- knowledge -- jesus -- christ -- india -- planet -- spiritual -- considered -- material -- death -- norma -- institution -- hindu -- alcohol -- coffee -- development -- caribbean -- hotmail -- stories -- friday -- josefina -- cruises -- names -- thousands -- filters -- article -- microsoft -- millions -- possibly -- yahoo -- division -- pieces -- lyrics -- protect -- filter -- marketing -- ernestina -- concert -- ticket -- product -- breaks -- quality -- treating -- assistance -- grandfather -- gmail -- teenager -- books -- tickets -- concerts -- sleeps -- hears -- sleeping -- piano -- movements -- birth -- lessons -- grows -- educational -- floor -- jenny -- favorite -- court -- animal -- previous -- traditional -- queen -- bachata -- recording -- latino -- cristina -- daddy -- betsy -- pills -- welfare -- abusing -- search -- limit -- sitting -- survive -- include -- healthy -- humidity -- expecting -- dirty -- sleepy -- annie -- alaska -- construction -- danger -- tourist -- trips -- weight -- arizona -- uncomfortable -- grand -- monterrey -- falls -- pacific -- seattle -- montreal -- developed -- cruise -- atlantic -- beauty -- fantastic -- robbed -- roman -- damaged -- chemistry -- plane -- constantly -- sunny -- report -- sweet -- hungry -- separate -- citizenship -- citizens -- flight -- kansas -- australia -- mildred -- relaxing -- japanese -- aggressive -- instrument -- guitar -- arturo -- doors -- crimes -- denver -- camping -- steal -- familiar -- relaxed -- block -- highway -- windows -- buildings -- swear -- kitchen -- criminals -- impressive -- catch -- march -- dressed -- campus -- columbia -- paradise -- penalty -- sells -- medicare -- points -- letter -- ignorance -- yellow -- professional -- goodbye -- professionals -- embarrassed -- fault -- ignorant -- criticize -- surprise -- tennessee -- cents -- tonight -- david -- theory -- translator\n",
      "number of words: 208\n",
      "--------------------\n",
      "155 out of 208 retrieved with atleast 1 correct instance\n",
      "eval refs found, loading\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=88.65, recall=80.37\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "around -- always -- chicago -- thank -- puerto -- something -- colombia -- going -- exactly -- people -- carmen -- better -- spanish -- school -- family -- quiet -- florida -- house -- university -- college -- jenny -- ricardo -- barranquilla -- think -- speak -- pennsylvania -- general -- everything -- person -- still -- cultural -- maybe -- connecticut -- milgred -- mildred -- hello -- right -- thing -- crazy -- mercedes -- philadelphia -- please -- everybody -- karma -- georgia -- atlanta -- krishna -- center -- india -- material -- american -- norma -- arturo -- alcohol -- group -- three -- email -- hotmail -- terrible -- josefina -- website -- internet -- money -- software -- microsoft -- social -- break -- emails -- attachment -- yahoo -- marketing -- telemundo -- close -- history -- excuse -- computers -- shudong -- ernestina -- ticket -- sorry -- vancouver -- south -- state -- virginia -- north -- federal -- horrible -- argentina -- legal -- miguel -- gmail -- teenager -- personal -- amazon -- tickets -- machine -- sound -- washington -- point -- piano -- development -- maternity -- playstation -- channel -- goodness -- discovery -- miami -- annie -- animal -- radio -- nebraska -- manhattan -- texmex -- texas -- mariachi -- regular -- hospital -- relax -- houston -- salsa -- health -- manuel -- elvis -- crespo -- idaho -- boricua -- chile -- bachata -- merengue -- cumbias -- simple -- topic -- latino -- california -- cristina -- queen -- daddy -- betsy -- salvador -- welfare -- tammy -- utilities -- sixty -- honduras -- bronx -- jamaica -- pesos -- alaska -- guatemala -- costa -- vallarta -- chiapas -- oaxaca -- aruba -- sulma -- white -- doctor -- arizona -- wisconsin -- grand -- monterrey -- durango -- industrial -- falls -- montreal -- hawaii -- super -- israel -- colorado -- sweet -- miles -- hotel -- kansas -- australia -- corridos -- civil -- global -- denver -- camping -- santiago -- meeting -- ballet -- highway -- philly -- highrises -- baltimore -- cornell -- brutal -- upenn -- racial -- undergraduate -- express -- olaya -- campus -- ghetto -- javier -- british -- columbia -- ecuador -- medicare -- video -- jennifer -- interracial -- meant -- republicans -- santo -- domingo -- apart -- chatrooms -- honey -- fault -- dating -- flexible -- david\n",
      "number of words: 214\n",
      "--------------------\n",
      "164 out of 214 retrieved with atleast 1 correct instance\n",
      "eval refs found, loading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=92.82, recall=87.38\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "change -- time -- women -- people -- remember -- first -- waiting -- name -- years -- live -- home -- coming -- house -- morning -- another -- even -- life -- girl -- want -- help -- need -- send -- city -- lives -- terrible -- news -- saying -- someone -- case -- watch -- love -- stay -- town -- leave -- give -- found -- high -- make -- huge\n",
      "number of words: 39\n",
      "--------------------\n",
      "36 out of 39 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "metrics = eval_all_word_lists(\"\", use_google=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 150 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict_path = os.path.join(m_cfg['data_path'], \"mix_sim.dict\")\n",
    "sim_dict = pickle.load(open(sim_dict_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_dict_es['freq_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in sim_dict[\"w\"]:\n",
    "#     if len(sim_dict[\"w\"][w]) > 1 and w in bow_dict_es[\"w2i\"]:\n",
    "#         print(w)\n",
    "#         print(sim_dict[\"w\"][w])\n",
    "#         print(bow_dict_es[\"w2i\"][w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=76.34, recall=55.09\n",
      "--------------------\n",
      "37 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=83.82, recall=68.71\n",
      "--------------------\n",
      "36 out of 39 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=64.54, recall=54.73\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=27.794072\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "3977 out of 3977 have len >= 1\n",
      "BLEU=27.79\n",
      "precision=64.54, recall=54.73\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\", min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=75.48, recall=67.89\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "afternoon -- spanish -- working -- person -- school -- going -- puerto -- brothers -- first -- heard -- chicago -- brother -- university -- called -- college -- small -- family -- colombia -- another -- visit -- friends -- sometimes -- think -- comes -- talked -- people -- catholic -- everything -- important -- things -- always -- thank -- money -- something -- problem -- church -- house -- women -- beautiful -- exactly -- attention -- place -- thinking -- nothing -- anything -- religion -- married -- years -- bought -- united -- example -- states -- thing -- better -- already -- stayed -- children -- still -- husband -- quiet -- florida -- speak -- right -- since -- rican -- coming -- living -- spoke -- parents -- little -- mother -- believe -- morning -- night -- talking -- father -- twenty -- hours -- problems -- remember -- every -- around -- could -- change -- wants -- waiting -- wanted -- thought -- really -- woman -- almost -- topic -- everybody -- saying -- moment -- leave -- english -- least -- hundred -- matter -- seeing -- everyone -- would -- dance -- never -- asked -- world -- helps -- understand -- different -- daughter -- difference -- country -- program -- philadelphia -- happened -- watch -- interesting -- truth -- needs -- course -- maybe -- imagine -- child -- whatever -- either -- american -- times -- calling -- words -- happy -- happen -- starts -- close -- mexican -- lives -- center -- brought -- interested -- dominican -- alone -- study -- start -- places -- mexico -- possible -- hello -- calls -- getting -- three -- spend -- history -- happens -- email -- bring -- someone -- white -- class -- classes -- month -- terrible -- north -- enough -- worry -- general -- student -- without -- studying -- looking -- spain -- sorry -- super -- phone -- knows -- eight -- reason -- state -- crazy -- especially -- neither -- lived -- music -- listening -- listen -- started -- sleep -- months -- moved -- today -- outside -- finish -- telling -- neighborhood -- relax -- dollars -- fifteen -- found -- dominicans -- salsa -- idaho -- great -- reggaeton -- listened -- careful -- canada -- check -- difficult -- makes -- ricans -- forty -- usually -- giving -- travel -- anywhere -- countries -- traveling -- parts -- dangerous -- security -- horrible -- colorado -- making -- crime -- philly\n",
      "number of words: 224\n",
      "--------------------\n",
      "186 out of 224 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=88.78, recall=37.85\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- professional -- sisters -- november -- husbands -- uncle -- earrings -- pants -- nieces -- fanatic -- showing -- cousins -- occasions -- celebrate -- dances -- cousin -- miracle -- baptized -- belief -- negative -- discuss -- organized -- strict -- norma -- philosophy -- hindu -- alcohol -- coffee -- develop -- supreme -- suffering -- circumstances -- karma -- reaction -- action -- activities -- actions -- serve -- temple -- sundays -- knowledge -- jesus -- christ -- krishna -- india -- planet -- spiritual -- considered -- material -- development -- death -- previous -- institution -- josefina -- ernestina -- caribbean -- cruises -- yahoo -- division -- pieces -- protect -- friday -- marketing -- hotmail -- names -- thousands -- article -- microsoft -- millions -- possibly -- filters -- filter -- gmail -- books -- tickets -- concerts -- concert -- ticket -- product -- breaks -- quality -- treating -- assistance -- grandfather -- floor -- sleeps -- hears -- sleeping -- educational -- favorite -- jenny -- moments -- court -- animal -- piano -- movements -- birth -- lessons -- teenager -- traditional -- latino -- cristina -- recording -- bachata -- daddy -- welfare -- limit -- survive -- pills -- abusing -- search -- betsy -- sitting -- include -- sleepy -- annie -- dirty -- expecting -- healthy -- humidity -- trips -- arizona -- alaska -- tourist -- construction -- uncomfortable -- grand -- monterrey -- danger -- weight -- robbed -- roman -- fantastic -- damaged -- seattle -- montreal -- developed -- cruise -- atlantic -- pacific -- beauty -- falls -- queen -- australia -- constantly -- separate -- flight -- kansas -- chemistry -- plane -- report -- sweet -- hungry -- sunny -- citizenship -- citizens -- mildred -- arturo -- japanese -- relaxing -- aggressive -- stories -- doors -- daily -- guitar -- instrument -- lyrics -- denver -- buildings -- crimes -- steal -- grows -- block -- highway -- windows -- camping -- relaxed -- familiar -- swear -- contrary -- kitchen -- criminals -- collect -- campus -- impressive -- catch -- march -- dressed -- letter -- medicare -- points -- columbia -- paradise -- penalty -- sells -- fault -- ignorant -- criticize -- professionals -- ignorance -- yellow -- embarrassed -- goodbye -- david -- theory -- translator -- surprise -- tennessee -- cents -- tonight\n",
      "number of words: 210\n",
      "--------------------\n",
      "91 out of 210 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=82.64, recall=63.68\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- spanish -- person -- school -- going -- puerto -- chicago -- university -- college -- ricardo -- barranquilla -- family -- colombia -- think -- pennsylvania -- people -- everything -- always -- thank -- money -- something -- exactly -- better -- right -- quiet -- florida -- house -- around -- connecticut -- milgred -- legal -- mercedes -- philadelphia -- still -- maybe -- cultural -- georgia -- norma -- atlanta -- american -- alcohol -- group -- texas -- social -- karma -- close -- internet -- center -- thing -- krishna -- india -- material -- development -- simple -- topic -- three -- point -- history -- josefina -- please -- hello -- excuse -- email -- speak -- ernestina -- emails -- attachment -- white -- yahoo -- telemundo -- marketing -- computers -- shudong -- break -- hotmail -- terrible -- north -- website -- everybody -- software -- microsoft -- general -- gmail -- miguel -- amazon -- tickets -- ticket -- argentina -- sorry -- personal -- state -- virginia -- federal -- miami -- annie -- machine -- sound -- maternity -- channel -- discovery -- jenny -- radio -- animal -- piano -- crazy -- teenager -- playstation -- goodness -- hospital -- manuel -- nebraska -- regular -- relax -- houston -- salsa -- texmex -- mariachi -- south -- chile -- merengue -- latino -- california -- cristina -- idaho -- bachata -- daddy -- video -- elvis -- crespo -- boricua -- cumbias -- tammy -- welfare -- betsy -- utilities -- salvador -- sixty -- honduras -- bronx -- jamaica -- pesos -- sulma -- arizona -- alaska -- guatemala -- costa -- vallarta -- chiapas -- aruba -- wisconsin -- grand -- monterrey -- durango -- industrial -- doctor -- horrible -- washington -- montreal -- super -- israel -- falls -- hawaii -- queen -- colorado -- australia -- hotel -- kansas -- oaxaca -- sweet -- miles -- mildred -- arturo -- global -- civil -- denver -- highway -- camping -- santiago -- ballet -- highrises -- philly -- cornell -- brutal -- baltimore -- olaya -- campus -- health -- racial -- upenn -- ghetto -- express -- undergraduate -- javier -- meeting -- vancouver -- ecuador -- medicare -- british -- columbia -- republicans -- santo -- domingo -- apart -- fault -- jennifer -- interracial -- manhattan -- chatrooms -- honey -- meant -- david -- flexible\n",
      "number of words: 212\n",
      "--------------------\n",
      "122 out of 212 retrieved with atleast 1 correct instance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=83.82, recall=68.71\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "name -- live -- house -- first -- time -- home -- another -- people -- even -- women -- coming -- stay -- morning -- years -- change -- remember -- waiting -- want -- send -- help -- life -- girl -- lives -- city -- need -- saying -- someone -- terrible -- news -- give -- love -- watch -- town -- high -- make -- found -- leave -- huge -- case\n",
      "number of words: 39\n",
      "--------------------\n",
      "36 out of 39 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "metrics = eval_all_word_lists(\"sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\", use_google=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU script\n",
    "```\n",
    "[bonnybridge]s1444673: export BLEU_SCRIPT=/afs/inf.ed.ac.uk/group/project/lowres/work/installs/mosesdecoder/scripts/generic/multi-bleu.perl\n",
    "[bonnybridge]s1444673: export PREDS=sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "BLEU = 29.44, 65.1/38.4/22.8/13.7 (BP=0.991, ratio=0.991, hyp_len=39719, ref_len=40096)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_test_fisher_test_en.ref* < $PREDS/fsh_test_fisher_test_hyp\n",
    "BLEU = 29.64, 66.2/38.4/22.7/13.5 (BP=0.999, ratio=0.999, hyp_len=39201, ref_len=39257)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref[0,1,2]* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "\n",
    "BLEU = 27.03, 62.7/35.8/20.7/12.1 (BP=0.987, ratio=0.987, hyp_len=39719, ref_len=40242)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref[1,2,3]* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "BLEU = 27.03, 62.6/35.8/20.8/12.2 (BP=0.984, ratio=0.984, hyp_len=39719, ref_len=40353)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref[2,3,0]* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "BLEU = 27.00, 62.8/35.9/20.8/12.1 (BP=0.984, ratio=0.984, hyp_len=39719, ref_len=40346)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref[1,3,0]* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "BLEU = 27.10, 62.9/35.9/20.8/12.2 (BP=0.985, ratio=0.985, hyp_len=39719, ref_len=40339)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 50 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_metrics = eval_nmt_model(\"./sp2enw_mel-80_vocab-nltk/sp_0.33_h-256_e-128_l2e-3_lstm_drpt-0.3_cnn-32-2-2_rnn-3_b-40-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 25 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw_mel-80_vocab-nltk/sp_0.16_h-256_e-128_l2e-3_lstm_drpt-0.3_cnn-32-2-2_rnn-3_b-80-25_no-ln-bn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 15 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw/sp_.10/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 50 hours model - sample word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed: 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.33_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.33_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_sample/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed: AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new50_metrics = eval_nmt_model(\"emb_sp2enw/sp_0.33_seed-AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new50_metrics = eval_nmt_model(\"emb_sp2enw/sp_0.33_seed-AA_mix-0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 80 hours model - sample word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.50_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.50_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_sample/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interspeech results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=59.87, recall=36.76\n",
      "--------------------\n",
      "35 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=68.80, recall=45.65\n",
      "--------------------\n",
      "33 out of 39 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=49.07, recall=41.42\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=13.856443\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "3977 out of 3977 have len >= 1\n",
      "BLEU=13.86\n",
      "precision=49.07, recall=41.42\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\", min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=53.07, recall=42.95\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "afternoon -- spanish -- working -- school -- puerto -- first -- heard -- chicago -- brother -- phone -- university -- called -- college -- calls -- woman -- going -- married -- family -- colombia -- another -- philadelphia -- hello -- visit -- friends -- sometimes -- comes -- think -- times -- problem -- people -- everything -- important -- saying -- matter -- things -- something -- always -- thank -- years -- remember -- church -- women -- exactly -- beautiful -- attention -- thinking -- nothing -- wants -- little -- religion -- united -- example -- states -- better -- looking -- brothers -- children -- still -- husband -- florida -- lived -- study -- stayed -- since -- coming -- lives -- house -- father -- catholic -- anything -- believe -- morning -- least -- talking -- happens -- twenty -- hours -- speak -- problems -- understand -- country -- state -- every -- around -- could -- would -- change -- waiting -- american -- wanted -- topic -- today -- already -- almost -- happy -- interested -- daughter -- thing -- mother -- moment -- person -- fifteen -- three -- hundred -- seeing -- usually -- close -- everyone -- never -- world -- money -- start -- different -- thought -- finish -- happened -- interesting -- truth -- needs -- right -- course -- maybe -- imagine -- child -- someone -- spoke -- talked -- places -- north -- super -- really -- helps -- mexican -- happen -- watch -- whatever -- terrible -- center -- telling -- brought -- living -- neighborhood -- place -- difference -- colorado -- mexico -- possible -- studying -- program -- giving -- history -- dollars -- email -- travel -- found -- month -- class -- classes -- enough -- general -- student -- without -- traveling -- spain -- bought -- sorry -- calling -- making -- started -- careful -- eight -- english -- words -- reason -- spend -- worry -- especially -- parents -- neither -- music -- listening -- sleep -- months -- night -- knows -- small -- listen -- rican -- crazy -- quiet -- moved -- either -- leave -- canada -- asked -- relax -- dance -- salsa -- bring -- countries -- idaho -- great -- listened -- outside -- makes -- check -- difficult -- horrible -- everybody -- dominican -- ricans -- white -- alone -- parts -- dangerous -- security -- crime -- getting -- reggaeton -- anywhere -- philly -- forty -- dominicans -- starts\n",
      "number of words: 224\n",
      "--------------------\n",
      "149 out of 224 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=62.07, recall=7.42\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- sisters -- november -- husbands -- uncle -- earrings -- pants -- nieces -- fanatic -- showing -- cousins -- occasions -- celebrate -- dances -- cousin -- miracle -- baptized -- belief -- negative -- discuss -- organized -- strict -- norma -- philosophy -- hindu -- alcohol -- coffee -- develop -- supreme -- suffering -- circumstances -- karma -- caribbean -- reaction -- action -- activities -- actions -- serve -- temple -- sundays -- knowledge -- jesus -- christ -- krishna -- india -- planet -- spiritual -- considered -- report -- material -- death -- institution -- chemistry -- josefina -- ernestina -- cruises -- yahoo -- division -- ticket -- pieces -- protect -- friday -- marketing -- hotmail -- names -- thousands -- article -- microsoft -- millions -- possibly -- filters -- filter -- gmail -- books -- tickets -- concert -- breaks -- quality -- treating -- product -- grandfather -- floor -- sleeps -- hears -- sleeping -- educational -- favorite -- jenny -- moments -- court -- animal -- piano -- movements -- development -- birth -- lessons -- collect -- teenager -- traditional -- previous -- latino -- cristina -- recording -- bachata -- daddy -- concerts -- welfare -- assistance -- limit -- survive -- pills -- abusing -- search -- betsy -- sitting -- include -- sleepy -- annie -- dirty -- expecting -- healthy -- humidity -- trips -- arizona -- alaska -- tourist -- construction -- uncomfortable -- grand -- monterrey -- danger -- weight -- robbed -- roman -- fantastic -- damaged -- seattle -- montreal -- developed -- cruise -- atlantic -- pacific -- beauty -- falls -- queen -- australia -- constantly -- separate -- flight -- kansas -- plane -- sweet -- hungry -- sunny -- citizenship -- citizens -- mildred -- arturo -- japanese -- relaxing -- aggressive -- stories -- daily -- guitar -- instrument -- lyrics -- denver -- crimes -- steal -- grows -- block -- highway -- windows -- buildings -- doors -- camping -- relaxed -- familiar -- swear -- contrary -- kitchen -- criminals -- campus -- impressive -- catch -- march -- dressed -- letter -- medicare -- points -- columbia -- paradise -- penalty -- sells -- fault -- ignorant -- criticize -- professionals -- ignorance -- yellow -- professional -- embarrassed -- goodbye -- david -- theory -- translator -- surprise -- tennessee -- cents -- tonight\n",
      "number of words: 210\n",
      "--------------------\n",
      "23 out of 210 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=60.99, recall=39.08\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- spanish -- school -- puerto -- chicago -- university -- college -- ricardo -- barranquilla -- going -- family -- colombia -- philadelphia -- hello -- think -- pennsylvania -- people -- everything -- something -- always -- thank -- exactly -- houston -- better -- right -- florida -- house -- around -- connecticut -- milgred -- radio -- internet -- person -- hospital -- thing -- mercedes -- still -- maybe -- cultural -- georgia -- norma -- atlanta -- american -- alcohol -- group -- super -- karma -- north -- center -- krishna -- india -- material -- money -- state -- bronx -- close -- three -- history -- josefina -- please -- email -- speak -- ernestina -- doctor -- attachment -- yahoo -- ticket -- telemundo -- marketing -- topic -- computers -- shudong -- break -- hotmail -- manhattan -- terrible -- website -- software -- microsoft -- emails -- general -- gmail -- chile -- miguel -- amazon -- tickets -- argentina -- sorry -- personal -- virginia -- federal -- legal -- miami -- annie -- machine -- sound -- maternity -- channel -- discovery -- jenny -- animal -- point -- piano -- development -- crazy -- quiet -- teenager -- playstation -- goodness -- texas -- manuel -- nebraska -- regular -- relax -- salsa -- guatemala -- texmex -- mariachi -- washington -- health -- merengue -- latino -- california -- cristina -- idaho -- bachata -- daddy -- elvis -- crespo -- boricua -- cumbias -- tammy -- welfare -- horrible -- betsy -- social -- utilities -- salvador -- everybody -- honduras -- jamaica -- pesos -- sulma -- arizona -- alaska -- south -- costa -- vallarta -- chiapas -- aruba -- white -- wisconsin -- grand -- monterrey -- durango -- industrial -- montreal -- israel -- falls -- hawaii -- queen -- colorado -- hotel -- australia -- sixty -- kansas -- oaxaca -- sweet -- miles -- mildred -- arturo -- global -- civil -- denver -- highway -- camping -- santiago -- ballet -- highrises -- philly -- cornell -- brutal -- baltimore -- olaya -- campus -- racial -- upenn -- ghetto -- express -- undergraduate -- javier -- meeting -- vancouver -- ecuador -- medicare -- video -- british -- columbia -- simple -- republicans -- santo -- domingo -- apart -- fault -- jennifer -- interracial -- excuse -- chatrooms -- honey -- meant -- david -- flexible\n",
      "number of words: 212\n",
      "--------------------\n",
      "74 out of 212 retrieved with atleast 1 correct instance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=68.80, recall=45.65\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "name -- live -- case -- first -- time -- home -- another -- people -- even -- women -- give -- coming -- lives -- house -- morning -- years -- change -- want -- remember -- waiting -- girl -- help -- love -- life -- city -- need -- someone -- saying -- send -- stay -- terrible -- news -- watch -- found -- town -- make -- high -- leave -- huge\n",
      "number of words: 39\n",
      "--------------------\n",
      "33 out of 39 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "_ = eval_all_word_lists(\"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=62.27, recall=35.69\n",
      "--------------------\n",
      "34 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=71.06, recall=44.84\n",
      "--------------------\n",
      "30 out of 39 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=50.58, recall=42.40\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=14.310820\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "3977 out of 3977 have len >= 1\n",
      "BLEU=14.31\n",
      "precision=50.58, recall=42.40\n"
     ]
    }
   ],
   "source": [
    "model_50_da = eval_nmt_model(\"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5_sample-mix-0.5\", \n",
    "                             min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=57.64, recall=44.47\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "afternoon -- spanish -- working -- person -- school -- puerto -- brother -- first -- heard -- chicago -- phone -- university -- called -- college -- house -- music -- family -- colombia -- another -- going -- visit -- friends -- sometimes -- people -- everything -- think -- important -- matter -- things -- always -- thank -- something -- problem -- church -- exactly -- money -- attention -- thinking -- different -- course -- really -- nothing -- religion -- married -- years -- twenty -- united -- example -- states -- better -- right -- thing -- brothers -- stayed -- children -- would -- still -- husband -- florida -- history -- coming -- living -- wants -- catholic -- women -- parents -- anything -- problems -- believe -- morning -- night -- already -- talking -- hours -- remember -- every -- around -- understand -- change -- looking -- waiting -- american -- email -- daughter -- almost -- topic -- everyone -- giving -- lives -- least -- seeing -- close -- little -- small -- started -- never -- talked -- world -- needs -- since -- could -- place -- places -- difference -- philadelphia -- happened -- countries -- interesting -- maybe -- imagine -- child -- whatever -- listen -- times -- class -- happens -- moment -- interested -- without -- someone -- happy -- terrible -- helps -- happen -- center -- moved -- brought -- woman -- saying -- general -- mexican -- country -- mexico -- canada -- possible -- study -- hello -- calls -- program -- three -- speak -- spoke -- comes -- thought -- classes -- enough -- worry -- outside -- student -- studying -- spain -- bought -- found -- wanted -- sorry -- telling -- calling -- months -- eight -- reason -- english -- words -- spend -- state -- especially -- neither -- lived -- listening -- sleep -- start -- month -- watch -- travel -- crazy -- quiet -- finish -- knows -- either -- crime -- asked -- neighborhood -- hundred -- relax -- dance -- dollars -- fifteen -- starts -- bring -- salsa -- idaho -- great -- listened -- ricans -- today -- careful -- check -- mother -- dominican -- rican -- father -- traveling -- difficult -- getting -- white -- beautiful -- dangerous -- security -- north -- super -- colorado -- truth -- makes -- parts -- leave -- horrible -- reggaeton -- making -- anywhere -- philly -- alone -- forty -- everybody -- dominicans -- usually\n",
      "number of words: 224\n",
      "--------------------\n",
      "155 out of 224 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=78.57, recall=4.35\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- sisters -- november -- husbands -- uncle -- earrings -- pants -- nieces -- fanatic -- showing -- cousins -- occasions -- celebrate -- dances -- cousin -- miracle -- baptized -- belief -- negative -- discuss -- organized -- strict -- norma -- philosophy -- hindu -- alcohol -- coffee -- develop -- supreme -- suffering -- circumstances -- karma -- reaction -- action -- activities -- actions -- serve -- temple -- sundays -- knowledge -- jesus -- christ -- krishna -- india -- planet -- spiritual -- considered -- material -- death -- institution -- chemistry -- josefina -- ernestina -- caribbean -- cruises -- yahoo -- division -- pieces -- protect -- friday -- marketing -- hotmail -- names -- thousands -- article -- microsoft -- millions -- possibly -- filters -- filter -- gmail -- books -- tickets -- concert -- ticket -- breaks -- quality -- treating -- product -- grandfather -- floor -- sleeps -- hears -- sleeping -- educational -- favorite -- jenny -- moments -- court -- animal -- piano -- movements -- development -- birth -- lessons -- teenager -- traditional -- previous -- latino -- cristina -- recording -- bachata -- daddy -- concerts -- welfare -- assistance -- limit -- survive -- pills -- abusing -- search -- betsy -- sitting -- include -- sleepy -- annie -- dirty -- expecting -- healthy -- humidity -- trips -- arizona -- alaska -- tourist -- construction -- uncomfortable -- grand -- monterrey -- danger -- weight -- robbed -- roman -- fantastic -- damaged -- seattle -- montreal -- developed -- cruise -- atlantic -- pacific -- beauty -- falls -- queen -- australia -- constantly -- separate -- flight -- kansas -- plane -- report -- sweet -- hungry -- sunny -- citizenship -- citizens -- mildred -- arturo -- japanese -- relaxing -- aggressive -- stories -- daily -- guitar -- instrument -- lyrics -- denver -- crimes -- steal -- grows -- block -- highway -- windows -- buildings -- doors -- camping -- relaxed -- familiar -- swear -- contrary -- kitchen -- criminals -- collect -- campus -- impressive -- catch -- march -- dressed -- letter -- medicare -- points -- columbia -- paradise -- penalty -- sells -- fault -- ignorant -- criticize -- professionals -- ignorance -- yellow -- professional -- embarrassed -- goodbye -- david -- theory -- translator -- surprise -- tennessee -- cents -- tonight\n",
      "number of words: 210\n",
      "--------------------\n",
      "14 out of 210 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=63.90, recall=39.50\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- spanish -- person -- school -- puerto -- chicago -- university -- college -- ricardo -- barranquilla -- house -- argentina -- family -- colombia -- going -- pennsylvania -- guatemala -- people -- everything -- think -- always -- thank -- something -- exactly -- better -- right -- thing -- florida -- history -- doctor -- around -- connecticut -- milgred -- speak -- south -- mercedes -- philadelphia -- still -- maybe -- cultural -- georgia -- norma -- atlanta -- american -- alcohol -- group -- karma -- internet -- center -- krishna -- india -- material -- general -- computers -- close -- three -- josefina -- please -- hello -- email -- ernestina -- hospital -- money -- attachment -- yahoo -- telemundo -- marketing -- topic -- shudong -- break -- hotmail -- point -- terrible -- website -- software -- microsoft -- emails -- gmail -- miguel -- amazon -- tickets -- ticket -- sorry -- personal -- state -- virginia -- federal -- legal -- miami -- machine -- sound -- maternity -- channel -- discovery -- jenny -- radio -- animal -- piano -- development -- crazy -- quiet -- teenager -- playstation -- goodness -- texas -- manuel -- nebraska -- regular -- relax -- houston -- texmex -- mariachi -- bronx -- salsa -- chile -- merengue -- latino -- california -- cristina -- idaho -- manhattan -- bachata -- daddy -- elvis -- crespo -- boricua -- cumbias -- tammy -- welfare -- betsy -- social -- utilities -- salvador -- annie -- honduras -- jamaica -- pesos -- sulma -- arizona -- alaska -- costa -- vallarta -- chiapas -- aruba -- white -- wisconsin -- grand -- monterrey -- durango -- industrial -- north -- washington -- montreal -- super -- israel -- falls -- hawaii -- queen -- colorado -- australia -- hotel -- kansas -- horrible -- oaxaca -- sweet -- miles -- mildred -- arturo -- global -- civil -- denver -- highway -- camping -- santiago -- ballet -- highrises -- philly -- cornell -- brutal -- baltimore -- olaya -- campus -- racial -- upenn -- ghetto -- everybody -- express -- undergraduate -- javier -- meeting -- vancouver -- ecuador -- medicare -- video -- british -- columbia -- health -- simple -- republicans -- santo -- domingo -- apart -- fault -- jennifer -- interracial -- excuse -- chatrooms -- honey -- meant -- david -- sixty -- flexible\n",
      "number of words: 212\n",
      "--------------------\n",
      "66 out of 212 retrieved with atleast 1 correct instance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=71.06, recall=44.84\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "name -- live -- case -- first -- time -- home -- house -- another -- people -- even -- coming -- women -- morning -- years -- change -- want -- remember -- help -- waiting -- give -- life -- girl -- someone -- lives -- city -- make -- need -- saying -- send -- terrible -- news -- love -- watch -- town -- found -- high -- stay -- leave -- huge\n",
      "number of words: 39\n",
      "--------------------\n",
      "30 out of 39 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "_ = eval_all_word_lists(\"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5_sample-mix-0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=67.16, recall=44.03\n",
      "--------------------\n",
      "34 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=76.75, recall=56.45\n",
      "--------------------\n",
      "31 out of 39 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=55.74, recall=47.08\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=19.178108\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "3977 out of 3977 have len >= 1\n",
      "BLEU=19.18\n",
      "precision=55.74, recall=47.08\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\", min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=63.75, recall=54.13\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "afternoon -- spanish -- working -- person -- school -- puerto -- brothers -- first -- going -- heard -- chicago -- brother -- university -- called -- college -- family -- colombia -- another -- visit -- friends -- sometimes -- think -- comes -- talked -- house -- people -- important -- things -- something -- thing -- always -- thank -- problem -- little -- church -- married -- women -- right -- exactly -- attention -- thinking -- nothing -- anything -- religion -- today -- years -- bought -- united -- example -- states -- better -- children -- still -- florida -- stayed -- lived -- wanted -- since -- coming -- living -- catholic -- parents -- believe -- morning -- program -- north -- talking -- hours -- problems -- really -- country -- husband -- every -- around -- could -- change -- wants -- remember -- small -- waiting -- american -- would -- woman -- topic -- already -- almost -- everything -- speak -- english -- enough -- least -- seeing -- found -- different -- never -- asked -- world -- matter -- understand -- daughter -- neighborhood -- money -- finish -- close -- everyone -- difference -- philadelphia -- happened -- interesting -- truth -- needs -- course -- maybe -- imagine -- child -- someone -- without -- phone -- times -- reason -- everybody -- interested -- happy -- happen -- helps -- lives -- happens -- center -- brought -- worry -- alone -- especially -- study -- mother -- places -- canada -- mexican -- mexico -- possible -- studying -- calls -- three -- twenty -- spend -- spoke -- history -- hello -- email -- telling -- saying -- horrible -- thought -- class -- classes -- quiet -- words -- terrible -- either -- place -- giving -- general -- student -- moment -- traveling -- spain -- whatever -- sorry -- father -- calling -- month -- eight -- watch -- state -- neither -- music -- listening -- started -- sleep -- months -- night -- salsa -- hundred -- knows -- dangerous -- dance -- travel -- looking -- listen -- crazy -- start -- relax -- dollars -- fifteen -- bring -- outside -- idaho -- ricans -- listened -- great -- rican -- super -- moved -- careful -- difficult -- check -- dominican -- dominicans -- forty -- security -- leave -- countries -- starts -- beautiful -- white -- parts -- colorado -- makes -- reggaeton -- making -- crime -- anywhere -- philly -- usually -- getting\n",
      "number of words: 224\n",
      "--------------------\n",
      "163 out of 224 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=72.97, recall=15.35\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- sisters -- reaction -- november -- husbands -- uncle -- earrings -- pants -- nieces -- fanatic -- showing -- cousins -- occasions -- celebrate -- dances -- cousin -- miracle -- baptized -- belief -- negative -- discuss -- organized -- strict -- norma -- philosophy -- hindu -- alcohol -- coffee -- develop -- supreme -- suffering -- circumstances -- karma -- caribbean -- action -- activities -- actions -- serve -- survive -- temple -- sundays -- knowledge -- jesus -- christ -- krishna -- india -- planet -- spiritual -- considered -- material -- death -- institution -- josefina -- ernestina -- cruises -- yahoo -- division -- teenager -- pieces -- protect -- friday -- marketing -- hotmail -- names -- thousands -- article -- microsoft -- millions -- possibly -- filters -- filter -- gmail -- books -- tickets -- concerts -- concert -- ticket -- product -- breaks -- quality -- treating -- assistance -- grandfather -- floor -- latino -- sleeps -- hears -- sleeping -- educational -- favorite -- jenny -- moments -- court -- animal -- piano -- movements -- development -- plane -- birth -- lessons -- traditional -- previous -- cristina -- recording -- bachata -- daddy -- welfare -- limit -- pills -- abusing -- search -- betsy -- sitting -- include -- sleepy -- annie -- dirty -- expecting -- healthy -- humidity -- trips -- arizona -- alaska -- tourist -- construction -- uncomfortable -- grand -- monterrey -- danger -- weight -- robbed -- roman -- fantastic -- damaged -- seattle -- montreal -- developed -- cruise -- atlantic -- pacific -- beauty -- falls -- queen -- australia -- constantly -- separate -- flight -- kansas -- chemistry -- report -- sweet -- hungry -- sunny -- citizenship -- citizens -- mildred -- arturo -- japanese -- relaxing -- aggressive -- stories -- daily -- guitar -- instrument -- lyrics -- denver -- crimes -- steal -- grows -- block -- impressive -- highway -- windows -- buildings -- doors -- camping -- relaxed -- familiar -- swear -- contrary -- kitchen -- criminals -- collect -- campus -- theory -- catch -- march -- dressed -- letter -- medicare -- points -- columbia -- paradise -- penalty -- sells -- fault -- ignorant -- criticize -- professionals -- ignorance -- yellow -- professional -- embarrassed -- goodbye -- david -- translator -- surprise -- tennessee -- cents -- tonight\n",
      "number of words: 210\n",
      "--------------------\n",
      "46 out of 210 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=70.76, recall=49.62\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- spanish -- person -- school -- puerto -- chicago -- university -- college -- jenny -- ricardo -- barranquilla -- family -- colombia -- think -- house -- pennsylvania -- people -- something -- thing -- always -- thank -- vancouver -- right -- exactly -- better -- doctor -- going -- florida -- virginia -- around -- connecticut -- milgred -- everything -- speak -- merengue -- mercedes -- philadelphia -- still -- hospital -- maybe -- cultural -- georgia -- norma -- atlanta -- american -- alcohol -- group -- everybody -- karma -- north -- center -- bronx -- krishna -- india -- material -- social -- money -- close -- three -- history -- josefina -- please -- hello -- email -- ernestina -- argentina -- horrible -- attachment -- yahoo -- teenager -- telemundo -- marketing -- computers -- shudong -- break -- hotmail -- terrible -- hotel -- website -- internet -- software -- microsoft -- emails -- general -- radio -- gmail -- miguel -- amazon -- tickets -- ticket -- sorry -- personal -- state -- federal -- chile -- legal -- miami -- annie -- machine -- sound -- salsa -- maternity -- channel -- discovery -- animal -- point -- piano -- development -- crazy -- quiet -- playstation -- goodness -- texas -- manuel -- civil -- nebraska -- regular -- relax -- houston -- texmex -- mariachi -- latino -- california -- cristina -- idaho -- topic -- bachata -- daddy -- elvis -- crespo -- boricua -- cumbias -- tammy -- welfare -- betsy -- utilities -- salvador -- honduras -- jamaica -- pesos -- sulma -- arizona -- alaska -- south -- guatemala -- costa -- vallarta -- chiapas -- aruba -- white -- wisconsin -- grand -- monterrey -- durango -- industrial -- washington -- montreal -- super -- israel -- falls -- hawaii -- queen -- colorado -- australia -- kansas -- oaxaca -- sweet -- miles -- mildred -- arturo -- medicare -- global -- denver -- highway -- camping -- santiago -- ballet -- highrises -- philly -- cornell -- brutal -- baltimore -- olaya -- campus -- ecuador -- racial -- upenn -- ghetto -- express -- undergraduate -- javier -- meeting -- video -- british -- columbia -- health -- simple -- republicans -- santo -- domingo -- apart -- fault -- jennifer -- interracial -- manhattan -- excuse -- chatrooms -- honey -- meant -- david -- sixty -- flexible\n",
      "number of words: 212\n",
      "--------------------\n",
      "93 out of 212 retrieved with atleast 1 correct instance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=76.75, recall=56.45\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "name -- live -- first -- time -- home -- another -- house -- people -- even -- women -- coming -- need -- morning -- years -- change -- want -- remember -- waiting -- give -- help -- life -- girl -- lives -- city -- make -- saying -- someone -- send -- terrible -- news -- love -- town -- watch -- stay -- found -- high -- leave -- huge -- case\n",
      "number of words: 39\n",
      "--------------------\n",
      "31 out of 39 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "_ = eval_all_word_lists(\"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=71.71, recall=45.49\n",
      "--------------------\n",
      "35 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=80.34, recall=57.26\n",
      "--------------------\n",
      "34 out of 39 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=56.72, recall=47.40\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=19.435366\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "3977 out of 3977 have len >= 1\n",
      "BLEU=19.44\n",
      "precision=56.72, recall=47.40\n"
     ]
    }
   ],
   "source": [
    "model_80_da = eval_nmt_model(\"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_mix-0.5/\", min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=68.02, recall=54.22\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "afternoon -- spanish -- working -- school -- seeing -- puerto -- brother -- first -- heard -- chicago -- lived -- university -- called -- college -- going -- family -- colombia -- another -- visit -- friends -- sometimes -- people -- think -- important -- things -- always -- thank -- something -- problem -- church -- married -- leave -- exactly -- attention -- thinking -- right -- really -- nothing -- religion -- years -- united -- example -- states -- thing -- moment -- better -- place -- brothers -- children -- still -- husband -- florida -- stayed -- spoke -- wanted -- since -- coming -- living -- house -- catholic -- women -- parents -- anything -- believe -- morning -- talking -- twenty -- hours -- could -- problems -- country -- every -- around -- change -- wants -- canada -- woman -- remember -- small -- waiting -- daughter -- either -- everything -- already -- almost -- understand -- everyone -- saying -- person -- start -- least -- would -- little -- never -- asked -- world -- matter -- thought -- english -- different -- bought -- finish -- close -- difference -- program -- philadelphia -- happened -- interesting -- truth -- needs -- course -- maybe -- imagine -- child -- whatever -- american -- talked -- times -- super -- helps -- happy -- happen -- lives -- center -- alone -- brought -- everybody -- words -- study -- mexican -- mexico -- possible -- hello -- calls -- three -- spend -- history -- happens -- telling -- email -- speak -- someone -- crime -- money -- idaho -- class -- classes -- quiet -- spain -- months -- terrible -- neighborhood -- horrible -- father -- enough -- calling -- giving -- general -- student -- without -- studying -- north -- sorry -- phone -- interested -- eight -- neither -- found -- reason -- state -- worry -- especially -- music -- listening -- started -- sleep -- night -- month -- knows -- hundred -- watch -- looking -- comes -- listen -- crazy -- relax -- salsa -- places -- dance -- dollars -- fifteen -- difficult -- bring -- moved -- great -- topic -- ricans -- listened -- careful -- check -- today -- mother -- dominican -- dominicans -- forty -- beautiful -- rican -- travel -- starts -- countries -- traveling -- white -- parts -- dangerous -- security -- makes -- colorado -- outside -- reggaeton -- making -- anywhere -- philly -- usually -- getting\n",
      "number of words: 224\n",
      "--------------------\n",
      "162 out of 224 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=76.14, recall=13.30\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- sisters -- november -- husbands -- uncle -- earrings -- pants -- nieces -- fanatic -- showing -- cousins -- occasions -- celebrate -- dances -- cousin -- miracle -- baptized -- belief -- negative -- discuss -- organized -- strict -- norma -- philosophy -- hindu -- alcohol -- coffee -- develop -- suffering -- supreme -- circumstances -- karma -- reaction -- action -- activities -- actions -- serve -- caribbean -- temple -- sundays -- knowledge -- jesus -- christ -- krishna -- kitchen -- india -- planet -- spiritual -- considered -- material -- death -- institution -- chemistry -- josefina -- ernestina -- cruises -- books -- yahoo -- division -- pieces -- protect -- friday -- marketing -- hotmail -- names -- search -- thousands -- article -- microsoft -- millions -- possibly -- filters -- filter -- gmail -- tickets -- concert -- ticket -- breaks -- quality -- treating -- assistance -- product -- grandfather -- floor -- sleeps -- hears -- sleeping -- educational -- favorite -- jenny -- moments -- court -- animal -- piano -- movements -- development -- birth -- lessons -- teenager -- traditional -- previous -- latino -- cristina -- recording -- bachata -- daddy -- concerts -- welfare -- limit -- survive -- pills -- abusing -- betsy -- sitting -- include -- sleepy -- annie -- dirty -- expecting -- healthy -- humidity -- trips -- arizona -- alaska -- tourist -- construction -- uncomfortable -- grand -- monterrey -- danger -- weight -- robbed -- roman -- fantastic -- damaged -- crimes -- seattle -- montreal -- developed -- cruise -- atlantic -- pacific -- beauty -- falls -- queen -- australia -- constantly -- separate -- flight -- kansas -- plane -- report -- sweet -- hungry -- sunny -- citizenship -- citizens -- mildred -- arturo -- japanese -- relaxing -- aggressive -- stories -- daily -- guitar -- instrument -- lyrics -- denver -- buildings -- steal -- grows -- block -- highway -- windows -- doors -- camping -- relaxed -- familiar -- swear -- contrary -- criminals -- collect -- campus -- impressive -- catch -- march -- dressed -- letter -- medicare -- points -- columbia -- paradise -- penalty -- sells -- fault -- ignorant -- criticize -- professionals -- professional -- ignorance -- yellow -- embarrassed -- goodbye -- david -- theory -- translator -- surprise -- tennessee -- cents -- tonight\n",
      "number of words: 210\n",
      "--------------------\n",
      "40 out of 210 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=75.66, recall=48.37\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "carmen -- spanish -- school -- puerto -- chicago -- university -- college -- ricardo -- barranquilla -- going -- family -- colombia -- pennsylvania -- people -- think -- always -- thank -- something -- exactly -- better -- argentina -- florida -- house -- around -- connecticut -- milgred -- hello -- everything -- teenager -- person -- mercedes -- philadelphia -- still -- right -- thing -- maybe -- cultural -- georgia -- norma -- atlanta -- american -- alcohol -- group -- super -- karma -- internet -- center -- medicare -- krishna -- india -- material -- everybody -- channel -- close -- three -- point -- history -- josefina -- please -- money -- email -- speak -- ernestina -- hospital -- attachment -- yahoo -- idaho -- telemundo -- marketing -- shudong -- break -- hotmail -- terrible -- website -- horrible -- software -- microsoft -- emails -- general -- north -- gmail -- miguel -- amazon -- tickets -- ticket -- sorry -- personal -- state -- virginia -- federal -- legal -- miami -- machine -- sound -- maternity -- discovery -- jenny -- radio -- animal -- piano -- development -- crazy -- quiet -- playstation -- goodness -- texas -- manuel -- nebraska -- regular -- relax -- houston -- salsa -- guatemala -- texmex -- mariachi -- south -- arizona -- chile -- merengue -- latino -- california -- cristina -- bachata -- daddy -- elvis -- crespo -- boricua -- cumbias -- tammy -- welfare -- betsy -- social -- utilities -- salvador -- annie -- honduras -- bronx -- jamaica -- pesos -- sulma -- alaska -- costa -- vallarta -- chiapas -- aruba -- white -- wisconsin -- grand -- monterrey -- durango -- industrial -- doctor -- washington -- montreal -- israel -- falls -- hawaii -- queen -- colorado -- australia -- hotel -- kansas -- oaxaca -- sweet -- miles -- mildred -- arturo -- global -- civil -- denver -- highway -- camping -- santiago -- ballet -- highrises -- philly -- cornell -- brutal -- baltimore -- olaya -- campus -- racial -- upenn -- ghetto -- health -- express -- undergraduate -- javier -- meeting -- ecuador -- video -- british -- columbia -- vancouver -- simple -- republicans -- santo -- domingo -- apart -- fault -- jennifer -- interracial -- topic -- manhattan -- excuse -- chatrooms -- honey -- meant -- david -- sixty -- computers -- flexible\n",
      "number of words: 212\n",
      "--------------------\n",
      "79 out of 212 retrieved with atleast 1 correct instance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=80.34, recall=57.26\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "name -- live -- case -- first -- time -- home -- another -- people -- even -- leave -- give -- life -- coming -- house -- women -- morning -- years -- high -- change -- want -- remember -- waiting -- send -- help -- girl -- lives -- city -- need -- saying -- someone -- terrible -- news -- love -- watch -- found -- town -- make -- stay -- huge\n",
      "number of words: 39\n",
      "--------------------\n",
      "34 out of 39 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "_ = eval_all_word_lists(\"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_mix-0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data['google'] = {}\n",
    "en_data['google']['refs'], en_data['google']['hyps'], en_data['google']['4refs'] = get_model_data(\"\", \n",
    "                                                                                                  use_google=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_path = \"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\"\n",
    "en_data['80hrs'] = {}\n",
    "en_data['80hrs']['refs'], en_data['80hrs']['hyps'], en_data['80hrs']['4refs'] = get_model_data(nmt_path, \n",
    "                                                                                               use_google=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_path = \"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_mix-0.5\"\n",
    "en_data['80hrs_da'] = {}\n",
    "en_data['80hrs_da']['refs'], en_data['80hrs_da']['hyps'], en_data['80hrs_da']['4refs'] = get_model_data(nmt_path, \n",
    "                                                                                               use_google=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_path = \"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\"\n",
    "en_data['50hrs'] = {}\n",
    "en_data['50hrs']['refs'], en_data['50hrs']['hyps'], en_data['50hrs']['4refs'] = get_model_data(nmt_path, \n",
    "                                                                                               use_google=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_path = \"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5_sample-mix-0.5\"\n",
    "en_data['50hrs_da'] = {}\n",
    "en_data['50hrs_da']['refs'], en_data['50hrs_da']['hyps'], en_data['50hrs_da']['4refs'] = get_model_data(nmt_path, \n",
    "                                                                                               use_google=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt = '20051023_232057_325_fsp-A-3'\n",
    "utt = '20051017_180712_270_fsp-B-2'\n",
    "utt = '20051009_182032_217_fsp-B-149'\n",
    "\n",
    "for m in en_data:\n",
    "    print(m, ' & ', \" \".join(en_data[m]['hyps'][utt]), ' \\\\\\\\')\n",
    "    print()\n",
    "    \n",
    "print(\" \".join(en_data[m]['refs'][utt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join([w.decode() for w in bow_dict['w2i'].keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_dict['freq'])-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"help\"\n",
    "data_key = \"80hrs_da\"\n",
    "\n",
    "print(bow_dict[\"freq\"][keyword.encode()], bow_dict[\"freq_dev\"][keyword.encode()])\n",
    "\n",
    "t_count = 0\n",
    "c_count = 0\n",
    "tp_count = 0\n",
    "corr_utts = []\n",
    "\n",
    "for u in en_data[data_key][\"hyps\"]:\n",
    "    common_ref_words = set(en_data[data_key][\"4refs\"][u][0])\n",
    "    for curr_ref in en_data[data_key][\"4refs\"][u][1:]:\n",
    "        common_ref_words &= set(curr_ref)\n",
    "#     if sum([1 if keyword in set(r) else 0 for r in en_data['50hrs_da'][\"4refs\"][u]]) >= 4:\n",
    "    if keyword in common_ref_words:\n",
    "        t_count += 1\n",
    "    if keyword in en_data[data_key][\"hyps\"][u]:\n",
    "        tp_count += 1\n",
    "    if keyword in en_data[data_key][\"hyps\"][u] and keyword in common_ref_words:\n",
    "        c_count += 1\n",
    "        corr_utts.append(u)\n",
    "\n",
    "print(t_count, c_count, tp_count)\n",
    "        \n",
    "for u in corr_utts:\n",
    "    print(\" \".join(en_data[data_key][\"hyps\"][u]))\n",
    "    for r in en_data[data_key][\"4refs\"][u]:\n",
    "        if keyword in r:\n",
    "            print(\" \".join(r))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_50_da[1]['word'][keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = sum([model_50_da[1]['word'][w]['tc'] for w in model_50_da[1]['word']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = sum([model_50_da[1]['word'][w]['t'] for w in model_50_da[1]['word']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc / tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in model_50_da[1]['word']:\n",
    "    if w in model_50_da[1]['word']: \n",
    "        p_50 = model_50_da[1]['word'][w]['tc'] / model_50_da[1]['word'][w]['t'] if model_50_da[1]['word'][w]['t'] > 0 else 0\n",
    "    else:\n",
    "        p_50 = 0\n",
    "    if w in model_80_da[1]['word']:\n",
    "        p_80 = model_80_da[1]['word'][w]['tc'] / model_80_da[1]['word'][w]['t'] if model_80_da[1]['word'][w]['t'] > 0 else 0\n",
    "    else:\n",
    "        p_80 = 0\n",
    "    print(w, \n",
    "          \"{0:20.1f}\".format(p_50 * 100),\n",
    "          \"{0:20.1f}\".format(p_80 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in model_50_da[1]['word']:\n",
    "    if model_50_da[1]['word'][w]['t'] > 0 and model_50_da[1]['word'][w]['tc'] / model_50_da[1]['word'][w]['t'] >= 0:\n",
    "        print(w, \"{0:.1f}\".format(model_50_da[1]['word'][w]['tc'] / model_50_da[1]['word'][w]['t'] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in model_80_da[1]['word']:\n",
    "    if model_80_da[1]['word'][w]['t'] > 0 and model_80_da[1]['word'][w]['tc'] / model_80_da[1]['word'][w]['t'] >= 0.4:\n",
    "        print(w, \"{0:.1f}\".format(model_80_da[1]['word'][w]['tc'] / model_80_da[1]['word'][w]['t'] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sorted_words = [w.decode() for w,f in sorted(bow_dict['freq'].items(), reverse=True, key=lambda t: t[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_top_K = 5\n",
    "top_K_words = freq_sorted_words[:predict_top_K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" --- \".join(top_K_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_preds = [top_K_words for u in google_hyp_r0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_utt_refs_words_bow.values(), dummy_preds)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_single_ref.values(), dummy_preds)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pred = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_p_vals = np.zeros((max_pred), dtype=\"f\")\n",
    "dummy_r_vals = np.zeros((max_pred), dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_dummy = {}\n",
    "# for thresh in tqdm(np.arange(min_prob, max_prob+thresh_delta,thresh_delta)):\n",
    "for num_pred in tqdm(range(0,max_pred)):\n",
    "    top_K_words = freq_sorted_words[:num_pred+1]\n",
    "    dummy_preds = [top_K_words for u in google_hyp_r0]\n",
    "    dummy_p_vals[num_pred], dummy_r_vals[num_pred] = basic_precision_recall(google_utt_refs_words_bow.values(), \n",
    "                                                                            dummy_preds)[:2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_p_vals /= 100.0\n",
    "dummy_r_vals /= 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(dummy_p_vals, dummy_r_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels = range(1,max_pred+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(dummy_r_vals*100, dummy_p_vals*100, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(dummy_r_vals*100, dummy_p_vals*100, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_precision_recall(single_dev_ref_words.values(), dummy_preds)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google refs vs Edin refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if len(r[0]) > 0 else 0 for r in single_dev_ref_words.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(set(r[0])-{'_UNK'})for r in single_dev_ref_words.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_count = 0\n",
    "for u in set(google_single_ref.keys()) & set(single_dev_ref_words.keys()):\n",
    "    if set(single_dev_ref_words[u][0]) - {'_UNK'} != set(google_single_ref[u][0]):\n",
    "        mismatch_count += max(len(set(single_dev_ref_words[u][0]) - set(google_single_ref[u][0])), \n",
    "                              len(set(google_single_ref[u][0]) - set(single_dev_ref_words[u][0])))\n",
    "        print(u, single_dev_ref_words[u], google_single_ref[u])\n",
    "        print((set(single_dev_ref_words[u][0]) - set(google_single_ref[u][0])), \n",
    "                              (set(google_single_ref[u][0]) - set(single_dev_ref_words[u][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mismatch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(set(r[0])-{'_UNK'})for r in single_dev_ref_words.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
