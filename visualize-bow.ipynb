{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bow_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmt_basic_precision_recall(r, h, display=False):\n",
    "    p_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    p_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    r_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    r_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    metrics = {\"rc\": 0, \"rt\": 0, \"tp\": 0, \"tc\": 0, \"word\": {}}\n",
    "\n",
    "    if display:\n",
    "        print(\"total utts={0:d}\".format(len(r)))\n",
    "\n",
    "    i=1\n",
    "\n",
    "    for references, hypothesis in zip(r, h):\n",
    "        if min([len(any_ref) for any_ref in references]) > 0:\n",
    "            if len(hypothesis) > 0:\n",
    "                p_i = modified_precision(references, hypothesis, i)\n",
    "                p_numerators[i] += p_i.numerator\n",
    "                p_denominators[i] += p_i.denominator\n",
    "\n",
    "                metrics[\"tc\"] += p_i.numerator\n",
    "                metrics[\"tp\"] += p_i.denominator\n",
    "            else:\n",
    "                p_numerators[i] += 0\n",
    "                p_denominators[i] += 0\n",
    "\n",
    "                metrics[\"tc\"] += 0\n",
    "                metrics[\"tp\"] += 0\n",
    "\n",
    "            #print(p_i.numerator, p_i.denominator)\n",
    "\n",
    "            tot_match = 0\n",
    "            tot_count = 0\n",
    "\n",
    "            max_recall_match, max_tp, max_t, max_word_level_details = count_match(references[0], hypothesis)\n",
    "            max_recall = max_recall_match / max_t if max_t > 0 else 0\n",
    "\n",
    "            for curr_ref in references:\n",
    "                curr_match, curr_tp, curr_t, curr_word_level_details = count_match(curr_ref, hypothesis)\n",
    "                curr_recall = curr_match / curr_t if curr_t > 0 else 0\n",
    "\n",
    "                if curr_recall > max_recall:\n",
    "                    max_recall_match = curr_match\n",
    "                    max_t = curr_t\n",
    "                    max_recall = curr_recall\n",
    "                    max_word_level_details = curr_word_level_details\n",
    "\n",
    "            r_numerators[i] += max_recall_match\n",
    "            r_denominators[i] += max_t\n",
    "            metrics[\"rc\"] += max_recall_match\n",
    "            metrics[\"rt\"] += max_t\n",
    "            for key in {\"t\",\"tp\",\"tc\"}:\n",
    "                for w in max_word_level_details[key]:\n",
    "                    if w not in metrics[\"word\"]:\n",
    "                        metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "                    metrics[\"word\"][w][key] += max_word_level_details[key][w]\n",
    "\n",
    "    prec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(p_numerators.values(), p_denominators.values())]\n",
    "    rec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(r_numerators.values(), r_denominators.values())]\n",
    "\n",
    "    if display:\n",
    "        print(\"{0:10s} | {1:>8s}\".format(\"metric\", \"1-gram\"))\n",
    "        print(\"-\"*54)\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"precision\", *prec))\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"recall\", *rec))\n",
    "\n",
    "    return prec[0], rec[0], metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(nmt_path, use_google=False):\n",
    "    if use_google:\n",
    "        google_s2t_hyps, google_s2t_refs, nmt_4refs = get_google_data()\n",
    "        nmt_hyps = google_s2t_hyps['fisher_dev_r0']\n",
    "        nmt_refs = google_s2t_refs['fisher_dev_ref_0']\n",
    "    else:\n",
    "        nmt_refs = pickle.load(open(os.path.join(nmt_path, \n",
    "                                                 \"model_s2t_refs.dict\"), \"rb\"))\n",
    "        nmt_hyps = pickle.load(open(os.path.join(nmt_path, \n",
    "                                                 \"model_s2t_hyps.dict\"), \"rb\"))\n",
    "        nmt_4refs = pickle.load(open(os.path.join(nmt_path,\n",
    "                                                  \"model_s2t_refs_for_eval.dict\"), \"rb\"))\n",
    "    \n",
    "    return nmt_refs, nmt_hyps, nmt_4refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nmt_model(nmt_path, use_google=False, min_len=10):\n",
    "    smooth_fun = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    \n",
    "    nmt_refs, nmt_hyps, nmt_4refs = get_model_data(nmt_path, use_google=use_google)\n",
    "\n",
    "    nmt_preds_bow = {}\n",
    "    nmt_1_ref = {}\n",
    "    nmt_refs_bow = {}\n",
    "\n",
    "    dev_utt_ids = nmt_hyps.keys()\n",
    "\n",
    "    for u in dev_utt_ids:\n",
    "        nmt_preds_bow[u] = list(get_words_in_bow_vocab(nmt_hyps[u], bow_dict))\n",
    "        nmt_refs_bow[u] = []\n",
    "        nmt_1_ref[u] = [list(get_words_in_bow_vocab(nmt_refs[u], bow_dict))]\n",
    "        for r in nmt_4refs[u]:\n",
    "            nmt_refs_bow[u].append(list(get_words_in_bow_vocab(r, bow_dict)))\n",
    "    \n",
    "    \n",
    "    p_bow, r_bow, metrics_1_bow = basic_precision_recall(nmt_1_ref.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    print(\"-\"*80)\n",
    "    print(\"BOW - using 1 reference\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_bow, r_bow))\n",
    "    num_1correct = len([item for item in metrics_1_bow[\"word\"].items() \n",
    "                        if item[1]['tc'] > 0])\n",
    "    \n",
    "    num_all = len([item for item in metrics_1_bow[\"word\"].items() \n",
    "                        if item[1]['t'] > 0])\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"{0:d} out of {1:d} retrieved with atleast 1 correct instance\".format(num_1correct, num_all))\n",
    "    \n",
    "\n",
    "    p_bow, r_bow, metrics_bow = basic_precision_recall(nmt_refs_bow.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(\"BOW - using all 4 references\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_bow, r_bow))\n",
    "    \n",
    "    num_1correct = len([item for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['tc'] > 0])\n",
    "    \n",
    "    num_all = len([item for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['t'] > 0])\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"{0:d} out of {1:d} retrieved with atleast 1 correct instance\".format(num_1correct, num_all))\n",
    "    \n",
    "    # MT PRECISION RECALL - NOOOT BOW\n",
    "    p_nmt, r_nmt, metrics_nmt = nmt_basic_precision_recall(nmt_4refs.values(), \n",
    "                                                       nmt_hyps.values())\n",
    "    print(\"-\"*80)\n",
    "    print(\"MT task - using all 4 references\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_nmt, r_nmt))\n",
    "\n",
    "    nmt_bleu = corpus_bleu(nmt_4refs.values(), \n",
    "                           nmt_hyps.values(),\n",
    "                           smoothing_function=smooth_fun.method2)\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    print(\"4 references bleu={0:2f}\".format(nmt_bleu*100))\n",
    "    \n",
    "    one_ref_list = []\n",
    "    one_hyp_list = []\n",
    "    \n",
    "#     for u in nmt_refs:\n",
    "#         one_ref_list.append([nmt_refs[u]])\n",
    "#         one_hyp_list.append(nmt_hyps[u])\n",
    "        \n",
    "#     p_nmt_one, r_nmt_one, metrics_nmt_one = nmt_basic_precision_recall(one_ref_list, \n",
    "#                                                            one_hyp_list)\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"MT task - using single references\")\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"precision={0:.2f}, recall={1:.2f}\".format(p_nmt_one, r_nmt_one))\n",
    "    \n",
    "#     nmt_bleu = corpus_bleu(one_ref_list, \n",
    "#                            one_hyp_list,\n",
    "#                            smoothing_function=smooth_fun.method2)\n",
    "\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"single reference bleu={0:2f}\".format(nmt_bleu*100))\n",
    "    print(\"-\"*80)\n",
    "    print(\"using min len filter\")\n",
    "    print(\"-\"*20)\n",
    "    check_bleu_with_len_filter(nmt_4refs, nmt_hyps, min_len=min_len)\n",
    "    \n",
    "    return metrics_1_bow, metrics_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bleu_with_len_filter(refs, hyps, min_len):\n",
    "    sel_refs, sel_hyps = [], []\n",
    "    for u in refs:\n",
    "        len_ref = min([len(r) for r in refs[u]])\n",
    "        if len_ref >= min_len:\n",
    "            sel_refs.append(refs[u])\n",
    "            sel_hyps.append(hyps[u])\n",
    "    print(\"{0:d} out of {1:d} have len >= {2:d}\".format(len(sel_refs), len(refs), min_len))\n",
    "    bleu_score = corpus_bleu(sel_refs, sel_hyps, smoothing_function=smooth_fun.method2)*100\n",
    "    print(\"BLEU={0:.2f}\".format(bleu_score))\n",
    "    sel_p, sel_r, _ = nmt_basic_precision_recall(sel_refs, sel_hyps)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(sel_p, sel_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"sp2bagwords/sp_0.50_trial-A/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_out_dim = rnn_in_units =  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/project/lowres/work/miniconda3/envs/chainer3/lib/python3.6/site-packages/chainer/utils/experimental.py:104: FutureWarning: chainer.links.normalization.layer_normalization.py is experimental. The interface can change in the future.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ADAM optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "model not found\n"
     ]
    }
   ],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict, bow_dict = get_data_dicts(m_cfg)\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\")):\n",
    "    dev_utts = pickle.load(open(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\"), \"rb\"))\n",
    "\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"model_s2t_train_out.dict\")):\n",
    "    train_utts = pickle.load(open(os.path.join(m_cfg['model_dir'], \"model_s2t_train_out.dict\"), \"rb\"))\n",
    "# batch_size = {'max': 128, 'med': 128, 'min': 128, 'scale': 1}\n",
    "batch_size = {'max': 64, 'med': 64, 'min': 64, 'scale': 1}\n",
    "batch_size = t_cfg['batch_size']\n",
    "\n",
    "edin_s2t_refs_for_eval_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \n",
    "                                           \"edin_s2t_refs_for_eval.dict\")\n",
    "edin_s2t_refs_for_eval = pickle.load(open(edin_s2t_refs_for_eval_path, \"rb\"))\n",
    "single_dev_ref = [[i[0]] for i in dev_utts[\"refs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict[\"fisher_dev\"]['20051009_182032_217_fsp-B-1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'],\n",
    "                                      m_cfg['train_set'])\n",
    "train_utts, train_loss = feed_model(model,\n",
    "                              optimizer=optimizer,\n",
    "                              m_dict=map_dict[train_key],\n",
    "                              b_dict=bucket_dict[train_key],\n",
    "                              vocab_dict=vocab_dict,\n",
    "                              bow_dict=bow_dict,\n",
    "                              batch_size=batch_size,\n",
    "                              x_key=enc_key,\n",
    "                              y_key=dec_key,\n",
    "                              train=False,\n",
    "                              input_path=input_path,\n",
    "                              max_dec=m_cfg['max_en_pred'],\n",
    "                              t_cfg=t_cfg,\n",
    "                              use_y=True,\n",
    "                              get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(train_utts, open(os.path.join(m_cfg['model_dir'], \"model_s2t_train_out.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pos_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "mean_neg_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "\n",
    "\n",
    "for i_w in range(4, len(bow_dict[\"i2w\"])):\n",
    "    this_word = bow_dict[\"i2w\"][i_w]\n",
    "    pos_indx = [i_w in r[0] for r in train_utts[\"refs\"]]\n",
    "    neg_indx = [i_w not in r[0] for r in train_utts[\"refs\"]]\n",
    "    mean_pos_scores[i_w] = np.mean(train_utts[\"probs\"][:,i_w][pos_indx])\n",
    "    mean_neg_scores[i_w] = np.mean(train_utts[\"probs\"][:,i_w][neg_indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.mean(mean_pos_scores), xp.mean(mean_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_p, _ = compute_avg_precision(train_utts[\"probs\"],\n",
    "                                                     0.0, 1.0, 5,\n",
    "                                                     m_cfg['max_en_pred'],\n",
    "                                                     train_utts[\"refs\"])\n",
    "train_avg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = m_cfg[\"pred_thresh\"]\n",
    "train_pred_words = get_pred_words_from_probs(train_utts[\"probs\"],\n",
    "#                                              mean_pos_scores,\n",
    "                                               0.5,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "\n",
    "train_prec, train_rec, _ = basic_precision_recall(train_utts[\"refs\"], train_pred_words)\n",
    "train_prec, train_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "\n",
    "dev_utts, dev_loss = feed_model(model,\n",
    "                                optimizer=optimizer,\n",
    "                                m_dict=map_dict[dev_key],\n",
    "                                b_dict=bucket_dict[dev_key],\n",
    "                                vocab_dict=vocab_dict,\n",
    "                                bow_dict=bow_dict,\n",
    "                                batch_size=batch_size,\n",
    "                                x_key=enc_key,\n",
    "                                y_key=dec_key,\n",
    "                                train=False,\n",
    "                                input_path=input_path,\n",
    "                                max_dec=m_cfg['max_en_pred'],\n",
    "                                t_cfg=t_cfg,\n",
    "                                use_y=True,\n",
    "                                get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dev_ref = [[i[0]] for i in dev_utts[\"refs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(dev_utts[\"probs\"]), np.max(dev_utts[\"probs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dev_pos_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "mean_dev_neg_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "\n",
    "\n",
    "for i_w in range(4, len(bow_dict[\"i2w\"])):\n",
    "    this_word = bow_dict[\"i2w\"][i_w]\n",
    "    pos_indx = [i_w in r[0] for r in dev_utts[\"refs\"]]\n",
    "    neg_indx = [i_w not in r[0] for r in dev_utts[\"refs\"]]\n",
    "    mean_dev_pos_scores[i_w] = np.mean(dev_utts[\"probs\"][:,i_w][pos_indx])\n",
    "    mean_dev_neg_scores[i_w] = np.mean(dev_utts[\"probs\"][:,i_w][neg_indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.2\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, haha = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                               mean_dev_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "p, r, _ = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.1\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                               mean_dev_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_prob, max_prob = float(xp.min(dev_utts[\"probs\"])), float(xp.max(dev_utts[\"probs\"]))\n",
    "min_prob, max_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Plot - word level threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-0.5, 0.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pos_scores[4:14]*1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_deltas = np.asarray([0.7,0.8,0.9,1,1.1,1.2,1.3], dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_thresh = {}\n",
    "thresh_delta = 0.05\n",
    "for thresh in tqdm(np.arange(-0.5, 0.5+thresh_delta, thresh_delta)):\n",
    "# for thresh in tqdm(thresh_deltas):\n",
    "# for thresh in tqdm(np.linspace(min_prob, max_prob,num=20,endpoint=True)):\n",
    "    p_r_thresh[thresh] = {}\n",
    "    dev_pred_words_at_thresh = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                                           mean_pos_scores + thresh,\n",
    "                                                           len(bow_dict['i2w']))\n",
    "    p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(dev_utts[\"refs\"], \n",
    "                                                                              dev_pred_words_at_thresh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels, p_vals, r_vals = [], [], []\n",
    "for l in p_r_thresh:\n",
    "    p_vals.append(p_r_thresh[l][\"p\"])\n",
    "    r_vals.append(p_r_thresh[l][\"r\"])\n",
    "    thresh_labels.append(\"{0:.2f}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(r_vals, p_vals, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(r_vals, p_vals, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Plot - fixed threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_utts[\"probs\"][0]), len(bow_dict['i2w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_precision(probs, min_prob, max_prob, num_points, max_words, refs):\n",
    "    p_r_thresh = {}\n",
    "    for thresh in tqdm(np.linspace(min_prob, max_prob, num=num_points, endpoint=True)):\n",
    "        p_r_thresh[thresh] = {}\n",
    "        words_at_thresh = get_pred_words_from_probs(probs, thresh, max_words)\n",
    "        p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(refs, words_at_thresh)\n",
    "    \n",
    "    precision_array = np.array([p_r_thresh[i]['p']/100 for i in p_r_thresh], dtype=\"f\")\n",
    "    recall_array = np.array([p_r_thresh[i]['r']/100 for i in p_r_thresh], dtype=\"f\")\n",
    "    avg_p = np.trapz(precision_array[::-1], recall_array[::-1])\n",
    "    return avg_p, p_r_thresh\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_p, p_r_thresh = compute_avg_precision(dev_utts[\"probs\"], 0.0, 1.0, 50, 104, dev_utts[\"refs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_r_thresh = {}\n",
    "# thresh_delta = 0.01\n",
    "# # for thresh in tqdm(np.arange(min_prob, max_prob+thresh_delta,thresh_delta)):\n",
    "# for thresh in tqdm(np.linspace(min_prob, max_prob, num=30,endpoint=True)):\n",
    "#     p_r_thresh[thresh] = {}\n",
    "#     dev_pred_words_at_thresh = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "#                                                            thresh,\n",
    "#                                                            len(bow_dict['i2w']))\n",
    "#     p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(dev_utts[\"refs\"], \n",
    "#                                                                               dev_pred_words_at_thresh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels, p_vals, r_vals = [], [], []\n",
    "for l in p_r_thresh:\n",
    "    p_vals.append(p_r_thresh[l][\"p\"])\n",
    "    r_vals.append(p_r_thresh[l][\"r\"])\n",
    "    thresh_labels.append(\"{0:.2f}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(r_vals, p_vals, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(r_vals, p_vals, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.15\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)\n",
    "# print(\"using mean positive prediction threshold\")\n",
    "# dev_mean_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "#                                            mean_pos_scores,\n",
    "#                                            m_cfg['max_en_pred'])\n",
    "# p, r, _ = basic_precision_recall(single_dev_ref, dev_mean_pred_words)\n",
    "# print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.01\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)\n",
    "# print(\"using mean positive prediction threshold\")\n",
    "# dev_mean_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "#                                            mean_pos_scores,\n",
    "#                                            m_cfg['max_en_pred'])\n",
    "# p, r, _ = basic_precision_recall(single_dev_ref, dev_mean_pred_words)\n",
    "# print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_array = np.array([p_r_thresh[i]['p']/100 for i in p_r_thresh], dtype=\"f\")\n",
    "recall_array = np.array([p_r_thresh[i]['r']/100 for i in p_r_thresh], dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_array[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(precision_array[::-1], recall_array[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get preds and refs in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(train_pred_words, train_utts[\"refs\"]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_utt_preds_words = {}\n",
    "dev_utt_refs_words = {}\n",
    "for u, p, refs in zip(dev_utts['ids'], dev_pred_words, dev_utts[\"refs\"]):\n",
    "    dev_utt_preds_words[u] = list(set([bow_dict['i2w'][i].decode() for i in p]))\n",
    "    dev_utt_refs_words[u] = []\n",
    "    for r in refs:\n",
    "        #print(r)\n",
    "        dev_utt_refs_words[u].append([bow_dict['i2w'][i].decode() for i in set(r)])\n",
    "single_dev_ref_words = {u: [dev_utt_refs_words[u][0]] for u in dev_utt_refs_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, metric = basic_precision_recall(list(dev_utt_refs_words.values()), list(dev_utt_preds_words.values()))\n",
    "p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, rs, _ = basic_precision_recall(single_dev_ref_words.values(), dev_utt_preds_words.values())\n",
    "ps, rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, metric[k]) for k in ['rc', 'rt', 'tp', 'tc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_correctly_predicted = [item for item in metric[\"word\"].items() if item[1]['tc'] > 0]\n",
    "print(len(words_correctly_predicted))\n",
    "display(words_correctly_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common train words\n",
    "[w.decode() for w, f in sorted(bow_dict['freq'].items(), reverse=True, key=lambda t: t[1])][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(single_dev_ref_words.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_bow_words(single_dev_ref_words, \n",
    "                  dev_utt_preds_words, \n",
    "                  bow_dict, \n",
    "                  map_dict[\"fisher_dev\"], display_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval refs found, loading\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=84.85, recall=69.54\n",
      "--------------------\n",
      "39 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=92.82, recall=69.30\n",
      "--------------------\n",
      "38 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=76.60, recall=67.74\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=45.204494\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "1390 out of 3979 have len >= 10\n",
      "BLEU=46.59\n",
      "precision=78.12, recall=67.01\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"\", use_google=True, min_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 150 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=67.70, recall=50.34\n",
      "--------------------\n",
      "37 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=76.35, recall=49.95\n",
      "--------------------\n",
      "39 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=57.43, recall=53.60\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=24.021382\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "1458 out of 3977 have len >= 10\n",
      "BLEU=24.25\n",
      "precision=57.86, recall=52.48\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw/sp_1.0_h512_rnn4_l2e-4/\", min_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 50 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_metrics = eval_nmt_model(\"./sp2enw_mel-80_vocab-nltk/sp_0.33_h-256_e-128_l2e-3_lstm_drpt-0.3_cnn-32-2-2_rnn-3_b-40-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 25 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw_mel-80_vocab-nltk/sp_0.16_h-256_e-128_l2e-3_lstm_drpt-0.3_cnn-32-2-2_rnn-3_b-80-25_no-ln-bn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 15 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw/sp_.10/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 50 hours model - sample word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed: 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=55.92, recall=31.10\n",
      "--------------------\n",
      "31 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=65.76, recall=30.30\n",
      "--------------------\n",
      "31 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=46.71, recall=39.97\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=12.538613\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "1430 out of 3977 have len >= 10\n",
      "BLEU=12.10\n",
      "precision=46.53, recall=38.32\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.33_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=60.66, recall=33.91\n",
      "--------------------\n",
      "32 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=69.80, recall=33.33\n",
      "--------------------\n",
      "31 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=47.34, recall=41.31\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=13.377341\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "1430 out of 3977 have len >= 10\n",
      "BLEU=13.02\n",
      "precision=47.31, recall=39.86\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.33_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_sample/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed: AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=56.43, recall=31.88\n",
      "--------------------\n",
      "32 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=64.51, recall=30.20\n",
      "--------------------\n",
      "31 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=46.74, recall=39.26\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=11.939563\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "1430 out of 3977 have len >= 10\n",
      "BLEU=11.43\n",
      "precision=46.31, recall=37.64\n"
     ]
    }
   ],
   "source": [
    "new50_metrics = eval_nmt_model(\"emb_sp2enw/sp_0.33_seed-AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=57.14, recall=33.72\n",
      "--------------------\n",
      "34 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=63.39, recall=32.39\n",
      "--------------------\n",
      "31 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=47.45, recall=40.50\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=13.049108\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "1430 out of 3977 have len >= 10\n",
      "BLEU=12.53\n",
      "precision=47.77, recall=39.10\n"
     ]
    }
   ],
   "source": [
    "new50_metrics = eval_nmt_model(\"emb_sp2enw/sp_0.33_seed-AA_mix-0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 80 hours model - sample word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=65.99, recall=41.18\n",
      "--------------------\n",
      "35 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=74.42, recall=40.13\n",
      "--------------------\n",
      "34 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=53.21, recall=45.37\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=17.151387\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "1430 out of 3977 have len >= 10\n",
      "BLEU=17.01\n",
      "precision=53.54, recall=44.13\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.50_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=70.20, recall=43.60\n",
      "--------------------\n",
      "35 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=77.99, recall=42.22\n",
      "--------------------\n",
      "34 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=53.41, recall=45.68\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=17.622392\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "1430 out of 3977 have len >= 10\n",
      "BLEU=17.48\n",
      "precision=53.72, recall=44.56\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.50_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_sample/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sorted_words = [w.decode() for w,f in sorted(bow_dict['freq'].items(), reverse=True, key=lambda t: t[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_top_K = 5\n",
    "top_K_words = freq_sorted_words[:predict_top_K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" --- \".join(top_K_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_preds = [top_K_words for u in google_hyp_r0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_utt_refs_words_bow.values(), dummy_preds)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_single_ref.values(), dummy_preds)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pred = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_p_vals = np.zeros((max_pred), dtype=\"f\")\n",
    "dummy_r_vals = np.zeros((max_pred), dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_dummy = {}\n",
    "# for thresh in tqdm(np.arange(min_prob, max_prob+thresh_delta,thresh_delta)):\n",
    "for num_pred in tqdm(range(0,max_pred)):\n",
    "    top_K_words = freq_sorted_words[:num_pred+1]\n",
    "    dummy_preds = [top_K_words for u in google_hyp_r0]\n",
    "    dummy_p_vals[num_pred], dummy_r_vals[num_pred] = basic_precision_recall(google_utt_refs_words_bow.values(), \n",
    "                                                                            dummy_preds)[:2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_p_vals /= 100.0\n",
    "dummy_r_vals /= 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(dummy_p_vals, dummy_r_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels = range(1,max_pred+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(dummy_r_vals*100, dummy_p_vals*100, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(dummy_r_vals*100, dummy_p_vals*100, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_precision_recall(single_dev_ref_words.values(), dummy_preds)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google refs vs Edin refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if len(r[0]) > 0 else 0 for r in single_dev_ref_words.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(set(r[0])-{'_UNK'})for r in single_dev_ref_words.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_count = 0\n",
    "for u in set(google_single_ref.keys()) & set(single_dev_ref_words.keys()):\n",
    "    if set(single_dev_ref_words[u][0]) - {'_UNK'} != set(google_single_ref[u][0]):\n",
    "        mismatch_count += max(len(set(single_dev_ref_words[u][0]) - set(google_single_ref[u][0])), \n",
    "                              len(set(google_single_ref[u][0]) - set(single_dev_ref_words[u][0])))\n",
    "        print(u, single_dev_ref_words[u], google_single_ref[u])\n",
    "        print((set(single_dev_ref_words[u][0]) - set(google_single_ref[u][0])), \n",
    "                              (set(google_single_ref[u][0]) - set(single_dev_ref_words[u][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mismatch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(set(r[0])-{'_UNK'})for r in single_dev_ref_words.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
