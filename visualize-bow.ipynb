{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bow_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_basic_precision_recall(r, h, display=False):\n",
    "    p_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    p_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    r_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    r_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    metrics = {\"rc\": 0, \"rt\": 0, \"tp\": 0, \"tc\": 0, \"word\": {}}\n",
    "\n",
    "    if display:\n",
    "        print(\"total utts={0:d}\".format(len(r)))\n",
    "\n",
    "    i=1\n",
    "\n",
    "    for references, hypothesis in zip(r, h):\n",
    "        if min([len(any_ref) for any_ref in references]) > 0:\n",
    "            if len(hypothesis) > 0:\n",
    "                p_i = modified_precision(references, hypothesis, i)\n",
    "                p_numerators[i] += p_i.numerator\n",
    "                p_denominators[i] += p_i.denominator\n",
    "\n",
    "                metrics[\"tc\"] += p_i.numerator\n",
    "                metrics[\"tp\"] += p_i.denominator\n",
    "            else:\n",
    "                p_numerators[i] += 0\n",
    "                p_denominators[i] += 0\n",
    "\n",
    "                metrics[\"tc\"] += 0\n",
    "                metrics[\"tp\"] += 0\n",
    "\n",
    "            #print(p_i.numerator, p_i.denominator)\n",
    "\n",
    "            tot_match = 0\n",
    "            tot_count = 0\n",
    "\n",
    "            common_ref_words = set(references[0])\n",
    "            for curr_ref in references[1:]:\n",
    "                common_ref_words &= set(curr_ref)\n",
    "            \n",
    "            common_words = common_ref_words & set(hypothesis)\n",
    "            for w in common_ref_words:\n",
    "                if w not in metrics[\"word\"]:\n",
    "                    metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "                metrics[\"word\"][w][\"t\"] += 1\n",
    "            \n",
    "            for w in set(hypothesis):\n",
    "                if w not in metrics[\"word\"]:\n",
    "                    metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "                metrics[\"word\"][w][\"tp\"] += 1\n",
    "                \n",
    "            for w in common_words:\n",
    "                if w not in metrics[\"word\"]:\n",
    "                    metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "                metrics[\"word\"][w][\"tc\"] += 1\n",
    "            \n",
    "            r_numerators[i] += len(common_words)\n",
    "            r_denominators[i] += len(common_ref_words)\n",
    "            metrics[\"rc\"] += len(common_words)\n",
    "            metrics[\"rt\"] += len(common_ref_words)\n",
    "            \n",
    "\n",
    "#             max_recall_match, max_tp, max_t, max_word_level_details = count_match(list(common_ref_words), list(set(hypothesis)))\n",
    "#             max_recall = max_recall_match / max_t if max_t > 0 else 0\n",
    "\n",
    "            # max_recall_match, max_tp, max_t, max_word_level_details = count_match(references[0], hypothesis)\n",
    "            # max_recall = max_recall_match / max_t if max_t > 0 else 0\n",
    "\n",
    "            # for curr_ref in references:\n",
    "            #     curr_match, curr_tp, curr_t, curr_word_level_details = count_match(curr_ref, hypothesis)\n",
    "            #     curr_recall = curr_match / curr_t if curr_t > 0 else 0\n",
    "\n",
    "            #     if curr_recall > max_recall:\n",
    "            #         max_recall_match = curr_match\n",
    "            #         max_t = curr_t\n",
    "            #         max_recall = curr_recall\n",
    "            #         max_word_level_details = curr_word_level_details\n",
    "            \n",
    "\n",
    "#             r_numerators[i] += max_recall_match\n",
    "#             r_denominators[i] += max_t\n",
    "#             metrics[\"rc\"] += max_recall_match\n",
    "#             metrics[\"rt\"] += max_t\n",
    "#             for key in {\"t\",\"tp\",\"tc\"}:\n",
    "#                 for w in max_word_level_details[key]:\n",
    "#                     if w not in metrics[\"word\"]:\n",
    "#                         metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "#                     metrics[\"word\"][w][key] += max_word_level_details[key][w]\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    prec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(p_numerators.values(), p_denominators.values())]\n",
    "    rec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(r_numerators.values(), r_denominators.values())]\n",
    "\n",
    "    if display:\n",
    "        print(\"{0:10s} | {1:>8s}\".format(\"metric\", \"1-gram\"))\n",
    "        print(\"-\"*54)\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"precision\", *prec))\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"recall\", *rec))\n",
    "\n",
    "    return prec[0], rec[0], metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmt_basic_precision_recall(r, h, display=False):\n",
    "    p_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    p_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    r_numerators = Counter() # Key = ngram order, and value = no. of ngram matches.\n",
    "    r_denominators = Counter() # Key = ngram order, and value = no. of ngram in ref.\n",
    "    metrics = {\"rc\": 0, \"rt\": 0, \"tp\": 0, \"tc\": 0, \"word\": {}}\n",
    "\n",
    "    if display:\n",
    "        print(\"total utts={0:d}\".format(len(r)))\n",
    "\n",
    "    i=1\n",
    "\n",
    "    for references, hypothesis in zip(r, h):\n",
    "        if min([len(any_ref) for any_ref in references]) > 0:\n",
    "            if len(hypothesis) > 0:\n",
    "                p_i = modified_precision(references, hypothesis, i)\n",
    "                p_numerators[i] += p_i.numerator\n",
    "                p_denominators[i] += p_i.denominator\n",
    "\n",
    "                metrics[\"tc\"] += p_i.numerator\n",
    "                metrics[\"tp\"] += p_i.denominator\n",
    "            else:\n",
    "                p_numerators[i] += 0\n",
    "                p_denominators[i] += 0\n",
    "\n",
    "                metrics[\"tc\"] += 0\n",
    "                metrics[\"tp\"] += 0\n",
    "\n",
    "            #print(p_i.numerator, p_i.denominator)\n",
    "\n",
    "            tot_match = 0\n",
    "            tot_count = 0\n",
    "\n",
    "            max_recall_match, max_tp, max_t, max_word_level_details = count_match(references[0], hypothesis)\n",
    "            max_recall = max_recall_match / max_t if max_t > 0 else 0\n",
    "\n",
    "            for curr_ref in references:\n",
    "                curr_match, curr_tp, curr_t, curr_word_level_details = count_match(curr_ref, hypothesis)\n",
    "                curr_recall = curr_match / curr_t if curr_t > 0 else 0\n",
    "\n",
    "                if curr_recall > max_recall:\n",
    "                    max_recall_match = curr_match\n",
    "                    max_t = curr_t\n",
    "                    max_recall = curr_recall\n",
    "                    max_word_level_details = curr_word_level_details\n",
    "\n",
    "            r_numerators[i] += max_recall_match\n",
    "            r_denominators[i] += max_t\n",
    "            metrics[\"rc\"] += max_recall_match\n",
    "            metrics[\"rt\"] += max_t\n",
    "            for key in {\"t\",\"tp\",\"tc\"}:\n",
    "                for w in max_word_level_details[key]:\n",
    "                    if w not in metrics[\"word\"]:\n",
    "                        metrics[\"word\"][w] = {\"t\": 0, \"tp\": 0, \"tc\": 0}\n",
    "                    metrics[\"word\"][w][key] += max_word_level_details[key][w]\n",
    "\n",
    "    prec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(p_numerators.values(), p_denominators.values())]\n",
    "    rec = [(n / d) * 100 if d > 0 else 0 for n,d in zip(r_numerators.values(), r_denominators.values())]\n",
    "\n",
    "    if display:\n",
    "        print(\"{0:10s} | {1:>8s}\".format(\"metric\", \"1-gram\"))\n",
    "        print(\"-\"*54)\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"precision\", *prec))\n",
    "        print(\"{0:10s} | {1:8.2f}\".format(\"recall\", *rec))\n",
    "\n",
    "    return prec[0], rec[0], metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(nmt_path, use_google=False):\n",
    "    if use_google:\n",
    "        google_s2t_hyps, google_s2t_refs, nmt_4refs = get_google_data()\n",
    "        nmt_hyps = google_s2t_hyps['fisher_dev_r0']\n",
    "        nmt_refs = google_s2t_refs['fisher_dev_ref_0']\n",
    "    else:\n",
    "        nmt_refs = pickle.load(open(os.path.join(nmt_path, \n",
    "                                                 \"model_s2t_refs.dict\"), \"rb\"))\n",
    "        nmt_hyps = pickle.load(open(os.path.join(nmt_path, \n",
    "                                                 \"model_s2t_hyps.dict\"), \"rb\"))\n",
    "        nmt_4refs = pickle.load(open(os.path.join(nmt_path,\n",
    "                                                  \"model_s2t_refs_for_eval.dict\"), \"rb\"))\n",
    "    \n",
    "    return nmt_refs, nmt_hyps, nmt_4refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nmt_model(nmt_path, use_google=False, min_len=10):\n",
    "    smooth_fun = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    \n",
    "    nmt_refs, nmt_hyps, nmt_4refs = get_model_data(nmt_path, use_google=use_google)\n",
    "\n",
    "    nmt_preds_bow = {}\n",
    "    nmt_1_ref = {}\n",
    "    nmt_refs_bow = {}\n",
    "\n",
    "    dev_utt_ids = nmt_hyps.keys()\n",
    "\n",
    "    for u in dev_utt_ids:\n",
    "        nmt_preds_bow[u] = list(get_words_in_bow_vocab(nmt_hyps[u], bow_dict))\n",
    "        nmt_refs_bow[u] = []\n",
    "        nmt_1_ref[u] = [list(get_words_in_bow_vocab(nmt_refs[u], bow_dict))]\n",
    "        for r in nmt_4refs[u]:\n",
    "            nmt_refs_bow[u].append(list(get_words_in_bow_vocab(r, bow_dict)))\n",
    "    \n",
    "    \n",
    "    p_bow, r_bow, metrics_1_bow = bow_basic_precision_recall(nmt_1_ref.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    print(\"-\"*80)\n",
    "    print(\"BOW - using 1 reference\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_bow, r_bow))\n",
    "    num_1correct = len([item for item in metrics_1_bow[\"word\"].items() \n",
    "                        if item[1]['tc'] > 0])\n",
    "    \n",
    "    num_all = len([item for item in metrics_1_bow[\"word\"].items() \n",
    "                        if item[1]['t'] > 0])\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"{0:d} out of {1:d} retrieved with atleast 1 correct instance\".format(num_1correct, num_all))\n",
    "    \n",
    "\n",
    "    p_bow, r_bow, metrics_bow = bow_basic_precision_recall(nmt_refs_bow.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(\"BOW - using all 4 references\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_bow, r_bow))\n",
    "    \n",
    "    num_1correct = len([item for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['tc'] > 0])\n",
    "    \n",
    "    num_all = len([item for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['t'] > 0])\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"{0:d} out of {1:d} retrieved with atleast 1 correct instance\".format(num_1correct, num_all))\n",
    "    \n",
    "    # MT PRECISION RECALL - NOOOT BOW\n",
    "    p_nmt, r_nmt, metrics_nmt = nmt_basic_precision_recall(nmt_4refs.values(), \n",
    "                                                       nmt_hyps.values())\n",
    "    print(\"-\"*80)\n",
    "    print(\"MT task - using all 4 references\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_nmt, r_nmt))\n",
    "\n",
    "    nmt_bleu = corpus_bleu(nmt_4refs.values(), \n",
    "                           nmt_hyps.values(),\n",
    "                           smoothing_function=smooth_fun.method2)\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    print(\"4 references bleu={0:2f}\".format(nmt_bleu*100))\n",
    "    \n",
    "    one_ref_list = []\n",
    "    one_hyp_list = []\n",
    "    \n",
    "#     for u in nmt_refs:\n",
    "#         one_ref_list.append([nmt_refs[u]])\n",
    "#         one_hyp_list.append(nmt_hyps[u])\n",
    "        \n",
    "#     p_nmt_one, r_nmt_one, metrics_nmt_one = nmt_basic_precision_recall(one_ref_list, \n",
    "#                                                            one_hyp_list)\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"MT task - using single references\")\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"precision={0:.2f}, recall={1:.2f}\".format(p_nmt_one, r_nmt_one))\n",
    "    \n",
    "#     nmt_bleu = corpus_bleu(one_ref_list, \n",
    "#                            one_hyp_list,\n",
    "#                            smoothing_function=smooth_fun.method2)\n",
    "\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"single reference bleu={0:2f}\".format(nmt_bleu*100))\n",
    "    print(\"-\"*80)\n",
    "    print(\"using min len filter\")\n",
    "    print(\"-\"*20)\n",
    "    check_bleu_with_len_filter(nmt_4refs, nmt_hyps, min_len=min_len)\n",
    "    \n",
    "    return metrics_1_bow, metrics_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prec_recall_for_words(nmt_path, bow_dict, use_google=False):\n",
    "    smooth_fun = nltk.translate.bleu_score.SmoothingFunction()\n",
    "    \n",
    "    nmt_refs, nmt_hyps, nmt_4refs = get_model_data(nmt_path, use_google=use_google)\n",
    "\n",
    "    nmt_preds_bow = {}\n",
    "    nmt_1_ref = {}\n",
    "    nmt_refs_bow = {}\n",
    "\n",
    "    dev_utt_ids = nmt_hyps.keys()\n",
    "\n",
    "    for u in dev_utt_ids:\n",
    "        nmt_preds_bow[u] = list(get_words_in_bow_vocab(nmt_hyps[u], bow_dict))\n",
    "        nmt_refs_bow[u] = []\n",
    "        nmt_1_ref[u] = [list(get_words_in_bow_vocab(nmt_refs[u], bow_dict))]\n",
    "        for r in nmt_4refs[u]:\n",
    "            nmt_refs_bow[u].append(list(get_words_in_bow_vocab(r, bow_dict)))\n",
    "    \n",
    "    \n",
    "    p_bow, r_bow, metrics_1_bow = bow_basic_precision_recall(nmt_1_ref.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    \n",
    "#     print(\"-\"*80)\n",
    "#     print(\"Using word list: \\n{0:s}\".format(\" -- \".join([w.decode() for w in bow_dict[\"w2i\"].keys()])))\n",
    "#     print(\"number of words: {0:d}\".format(len(bow_dict[\"w2i\"])))\n",
    "    print(\"-\"*80)\n",
    "    print(\"-\"*20)\n",
    "    p_bow, r_bow, metrics_bow = bow_basic_precision_recall(nmt_refs_bow.values(), \n",
    "                                                       nmt_preds_bow.values())\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(\"BOW - using all 4 references\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(p_bow, r_bow))\n",
    "    \n",
    "    num_1correct = len([item for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['tc'] > 0])\n",
    "    \n",
    "    words_present = [item[0] for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['t'] > 0]\n",
    "    num_all = len(words_present)\n",
    "    print(\"-\"*80)\n",
    "    print(\"Using word list: \\n{0:s}\".format(\" -- \".join(words_present)))\n",
    "    print(\"number of words: {0:d}\".format(num_all))\n",
    "    top_five = [w[0] for w in sorted([(item[0], item[1]['t']) for item in metrics_bow[\"word\"].items() \n",
    "                        if item[1]['t'] > 0], reverse=True, key= lambda t: t[1])[:5]]\n",
    "    print(\"Top 5 words present: \\n{0:s}\".format(\" -- \".join(top_five)))\n",
    "\n",
    "    print(\"-\"*20)\n",
    "    print(\"{0:d} out of {1:d} retrieved with atleast 1 correct instance\".format(num_1correct, num_all))\n",
    "    \n",
    "    \n",
    "    return metrics_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"crisis\": os.path.join(m_cfg['data_path'], \n",
    "#                                                     \"bow_crises_vocab.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_word_lists(nmt_path, use_google=False):\n",
    "    metrics = {}\n",
    "    eval_word_lists = {\"en_freq\": os.path.join(m_cfg['data_path'], \"eval_en_freq_vocab.dict\"),\n",
    "                       \"en_rare\": os.path.join(m_cfg['data_path'], \"eval_en_rare_vocab.dict\"),\n",
    "#                        \"en_es_common\": os.path.join(m_cfg['data_path'], \n",
    "#                                                     \"eval_en_es_common_vocab.dict\"),\n",
    "                       \"crisis\": os.path.join(m_cfg['data_path'], \n",
    "                                                    \"eval_en_crisis_vocab.dict\")}\n",
    "    for key, word_list in eval_word_lists.items():\n",
    "        words = pickle.load(open(word_list, \"rb\"))\n",
    "        metrics[key] = eval_prec_recall_for_words(nmt_path, words, use_google=use_google)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bleu_with_len_filter(refs, hyps, min_len):\n",
    "    sel_refs, sel_hyps = [], []\n",
    "    for u in refs:\n",
    "        len_ref = min([len(r) for r in refs[u]])\n",
    "        if len_ref >= min_len:\n",
    "            sel_refs.append(refs[u])\n",
    "            sel_hyps.append(hyps[u])\n",
    "    print(\"{0:d} out of {1:d} have len >= {2:d}\".format(len(sel_refs), len(refs), min_len))\n",
    "    bleu_score = corpus_bleu(sel_refs, sel_hyps, smoothing_function=smooth_fun.method2)*100\n",
    "    print(\"BLEU={0:.2f}\".format(bleu_score))\n",
    "    sel_p, sel_r, _ = nmt_basic_precision_recall(sel_refs, sel_hyps)\n",
    "    print(\"precision={0:.2f}, recall={1:.2f}\".format(sel_p, sel_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"sp2bagwords/sp_0.50_trial-A/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_out_dim = rnn_in_units =  640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/group/project/lowres/work/miniconda3/envs/chainer3/lib/python3.6/site-packages/chainer/utils/experimental.py:104: FutureWarning: chainer.links.normalization.layer_normalization.py is experimental. The interface can change in the future.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ADAM optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "model not found\n"
     ]
    }
   ],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict, bow_dict = get_data_dicts(m_cfg)\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\")):\n",
    "    dev_utts = pickle.load(open(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\"), \"rb\"))\n",
    "\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"model_s2t_train_out.dict\")):\n",
    "    train_utts = pickle.load(open(os.path.join(m_cfg['model_dir'], \"model_s2t_train_out.dict\"), \"rb\"))\n",
    "# batch_size = {'max': 128, 'med': 128, 'min': 128, 'scale': 1}\n",
    "batch_size = {'max': 64, 'med': 64, 'min': 64, 'scale': 1}\n",
    "batch_size = t_cfg['batch_size']\n",
    "\n",
    "edin_s2t_refs_for_eval_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \n",
    "                                           \"edin_s2t_refs_for_eval.dict\")\n",
    "edin_s2t_refs_for_eval = pickle.load(open(edin_s2t_refs_for_eval_path, \"rb\"))\n",
    "single_dev_ref = [[i[0]] for i in dev_utts[\"refs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['es_w', 'es_c', 'en_w', 'en_c', 'seg'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_dict[\"fisher_dev\"]['20051009_182032_217_fsp-B-1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'],\n",
    "                                      m_cfg['train_set'])\n",
    "train_utts, train_loss = feed_model(model,\n",
    "                              optimizer=optimizer,\n",
    "                              m_dict=map_dict[train_key],\n",
    "                              b_dict=bucket_dict[train_key],\n",
    "                              vocab_dict=vocab_dict,\n",
    "                              bow_dict=bow_dict,\n",
    "                              batch_size=batch_size,\n",
    "                              x_key=enc_key,\n",
    "                              y_key=dec_key,\n",
    "                              train=False,\n",
    "                              input_path=input_path,\n",
    "                              max_dec=m_cfg['max_en_pred'],\n",
    "                              t_cfg=t_cfg,\n",
    "                              use_y=True,\n",
    "                              get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(train_utts, open(os.path.join(m_cfg['model_dir'], \"model_s2t_train_out.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pos_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "mean_neg_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "\n",
    "\n",
    "for i_w in range(4, len(bow_dict[\"i2w\"])):\n",
    "    this_word = bow_dict[\"i2w\"][i_w]\n",
    "    pos_indx = [i_w in r[0] for r in train_utts[\"refs\"]]\n",
    "    neg_indx = [i_w not in r[0] for r in train_utts[\"refs\"]]\n",
    "    mean_pos_scores[i_w] = np.mean(train_utts[\"probs\"][:,i_w][pos_indx])\n",
    "    mean_neg_scores[i_w] = np.mean(train_utts[\"probs\"][:,i_w][neg_indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.mean(mean_pos_scores), xp.mean(mean_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_p, _ = compute_avg_precision(train_utts[\"probs\"],\n",
    "                                                     0.0, 1.0, 5,\n",
    "                                                     m_cfg['max_en_pred'],\n",
    "                                                     train_utts[\"refs\"])\n",
    "train_avg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = m_cfg[\"pred_thresh\"]\n",
    "train_pred_words = get_pred_words_from_probs(train_utts[\"probs\"],\n",
    "#                                              mean_pos_scores,\n",
    "                                               0.5,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "\n",
    "train_prec, train_rec, _ = basic_precision_recall(train_utts[\"refs\"], train_pred_words)\n",
    "train_prec, train_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "\n",
    "dev_utts, dev_loss = feed_model(model,\n",
    "                                optimizer=optimizer,\n",
    "                                m_dict=map_dict[dev_key],\n",
    "                                b_dict=bucket_dict[dev_key],\n",
    "                                vocab_dict=vocab_dict,\n",
    "                                bow_dict=bow_dict,\n",
    "                                batch_size=batch_size,\n",
    "                                x_key=enc_key,\n",
    "                                y_key=dec_key,\n",
    "                                train=False,\n",
    "                                input_path=input_path,\n",
    "                                max_dec=m_cfg['max_en_pred'],\n",
    "                                t_cfg=t_cfg,\n",
    "                                use_y=True,\n",
    "                                get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dev_ref = [[i[0]] for i in dev_utts[\"refs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(dev_utts[\"probs\"]), np.max(dev_utts[\"probs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dev_pos_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "mean_dev_neg_scores = np.array([0.0 for _ in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "\n",
    "\n",
    "for i_w in range(4, len(bow_dict[\"i2w\"])):\n",
    "    this_word = bow_dict[\"i2w\"][i_w]\n",
    "    pos_indx = [i_w in r[0] for r in dev_utts[\"refs\"]]\n",
    "    neg_indx = [i_w not in r[0] for r in dev_utts[\"refs\"]]\n",
    "    mean_dev_pos_scores[i_w] = np.mean(dev_utts[\"probs\"][:,i_w][pos_indx])\n",
    "    mean_dev_neg_scores[i_w] = np.mean(dev_utts[\"probs\"][:,i_w][neg_indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.2\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, haha = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                               mean_dev_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "p, r, _ = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.1\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                               mean_dev_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_prob, max_prob = float(xp.min(dev_utts[\"probs\"])), float(xp.max(dev_utts[\"probs\"]))\n",
    "min_prob, max_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Plot - word level threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-0.5, 0.5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pos_scores[4:14]*1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_deltas = np.asarray([0.7,0.8,0.9,1,1.1,1.2,1.3], dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_thresh = {}\n",
    "thresh_delta = 0.05\n",
    "for thresh in tqdm(np.arange(-0.5, 0.5+thresh_delta, thresh_delta)):\n",
    "# for thresh in tqdm(thresh_deltas):\n",
    "# for thresh in tqdm(np.linspace(min_prob, max_prob,num=20,endpoint=True)):\n",
    "    p_r_thresh[thresh] = {}\n",
    "    dev_pred_words_at_thresh = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                                           mean_pos_scores + thresh,\n",
    "                                                           len(bow_dict['i2w']))\n",
    "    p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(dev_utts[\"refs\"], \n",
    "                                                                              dev_pred_words_at_thresh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels, p_vals, r_vals = [], [], []\n",
    "for l in p_r_thresh:\n",
    "    p_vals.append(p_r_thresh[l][\"p\"])\n",
    "    r_vals.append(p_r_thresh[l][\"r\"])\n",
    "    thresh_labels.append(\"{0:.2f}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(r_vals, p_vals, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(r_vals, p_vals, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Plot - fixed threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_utts[\"probs\"][0]), len(bow_dict['i2w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_precision(probs, min_prob, max_prob, num_points, max_words, refs):\n",
    "    p_r_thresh = {}\n",
    "    for thresh in tqdm(np.linspace(min_prob, max_prob, num=num_points, endpoint=True)):\n",
    "        p_r_thresh[thresh] = {}\n",
    "        words_at_thresh = get_pred_words_from_probs(probs, thresh, max_words)\n",
    "        p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(refs, words_at_thresh)\n",
    "    \n",
    "    precision_array = np.array([p_r_thresh[i]['p']/100 for i in p_r_thresh], dtype=\"f\")\n",
    "    recall_array = np.array([p_r_thresh[i]['r']/100 for i in p_r_thresh], dtype=\"f\")\n",
    "    avg_p = np.trapz(precision_array[::-1], recall_array[::-1])\n",
    "    return avg_p, p_r_thresh\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_p, p_r_thresh = compute_avg_precision(dev_utts[\"probs\"], 0.0, 1.0, 50, 104, dev_utts[\"refs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_r_thresh = {}\n",
    "# thresh_delta = 0.01\n",
    "# # for thresh in tqdm(np.arange(min_prob, max_prob+thresh_delta,thresh_delta)):\n",
    "# for thresh in tqdm(np.linspace(min_prob, max_prob, num=30,endpoint=True)):\n",
    "#     p_r_thresh[thresh] = {}\n",
    "#     dev_pred_words_at_thresh = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "#                                                            thresh,\n",
    "#                                                            len(bow_dict['i2w']))\n",
    "#     p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(dev_utts[\"refs\"], \n",
    "#                                                                               dev_pred_words_at_thresh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels, p_vals, r_vals = [], [], []\n",
    "for l in p_r_thresh:\n",
    "    p_vals.append(p_r_thresh[l][\"p\"])\n",
    "    r_vals.append(p_r_thresh[l][\"r\"])\n",
    "    thresh_labels.append(\"{0:.2f}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(r_vals, p_vals, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(r_vals, p_vals, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.15\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)\n",
    "# print(\"using mean positive prediction threshold\")\n",
    "# dev_mean_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "#                                            mean_pos_scores,\n",
    "#                                            m_cfg['max_en_pred'])\n",
    "# p, r, _ = basic_precision_recall(single_dev_ref, dev_mean_pred_words)\n",
    "# print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.01\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)\n",
    "# print(\"using mean positive prediction threshold\")\n",
    "# dev_mean_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "#                                            mean_pos_scores,\n",
    "#                                            m_cfg['max_en_pred'])\n",
    "# p, r, _ = basic_precision_recall(single_dev_ref, dev_mean_pred_words)\n",
    "# print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_array = np.array([p_r_thresh[i]['p']/100 for i in p_r_thresh], dtype=\"f\")\n",
    "recall_array = np.array([p_r_thresh[i]['r']/100 for i in p_r_thresh], dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_array[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(precision_array[::-1], recall_array[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get preds and refs in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(train_pred_words, train_utts[\"refs\"]))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_utt_preds_words = {}\n",
    "dev_utt_refs_words = {}\n",
    "for u, p, refs in zip(dev_utts['ids'], dev_pred_words, dev_utts[\"refs\"]):\n",
    "    dev_utt_preds_words[u] = list(set([bow_dict['i2w'][i].decode() for i in p]))\n",
    "    dev_utt_refs_words[u] = []\n",
    "    for r in refs:\n",
    "        #print(r)\n",
    "        dev_utt_refs_words[u].append([bow_dict['i2w'][i].decode() for i in set(r)])\n",
    "single_dev_ref_words = {u: [dev_utt_refs_words[u][0]] for u in dev_utt_refs_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, metric = basic_precision_recall(list(dev_utt_refs_words.values()), list(dev_utt_preds_words.values()))\n",
    "p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, rs, _ = basic_precision_recall(single_dev_ref_words.values(), dev_utt_preds_words.values())\n",
    "ps, rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, metric[k]) for k in ['rc', 'rt', 'tp', 'tc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_correctly_predicted = [item for item in metric[\"word\"].items() if item[1]['tc'] > 0]\n",
    "print(len(words_correctly_predicted))\n",
    "display(words_correctly_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common train words\n",
    "[w.decode() for w, f in sorted(bow_dict['freq'].items(), reverse=True, key=lambda t: t[1])][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(single_dev_ref_words.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_bow_words(single_dev_ref_words, \n",
    "                  dev_utt_preds_words, \n",
    "                  bow_dict, \n",
    "                  map_dict[\"fisher_dev\"], display_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval refs found, loading\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using 1 reference\n",
      "--------------------\n",
      "precision=84.85, recall=69.54\n",
      "--------------------\n",
      "39 out of 40 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=92.82, recall=87.38\n",
      "--------------------\n",
      "36 out of 39 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "MT task - using all 4 references\n",
      "--------------------------------------------------------------------------------\n",
      "precision=76.60, recall=67.74\n",
      "--------------------------------------------------------------------------------\n",
      "4 references bleu=45.204494\n",
      "--------------------------------------------------------------------------------\n",
      "using min len filter\n",
      "--------------------\n",
      "3977 out of 3979 have len >= 1\n",
      "BLEU=45.21\n",
      "precision=76.60, recall=67.74\n"
     ]
    }
   ],
   "source": [
    "_ = eval_nmt_model(\"\", use_google=True, min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval refs found, loading\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=90.70, recall=83.11\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "always -- chicago -- colombia -- beautiful -- going -- thinking -- close -- topic -- coming -- nothing -- united -- states -- married -- example -- things -- spanish -- school -- seeing -- brothers -- stayed -- still -- living -- brother -- father -- college -- hours -- problems -- thing -- husband -- friends -- speak -- general -- everything -- happened -- never -- child -- whatever -- asked -- understand -- leave -- matter -- different -- wanted -- without -- mexico -- lives -- study -- usually -- email -- terrible -- classes -- class -- studying -- getting -- hello -- interested -- spend -- reason -- state -- phone -- crazy -- spain -- travel -- parents -- neither -- words -- parts -- sleep -- country -- knows -- place -- spoke -- relax -- anywhere -- places -- idaho -- difficult -- check -- careful -- dominican -- white -- colorado -- forty\n",
      "number of words: 83\n",
      "Top 5 words present: \n",
      "always -- things -- spanish -- never -- husband\n",
      "--------------------\n",
      "76 out of 83 retrieved with atleast 1 correct instance\n",
      "eval refs found, loading\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=100.00, recall=21.88\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "skirt -- husbands -- barranquilla -- interpret -- doctrine -- interpreting -- sinner -- reincarnation -- planet -- institutions -- certificate -- filter -- folder -- cultivate -- lessons -- approval -- awake -- agitated -- queen -- elvis -- crespo -- parks -- coupons -- haiti -- jamaica -- chiapas -- oaxaca -- aruba -- rocky -- flights -- monterrey -- insecurity -- plains -- volcanoes -- abandon -- forgets -- rainy -- protest -- rocker -- fishing -- exwife -- custody -- basement -- strangers -- headache -- naturally -- highrises -- singapore -- brutal -- cornell -- intimidated -- frequently -- kidnap -- kidnapping -- thieves -- olaya -- javier -- province -- passport -- chamber -- optional -- nasty -- reads\n",
      "number of words: 63\n",
      "Top 5 words present: \n",
      "haiti -- skirt -- chiapas -- plains -- javier\n",
      "--------------------\n",
      "17 out of 63 retrieved with atleast 1 correct instance\n",
      "eval refs found, loading\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=92.28, recall=83.67\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "change -- time -- women -- remember -- waiting -- first -- name -- years -- live -- home -- coming -- house -- died -- legs -- morning -- another -- even -- heart -- life -- girl -- kill -- want -- help -- please -- need -- send -- city -- lives -- free -- service -- make -- brought -- death -- lost -- situation -- terrible -- news -- names -- thousands -- number -- saying -- someone -- case -- watch -- love -- town -- leave -- give -- water -- teenager -- found -- government -- high -- stay -- damaged -- killed -- public -- huge -- black -- tonight -- text\n",
      "number of words: 61\n",
      "Top 5 words present: \n",
      "time -- want -- years -- live -- city\n",
      "--------------------\n",
      "53 out of 61 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "metrics = eval_all_word_lists(\"\", use_google=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 150 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict_path = os.path.join(m_cfg['data_path'], \"mix_sim.dict\")\n",
    "sim_dict = pickle.load(open(sim_dict_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_dict_es['freq_dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in sim_dict[\"w\"]:\n",
    "#     if len(sim_dict[\"w\"][w]) > 1 and w in bow_dict_es[\"w2i\"]:\n",
    "#         print(w)\n",
    "#         print(sim_dict[\"w\"][w])\n",
    "#         print(bow_dict_es[\"w2i\"][w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\", min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=82.44, recall=65.59\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "spanish -- school -- going -- chicago -- brother -- college -- colombia -- friends -- everything -- things -- always -- beautiful -- thinking -- nothing -- married -- united -- states -- example -- thing -- brothers -- stayed -- still -- husband -- coming -- living -- father -- hours -- problems -- wanted -- topic -- seeing -- never -- asked -- matter -- understand -- different -- country -- happened -- child -- whatever -- leave -- words -- lives -- places -- mexico -- study -- close -- getting -- spend -- hello -- email -- speak -- class -- classes -- terrible -- general -- without -- studying -- spain -- phone -- interested -- reason -- state -- crazy -- parents -- neither -- spoke -- sleep -- knows -- place -- relax -- idaho -- careful -- check -- dominican -- anywhere -- difficult -- travel -- usually -- white -- parts -- colorado -- forty\n",
      "number of words: 83\n",
      "Top 5 words present: \n",
      "always -- things -- spanish -- never -- husband\n",
      "--------------------\n",
      "71 out of 83 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=100.00, recall=6.45\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "barranquilla -- husbands -- skirt -- interpret -- doctrine -- interpreting -- sinner -- reincarnation -- planet -- institutions -- certificate -- filter -- folder -- awake -- agitated -- cultivate -- lessons -- approval -- elvis -- crespo -- parks -- coupons -- haiti -- jamaica -- flights -- chiapas -- aruba -- rocky -- monterrey -- insecurity -- plains -- volcanoes -- queen -- abandon -- forgets -- oaxaca -- rainy -- rocker -- protest -- strangers -- headache -- exwife -- custody -- basement -- fishing -- highrises -- singapore -- cornell -- brutal -- naturally -- olaya -- intimidated -- frequently -- kidnap -- thieves -- javier -- optional -- chamber -- province -- passport -- nasty -- reads\n",
      "number of words: 62\n",
      "Top 5 words present: \n",
      "haiti -- skirt -- chiapas -- plains -- javier\n",
      "--------------------\n",
      "5 out of 62 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=80.22, recall=62.09\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "name -- live -- house -- first -- time -- home -- another -- even -- women -- coming -- stay -- died -- legs -- morning -- years -- change -- remember -- waiting -- want -- names -- kill -- send -- help -- heart -- case -- life -- girl -- lives -- city -- free -- service -- brought -- death -- lost -- need -- please -- saying -- someone -- number -- terrible -- news -- thousands -- give -- government -- love -- watch -- water -- teenager -- town -- high -- make -- found -- leave -- public -- damaged -- situation -- killed -- huge -- black -- text -- tonight\n",
      "number of words: 61\n",
      "Top 5 words present: \n",
      "time -- want -- live -- years -- city\n",
      "--------------------\n",
      "50 out of 61 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "metrics = eval_all_word_lists(\"sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\", \n",
    "                              use_google=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU script\n",
    "```\n",
    "[bonnybridge]s1444673: export BLEU_SCRIPT=/afs/inf.ed.ac.uk/group/project/lowres/work/installs/mosesdecoder/scripts/generic/multi-bleu.perl\n",
    "[bonnybridge]s1444673: export PREDS=sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "BLEU = 29.44, 65.1/38.4/22.8/13.7 (BP=0.991, ratio=0.991, hyp_len=39719, ref_len=40096)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_test_fisher_test_en.ref* < $PREDS/fsh_test_fisher_test_hyp\n",
    "BLEU = 29.64, 66.2/38.4/22.7/13.5 (BP=0.999, ratio=0.999, hyp_len=39201, ref_len=39257)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref[0,1,2]* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "\n",
    "BLEU = 27.03, 62.7/35.8/20.7/12.1 (BP=0.987, ratio=0.987, hyp_len=39719, ref_len=40242)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref[1,2,3]* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "BLEU = 27.03, 62.6/35.8/20.8/12.2 (BP=0.984, ratio=0.984, hyp_len=39719, ref_len=40353)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref[2,3,0]* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "BLEU = 27.00, 62.8/35.9/20.8/12.1 (BP=0.984, ratio=0.984, hyp_len=39719, ref_len=40346)\n",
    "[bonnybridge]s1444673: perl $BLEU_SCRIPT $PREDS/fsh_dev_fisher_dev_en.ref[1,3,0]* < $PREDS/fsh_dev_fisher_dev_hyp\n",
    "BLEU = 27.10, 62.9/35.9/20.8/12.2 (BP=0.985, ratio=0.985, hyp_len=39719, ref_len=40339)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 50 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_metrics = eval_nmt_model(\"./sp2enw_mel-80_vocab-nltk/sp_0.33_h-256_e-128_l2e-3_lstm_drpt-0.3_cnn-32-2-2_rnn-3_b-40-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 25 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw_mel-80_vocab-nltk/sp_0.16_h-256_e-128_l2e-3_lstm_drpt-0.3_cnn-32-2-2_rnn-3_b-80-25_no-ln-bn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 15 hours model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw/sp_.10/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 50 hours model - sample word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed: 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.33_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.33_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_sample/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seed: AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new50_metrics = eval_nmt_model(\"emb_sp2enw/sp_0.33_seed-AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new50_metrics = eval_nmt_model(\"emb_sp2enw/sp_0.33_seed-AA_mix-0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin 80 hours model - sample word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.50_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"sp2enw_mel-80_vocab-nltk/sp_0.50_h-300_e-128_l2e-6_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_sample/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interspeech results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\", min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=62.70, recall=42.57\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "spanish -- school -- chicago -- brother -- phone -- college -- going -- married -- colombia -- friends -- everything -- things -- always -- thinking -- nothing -- united -- states -- example -- brothers -- still -- husband -- stayed -- coming -- lives -- hours -- problems -- country -- understand -- wanted -- topic -- interested -- thing -- seeing -- never -- matter -- different -- happened -- child -- living -- mexico -- study -- studying -- close -- hello -- email -- travel -- speak -- class -- classes -- terrible -- general -- without -- spain -- place -- words -- spend -- reason -- state -- parents -- neither -- whatever -- sleep -- knows -- crazy -- father -- spoke -- asked -- relax -- places -- idaho -- careful -- check -- dominican -- difficult -- white -- beautiful -- getting -- colorado -- parts -- leave -- anywhere -- forty -- usually\n",
      "number of words: 83\n",
      "Top 5 words present: \n",
      "always -- things -- spanish -- never -- husband\n",
      "--------------------\n",
      "55 out of 83 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=0.00, recall=0.00\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "barranquilla -- husbands -- skirt -- interpret -- doctrine -- interpreting -- sinner -- reincarnation -- planet -- institutions -- certificate -- filter -- folder -- awake -- agitated -- cultivate -- lessons -- approval -- elvis -- crespo -- parks -- coupons -- haiti -- jamaica -- flights -- chiapas -- aruba -- rocky -- monterrey -- insecurity -- plains -- volcanoes -- queen -- abandon -- forgets -- oaxaca -- rainy -- rocker -- protest -- strangers -- headache -- exwife -- custody -- basement -- fishing -- highrises -- singapore -- cornell -- brutal -- naturally -- olaya -- intimidated -- frequently -- kidnap -- thieves -- javier -- optional -- chamber -- province -- passport -- nasty -- reads\n",
      "number of words: 62\n",
      "Top 5 words present: \n",
      "haiti -- skirt -- chiapas -- plains -- javier\n",
      "--------------------\n",
      "0 out of 62 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=62.07, recall=35.74\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "name -- live -- case -- first -- time -- home -- another -- even -- women -- give -- coming -- lives -- house -- died -- legs -- make -- morning -- years -- change -- want -- remember -- waiting -- girl -- kill -- help -- love -- heart -- life -- city -- free -- watch -- service -- terrible -- killed -- brought -- death -- lost -- need -- please -- saying -- someone -- send -- number -- news -- names -- stay -- thousands -- water -- teenager -- found -- town -- government -- high -- damaged -- situation -- leave -- public -- huge -- black -- text -- tonight\n",
      "number of words: 61\n",
      "Top 5 words present: \n",
      "time -- want -- live -- years -- city\n",
      "--------------------\n",
      "37 out of 61 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "_ = eval_all_word_lists(\"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_50_da = eval_nmt_model(\"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5_sample-mix-0.5\", \n",
    "                             min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_all_word_lists(\"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5_sample-mix-0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_nmt_model(\"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\", min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=71.50, recall=52.60\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "spanish -- school -- chicago -- brother -- college -- colombia -- friends -- things -- thing -- always -- thinking -- nothing -- married -- united -- states -- example -- brothers -- going -- still -- stayed -- wanted -- coming -- living -- hours -- problems -- husband -- topic -- everything -- speak -- seeing -- never -- asked -- matter -- understand -- different -- happened -- child -- without -- phone -- lives -- places -- mexico -- study -- studying -- close -- spend -- hello -- email -- class -- classes -- words -- terrible -- general -- spain -- interested -- place -- spoke -- country -- reason -- state -- parents -- neither -- whatever -- sleep -- knows -- crazy -- relax -- idaho -- careful -- difficult -- check -- dominican -- father -- travel -- beautiful -- white -- colorado -- leave -- anywhere -- parts -- forty -- usually -- getting\n",
      "number of words: 83\n",
      "Top 5 words present: \n",
      "always -- things -- spanish -- never -- husband\n",
      "--------------------\n",
      "64 out of 83 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=100.00, recall=1.08\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "barranquilla -- husbands -- skirt -- interpret -- doctrine -- interpreting -- sinner -- reincarnation -- planet -- institutions -- certificate -- filter -- folder -- awake -- agitated -- cultivate -- lessons -- approval -- elvis -- crespo -- parks -- coupons -- haiti -- jamaica -- flights -- chiapas -- aruba -- rocky -- monterrey -- insecurity -- plains -- volcanoes -- queen -- abandon -- forgets -- oaxaca -- rainy -- rocker -- protest -- strangers -- headache -- exwife -- custody -- basement -- fishing -- highrises -- singapore -- cornell -- brutal -- naturally -- olaya -- intimidated -- frequently -- kidnap -- thieves -- javier -- optional -- chamber -- province -- passport -- nasty -- reads\n",
      "number of words: 62\n",
      "Top 5 words present: \n",
      "haiti -- skirt -- chiapas -- plains -- javier\n",
      "--------------------\n",
      "1 out of 62 retrieved with atleast 1 correct instance\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------\n",
      "--------------------------------------------------------------------------------\n",
      "BOW - using all 4 references\n",
      "--------------------\n",
      "precision=71.99, recall=46.93\n",
      "--------------------------------------------------------------------------------\n",
      "Using word list: \n",
      "name -- live -- first -- time -- home -- another -- house -- even -- women -- coming -- died -- legs -- make -- need -- morning -- years -- change -- want -- remember -- waiting -- kill -- give -- help -- heart -- life -- girl -- lives -- free -- service -- brought -- city -- death -- lost -- please -- saying -- someone -- send -- teenager -- number -- terrible -- news -- names -- thousands -- love -- town -- watch -- black -- water -- stay -- found -- government -- high -- damaged -- situation -- leave -- killed -- public -- huge -- case -- text -- tonight\n",
      "number of words: 61\n",
      "Top 5 words present: \n",
      "time -- want -- live -- years -- city\n",
      "--------------------\n",
      "41 out of 61 retrieved with atleast 1 correct instance\n"
     ]
    }
   ],
   "source": [
    "_ = eval_all_word_lists(\"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_80_da = eval_nmt_model(\"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_mix-0.5/\", min_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eval_all_word_lists(\"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_mix-0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_out_str(out_str):\n",
    "    out_str = out_str.replace(\"`\", \"\")\n",
    "    out_str = out_str.replace('\"', '')\n",
    "    out_str = out_str.replace('', '')\n",
    "    out_str = out_str.replace(\"''\", \"\")\n",
    "    out_str = out_str.strip()\n",
    "    return out_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file_len_filtered_preds(nmt_path, \n",
    "                                     set_key=\"fisher_dev\", \n",
    "                                     min_len=0, max_len=300, \n",
    "                                     use_gooogle=False):\n",
    "    refs, hyps, allrefs = get_model_data(nmt_path, use_google=use_gooogle)\n",
    "    filt_utts = []\n",
    "    for u in refs:\n",
    "        if (len(map_dict[set_key][u][\"es_w\"]) >= min_len and \n",
    "           len(map_dict[set_key][u][\"es_w\"]) <= max_len):\n",
    "            filt_utts.append(u)\n",
    "    \n",
    "    filt_utts = sorted(filt_utts)\n",
    "    print(\"Utts matching len filter={0:d}\".format(len(filt_utts)))\n",
    "    hyp_path = os.path.join(nmt_path, \"hyps_min-{0:d}_max-{1:d}.en\".format(min_len, max_len))\n",
    "    print(\"writing hyps to: {0:s}\".format(hyp_path))\n",
    "    with open(hyp_path, \"w\") as out_f:\n",
    "        for u in filt_utts:\n",
    "            if use_gooogle:\n",
    "                out_str = \" \".join(hyps[u])\n",
    "            else:\n",
    "                out_str = \"\"\n",
    "                for w in hyps[u]:\n",
    "                    out_str += \"{0:s}\".format(w) if (w.startswith(\"'\") or w==\"n't\") else \" {0:s}\".format(w)\n",
    "                \n",
    "                out_str = clean_out_str(out_str)\n",
    "                \n",
    "            out_f.write(\"{0:s}\\n\".format(out_str))\n",
    "    \n",
    "    for i in range(len(list(allrefs.values())[0])):\n",
    "        refs_path = os.path.join(nmt_path, \"ref_min-{0:d}_max-{1:d}.en{2:d}\".format(min_len, \n",
    "                                                                                    max_len,\n",
    "                                                                                    i))\n",
    "        print(\"writing ref {0:d} to: {1:s}\".format(i, refs_path))\n",
    "        with open(refs_path, \"w\") as out_f:\n",
    "            for u in filt_utts:\n",
    "                if use_gooogle:\n",
    "                    out_str = \" \".join(allrefs[u][i])\n",
    "                else:\n",
    "                    out_str = \"\"\n",
    "                    for w in allrefs[u][i]:\n",
    "                        out_str += \"{0:s}\".format(w) if (w.startswith(\"'\") or w==\"n't\") else \" {0:s}\".format(w)\n",
    "                    out_str = clean_out_str(out_str)\n",
    "                out_f.write(\"{0:s}\\n\".format(out_str))\n",
    "    print(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmt_path = \"google\"\n",
    "# write_to_file_len_filtered_preds(nmt_path, \n",
    "#                                  set_key=\"fisher_dev\", \n",
    "#                                  min_len=MIN_LEN, max_len=MAX_LEN, \n",
    "#                                  use_gooogle=True)\n",
    "\n",
    "# !paste -d\"\\n\" google/ref_min-0_max-2.en* > google/all_ref_min-0_max-2_meteor\n",
    "# # !paste -d\"\\n\" google/ref_min-{$MIN_LEN}_max-{$MAX_LEN}.en* > google/all_ref_min-{$MIN_LEN}_max-{$MAX_LEN}_meteor\n",
    "# # !paste -d\"\\n\" $nmt_path/ref_min-{$MIN_LEN}_max-{$MAX_LEN}.en* > google/$meteor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINLEN = 25\n",
    "MAXLEN = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_models = [\"google\",\n",
    "              \"sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2\",\n",
    "              \"sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2_mix-0.5/\",\n",
    "              \"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\",\n",
    "              \"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_mix-0.5/\",\n",
    "              \"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\",\n",
    "              \"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5_sample-mix-0.5\"\n",
    "              \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nmt_path in nmt_models:\n",
    "    write_to_file_len_filtered_preds(nmt_path, \n",
    "                                     set_key=\"fisher_dev\", \n",
    "                                     min_len=MINLEN, max_len=MAXLEN, \n",
    "                                     use_gooogle = nmt_path == \"google\")\n",
    "    print(nmt_path == \"google\")\n",
    "    \n",
    "    meteor_out = os.path.join(nmt_path, \"meteor_4refs_min-{0:d}_max-{1:d}.en\".format(MINLEN, MAXLEN))\n",
    "    meteor_in = os.path.join(nmt_path, \"ref_min-{0:d}_max-{1:d}.en\".format(MINLEN, MAXLEN))\n",
    "    \n",
    "    !paste -d\"\\n\" $meteor_in* > $meteor_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "export PREDS=haha\n",
    "perl $BLEU_SCRIPT $PREDS/ref_min-0_max-300.* < $PREDS/hyps_min-0_max-300.en\n",
    "\n",
    "java -Xmx2G -jar ../installs/meteor-1.5/meteor-*.jar $PREDS/hyps_min-0_max-300.en $PREDS/meteor_4refs_min-0_max-300.en -r 4 -l en -norm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keys = [\"google\",\n",
    "              \"sp_160\",\n",
    "              \"sp_160_sample\",\n",
    "              \"sp_80\",\n",
    "              \"sp_80_sample\",\n",
    "              \"sp_50\",\n",
    "              \"sp_50_sample\"\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {model_keys[i]: nmt_models[i] for i in range(len(model_keys))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_filts = [(0,2), (3,5), (6,20), (21,40), (41,300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model_map:\n",
    "    model_data[m] = get_model_data(model_map[m], use_google= m == \"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_bucks = {i:[] for i in range(len(len_filts))}\n",
    "for u in map_dict[\"fisher_dev\"]:\n",
    "    es_w_len = len(map_dict[\"fisher_dev\"][u][\"es_w\"])\n",
    "    for i, f in enumerate(len_filts):\n",
    "        if es_w_len >= f[0] and es_w_len <= f[1]:\n",
    "            u_bucks[i].append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i,len(v)) for i, v in u_bucks.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"hmm\")\n",
    "selected_utts = []\n",
    "for i in u_bucks:\n",
    "    sub_set = random.sample(u_bucks[i], min(10,len(u_bucks[i])))\n",
    "    selected_utts.extend(sub_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_utt(utt, m_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_model_preds.txt\", \"w\") as out_f:\n",
    "    for u in selected_utts:\n",
    "        out_f.write(\"------{0:s}------\\n\".format(u))\n",
    "        es_words = \" \".join([w.decode() for w in map_dict[\"fisher_dev\"][u][\"es_w\"]])\n",
    "        out_f.write(\"{0:20s} : {1:s}\\n\".format(\"es reference\", es_words))\n",
    "        out_f.write(\"{0:20s} : {1:s}\\n\".format(\"en reference\", \" \".join(model_data[\"google\"][0][u])))\n",
    "        for m in model_data:\n",
    "            if m == \"google\":\n",
    "                out_str =  \" \".join(model_data[m][1][u])\n",
    "            else:\n",
    "                out_str = \"\"\n",
    "                for w in model_data[m][1][u]:\n",
    "                    out_str += \"{0:s}\".format(w) if (w.startswith(\"'\") or w==\"n't\") else \" {0:s}\".format(w)\n",
    "            out_f.write(\"{0:20s} : {1:s}\\n\".format(m, out_str))\n",
    "        out_f.write(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"wavs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STARTHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in selected_utts:\n",
    "    print(\"------{0:s}------\".format(u))\n",
    "    play_utt(u, map_dict[\"fisher_dev\"])\n",
    "    es_words = \" \".join([w.decode() for w in map_dict[\"fisher_dev\"][u][\"es_w\"]])\n",
    "    print(\"{0:20s} : {1:s}\".format(\"es reference\", es_words))\n",
    "    print(\"{0:20s} : {1:s}\".format(\"en reference\", \" \".join(model_data[\"google\"][0][u])))\n",
    "    for m in model_data:\n",
    "        if m == \"google\":\n",
    "            out_str =  \" \".join(model_data[m][1][u])\n",
    "        else:\n",
    "            out_str = \"\"\n",
    "            for w in model_data[m][1][u]:\n",
    "                out_str += \"{0:s}\".format(w) if (w.startswith(\"'\") or w==\"n't\") else \" {0:s}\".format(w)\n",
    "        print(\"{0:20s} : {1:s}\".format(m, out_str))\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!paste -d\"\\n\" $nmt_path/ref_min-0_max-2.en* > $nmt_path/all_ref_min-0_max-2_meteor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(en_data['google']['4refs'].values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_data['google'] = {}\n",
    "en_data['google']['refs'], en_data['google']['hyps'], en_data['google']['4refs'] = get_model_data(\"\", \n",
    "                                                                                                  use_google=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_path = \"sp2enw_hyp_search/sp_1.0_l2e-4_rnn-3_drpt-0.5_cnn_96-2-2_mix-0.5/\"\n",
    "en_data['160hrs_da'] = {}\n",
    "en_data['160hrs_da']['refs'], en_data['160hrs_da']['hyps'], en_data['160hrs_da']['4refs'] = get_model_data(nmt_path, \n",
    "                                                                                               use_google=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_path = \"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln\"\n",
    "en_data['80hrs'] = {}\n",
    "en_data['80hrs']['refs'], en_data['80hrs']['hyps'], en_data['80hrs']['4refs'] = get_model_data(nmt_path, \n",
    "                                                                                               use_google=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_path = \"./sp2enw_interspeech/sp_0.50_h-300_e-128_l2e-4_lstm_drpt-0.5_cnn-64-2-2_rnn-3_b-80-25_no-bn-ln_mix-0.5\"\n",
    "en_data['80hrs_da'] = {}\n",
    "en_data['80hrs_da']['refs'], en_data['80hrs_da']['hyps'], en_data['80hrs_da']['4refs'] = get_model_data(nmt_path, \n",
    "                                                                                               use_google=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_path = \"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5\"\n",
    "en_data['50hrs'] = {}\n",
    "en_data['50hrs']['refs'], en_data['50hrs']['hyps'], en_data['50hrs']['4refs'] = get_model_data(nmt_path, \n",
    "                                                                                               use_google=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_path = \"./sp2enw_interspeech/sp_0.33_seed-AB_l2e-4_drpt-0.5_sample-mix-0.5\"\n",
    "en_data['50hrs_da'] = {}\n",
    "en_data['50hrs_da']['refs'], en_data['50hrs_da']['hyps'], en_data['50hrs_da']['4refs'] = get_model_data(nmt_path, \n",
    "                                                                                               use_google=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt = '20051023_232057_325_fsp-A-3'\n",
    "utt = '20051017_180712_270_fsp-B-2'\n",
    "utt = '20051009_182032_217_fsp-B-149'\n",
    "\n",
    "for m in en_data:\n",
    "    print(m, ' & ', \" \".join(en_data[m]['hyps'][utt]), ' \\\\\\\\')\n",
    "    print()\n",
    "    \n",
    "print(\" \".join(en_data[m]['refs'][utt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\", \".join([w.decode() for w in bow_dict['w2i'].keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_dict['freq'])-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"help\"\n",
    "data_key = \"80hrs_da\"\n",
    "\n",
    "print(bow_dict[\"freq\"][keyword.encode()], bow_dict[\"freq_dev\"][keyword.encode()])\n",
    "\n",
    "t_count = 0\n",
    "c_count = 0\n",
    "tp_count = 0\n",
    "corr_utts = []\n",
    "\n",
    "for u in en_data[data_key][\"hyps\"]:\n",
    "    common_ref_words = set(en_data[data_key][\"4refs\"][u][0])\n",
    "    for curr_ref in en_data[data_key][\"4refs\"][u][1:]:\n",
    "        common_ref_words &= set(curr_ref)\n",
    "#     if sum([1 if keyword in set(r) else 0 for r in en_data['50hrs_da'][\"4refs\"][u]]) >= 4:\n",
    "    if keyword in common_ref_words:\n",
    "        t_count += 1\n",
    "    if keyword in en_data[data_key][\"hyps\"][u]:\n",
    "        tp_count += 1\n",
    "    if keyword in en_data[data_key][\"hyps\"][u] and keyword in common_ref_words:\n",
    "        c_count += 1\n",
    "        corr_utts.append(u)\n",
    "\n",
    "print(t_count, c_count, tp_count)\n",
    "        \n",
    "for u in corr_utts:\n",
    "    print(\" \".join(en_data[data_key][\"hyps\"][u]))\n",
    "    for r in en_data[data_key][\"4refs\"][u]:\n",
    "        if keyword in r:\n",
    "            print(\" \".join(r))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_50_da[1]['word'][keyword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = sum([model_50_da[1]['word'][w]['tc'] for w in model_50_da[1]['word']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = sum([model_50_da[1]['word'][w]['t'] for w in model_50_da[1]['word']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc / tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in model_50_da[1]['word']:\n",
    "    if w in model_50_da[1]['word']: \n",
    "        p_50 = model_50_da[1]['word'][w]['tc'] / model_50_da[1]['word'][w]['t'] if model_50_da[1]['word'][w]['t'] > 0 else 0\n",
    "    else:\n",
    "        p_50 = 0\n",
    "    if w in model_80_da[1]['word']:\n",
    "        p_80 = model_80_da[1]['word'][w]['tc'] / model_80_da[1]['word'][w]['t'] if model_80_da[1]['word'][w]['t'] > 0 else 0\n",
    "    else:\n",
    "        p_80 = 0\n",
    "    print(w, \n",
    "          \"{0:20.1f}\".format(p_50 * 100),\n",
    "          \"{0:20.1f}\".format(p_80 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in model_50_da[1]['word']:\n",
    "    if model_50_da[1]['word'][w]['t'] > 0 and model_50_da[1]['word'][w]['tc'] / model_50_da[1]['word'][w]['t'] >= 0:\n",
    "        print(w, \"{0:.1f}\".format(model_50_da[1]['word'][w]['tc'] / model_50_da[1]['word'][w]['t'] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in model_80_da[1]['word']:\n",
    "    if model_80_da[1]['word'][w]['t'] > 0 and model_80_da[1]['word'][w]['tc'] / model_80_da[1]['word'][w]['t'] >= 0.4:\n",
    "        print(w, \"{0:.1f}\".format(model_80_da[1]['word'][w]['tc'] / model_80_da[1]['word'][w]['t'] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sorted_words = [w.decode() for w,f in sorted(bow_dict['freq'].items(), reverse=True, key=lambda t: t[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_top_K = 5\n",
    "top_K_words = freq_sorted_words[:predict_top_K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" --- \".join(top_K_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_preds = [top_K_words for u in google_hyp_r0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_utt_refs_words_bow.values(), dummy_preds)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_single_ref.values(), dummy_preds)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pred = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_p_vals = np.zeros((max_pred), dtype=\"f\")\n",
    "dummy_r_vals = np.zeros((max_pred), dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_dummy = {}\n",
    "# for thresh in tqdm(np.arange(min_prob, max_prob+thresh_delta,thresh_delta)):\n",
    "for num_pred in tqdm(range(0,max_pred)):\n",
    "    top_K_words = freq_sorted_words[:num_pred+1]\n",
    "    dummy_preds = [top_K_words for u in google_hyp_r0]\n",
    "    dummy_p_vals[num_pred], dummy_r_vals[num_pred] = basic_precision_recall(google_utt_refs_words_bow.values(), \n",
    "                                                                            dummy_preds)[:2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_p_vals /= 100.0\n",
    "dummy_r_vals /= 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trapz(dummy_p_vals, dummy_r_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels = range(1,max_pred+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(dummy_r_vals*100, dummy_p_vals*100, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(dummy_r_vals*100, dummy_p_vals*100, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic_precision_recall(single_dev_ref_words.values(), dummy_preds)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google refs vs Edin refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if len(r[0]) > 0 else 0 for r in single_dev_ref_words.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(set(r[0])-{'_UNK'})for r in single_dev_ref_words.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_count = 0\n",
    "for u in set(google_single_ref.keys()) & set(single_dev_ref_words.keys()):\n",
    "    if set(single_dev_ref_words[u][0]) - {'_UNK'} != set(google_single_ref[u][0]):\n",
    "        mismatch_count += max(len(set(single_dev_ref_words[u][0]) - set(google_single_ref[u][0])), \n",
    "                              len(set(google_single_ref[u][0]) - set(single_dev_ref_words[u][0])))\n",
    "        print(u, single_dev_ref_words[u], google_single_ref[u])\n",
    "        print((set(single_dev_ref_words[u][0]) - set(google_single_ref[u][0])), \n",
    "                              (set(google_single_ref[u][0]) - set(single_dev_ref_words[u][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mismatch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(set(r[0])-{'_UNK'})for r in single_dev_ref_words.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'_EOS': 2,\n",
       " b'_GO': 1,\n",
       " b'_PAD': 0,\n",
       " b'_UNK': 3,\n",
       " b'another': 10,\n",
       " b'case': 27,\n",
       " b'change': 28,\n",
       " b'city': 13,\n",
       " b'coming': 38,\n",
       " b'even': 9,\n",
       " b'first': 17,\n",
       " b'found': 35,\n",
       " b'gets': 31,\n",
       " b'girl': 22,\n",
       " b'give': 15,\n",
       " b'help': 25,\n",
       " b'high': 37,\n",
       " b'home': 24,\n",
       " b'house': 11,\n",
       " b'huge': 42,\n",
       " b'leave': 29,\n",
       " b'life': 18,\n",
       " b'live': 8,\n",
       " b'lives': 33,\n",
       " b'love': 21,\n",
       " b'make': 16,\n",
       " b'morning': 40,\n",
       " b'name': 12,\n",
       " b'need': 20,\n",
       " b'news': 39,\n",
       " b'people': 4,\n",
       " b'remember': 19,\n",
       " b'saying': 26,\n",
       " b'send': 32,\n",
       " b'someone': 14,\n",
       " b'stay': 30,\n",
       " b'terrible': 36,\n",
       " b'time': 5,\n",
       " b'town': 41,\n",
       " b'waiting': 43,\n",
       " b'want': 7,\n",
       " b'watch': 34,\n",
       " b'women': 23,\n",
       " b'years': 6}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_dict[\"w2i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
