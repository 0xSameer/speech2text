{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bow_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_s2t_hyps, google_s2t_refs, google_s2t_refs_for_eval = get_google_data()\n",
    "google_hyp_r0 = google_s2t_hyps['fisher_dev_r0']\n",
    "google_dev_ref_0 = google_s2t_refs['fisher_dev_ref_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"sp2bagwords/sp_0.33_h-256_rnn-bi-2_hwy-3_l2e-6_cnn-32-5-5_task-0-500words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict, bow_dict = get_data_dicts(m_cfg)\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\")):\n",
    "    dev_utts = pickle.load(open(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\"), \"rb\"))\n",
    "\n",
    "if os.path.exists(os.path.join(m_cfg['model_dir'], \"mean_pos_scores.dict\")):\n",
    "    mean_pos_scores = pickle.load(open(os.path.join(m_cfg['model_dir'], \"mean_pos_scores.dict\"), \"rb\"))\n",
    "    mean_neg_scores = pickle.load(open(os.path.join(m_cfg['model_dir'], \"mean_neg_scores.dict\"), \"rb\"))\n",
    "# batch_size = {'max': 128, 'med': 128, 'min': 128, 'scale': 1}\n",
    "batch_size = {'max': 64, 'med': 64, 'min': 64, 'scale': 1}\n",
    "batch_size = t_cfg['batch_size']\n",
    "\n",
    "edin_s2t_refs_for_eval_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"edin_s2t_refs_for_eval.dict\")\n",
    "edin_s2t_refs_for_eval = pickle.load(open(edin_s2t_refs_for_eval_path, \"rb\"))\n",
    "single_dev_redev_uttsf = [[i[0]] for i in dev_utts[\"refs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path = os.path.join(m_cfg['data_path'],\n",
    "#                                       m_cfg['train_set'])\n",
    "# train_utts, train_loss = feed_model(model,\n",
    "#                               optimizer=optimizer,\n",
    "#                               m_dict=map_dict[train_key],\n",
    "#                               b_dict=bucket_dict[train_key],\n",
    "#                               vocab_dict=vocab_dict,\n",
    "#                               bow_dict=bow_dict,\n",
    "#                               batch_size=batch_size,\n",
    "#                               x_key=enc_key,\n",
    "#                               y_key=dec_key,\n",
    "#                               train=True,\n",
    "#                               input_path=input_path,\n",
    "#                               max_dec=m_cfg['max_en_pred'],\n",
    "#                               t_cfg=t_cfg,\n",
    "#                               use_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "\n",
    "dev_utts, dev_loss = feed_model(model,\n",
    "                                optimizer=optimizer,\n",
    "                                m_dict=map_dict[dev_key],\n",
    "                                b_dict=bucket_dict[dev_key],\n",
    "                                vocab_dict=vocab_dict,\n",
    "                                bow_dict=bow_dict,\n",
    "                                batch_size=batch_size,\n",
    "                                x_key=enc_key,\n",
    "                                y_key=dec_key,\n",
    "                                train=False,\n",
    "                                input_path=input_path,\n",
    "                                max_dec=m_cfg['max_en_pred'],\n",
    "                                t_cfg=t_cfg,\n",
    "                                use_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dev_ref = [[i[0]] for i in dev_utts[\"refs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(dev_utts[\"probs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.1\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                               mean_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "p, r, _ = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.1\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                               mean_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_prob, max_prob = float(xp.min(dev_utts[\"probs\"])), float(xp.max(dev_utts[\"probs\"]))\n",
    "min_prob, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_thresh = {}\n",
    "thresh_delta = 0.05\n",
    "for thresh in tqdm(np.arange(min_prob, max_prob+thresh_delta,thresh_delta)):\n",
    "    p_r_thresh[thresh] = {}\n",
    "    dev_pred_words_at_thresh = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                                           thresh,\n",
    "                                                           len(bow_dict['i2w']))\n",
    "    p_r_thresh[thresh]['p'], p_r_thresh[thresh]['r'], _ = basic_precision_recall(dev_utts[\"refs\"], \n",
    "                                                                              dev_pred_words_at_thresh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_labels, p_vals, r_vals = [], [], []\n",
    "for l in p_r_thresh:\n",
    "    p_vals.append(p_r_thresh[l][\"p\"])\n",
    "    r_vals.append(p_r_thresh[l][\"r\"])\n",
    "    thresh_labels.append(\"{0:.2f}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(r_vals, p_vals, label=\"Precision/Recall\")\n",
    "for i,j,k in zip(r_vals, p_vals, thresh_labels):\n",
    "    ax.annotate(k,xy=(i,j), fontsize=12)\n",
    "ax.set_ylabel(\"Precision\", fontsize=28)\n",
    "ax.set_xlabel(\"Recall\", fontsize=26)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.3, 0.9),\n",
    "                  ncol=1, fancybox=True, shadow=True, fontsize=26)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.1\n",
    "print(\"using prediction threshold={0:.2f}\".format(PRED_THRESH))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           PRED_THRESH,\n",
    "                                           len(bow_dict['i2w']))\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_pred_words)\n",
    "print(p,r)\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_mean_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           mean_pos_scores,\n",
    "                                           m_cfg['max_en_pred'])\n",
    "p, r, _ = basic_precision_recall(single_dev_ref, dev_mean_pred_words)\n",
    "print(p,r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get preds and refs in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_utt_preds_words = {}\n",
    "dev_utt_refs_words = {}\n",
    "for u, p, refs in zip(dev_utts['ids'], dev_pred_words, dev_utts[\"refs\"]):\n",
    "    dev_utt_preds_words[u] = list(set([bow_dict['i2w'][i].decode() for i in p]))\n",
    "    dev_utt_refs_words[u] = []\n",
    "    for r in refs:\n",
    "        #print(r)\n",
    "        dev_utt_refs_words[u].append([bow_dict['i2w'][i].decode() for i in set(r)])\n",
    "single_dev_ref_words = {u: [dev_utt_refs_words[u][0]] for u in dev_utt_refs_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, _ = basic_precision_recall(dev_utt_refs_words.values(), dev_utt_preds_words.values())\n",
    "p, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, metric = basic_precision_recall(single_dev_ref_words.values(), dev_utt_preds_words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, metric[k]) for k in ['rc', 'rt', 'tp', 'tc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item for item in metric[\"word\"].items() if item[1]['tc'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b\"finish\" in bow_dict['w2i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([w.decode() for w in map_dict[\"fisher_dev\"]['20051018_210220_279_fsp-A-41'][\"en_w\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[single_dev_ref_words[u] for u in dev_utts[\"ids\"][:5]], single_dev_ref[:5], dev_utts[\"ids\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_bow_words(edin_s2t_refs_for_eval, dev_utt_preds_words, bow_dict, map_dict[\"fisher_dev\"], display_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_utt_preds_words_bow = {}\n",
    "google_single_ref = {}\n",
    "google_utt_refs_words_bow = {}\n",
    "for u in dev_utts['ids']:\n",
    "    google_utt_preds_words_bow[u] = list(get_words_in_bow_vocab(google_hyp_r0[u], bow_dict))\n",
    "    google_utt_refs_words_bow[u] = []\n",
    "    google_single_ref[u] = [list(get_words_in_bow_vocab(google_dev_ref_0[u], bow_dict))]\n",
    "    for r in google_s2t_refs:\n",
    "        google_utt_refs_words_bow[u].append(list(get_words_in_bow_vocab(google_s2t_refs[r][u], bow_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_utt_refs_words_bow.values(), google_utt_preds_words_bow.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_single_ref.values(), google_utt_preds_words_bow.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_bow_words(google_s2t_refs_for_eval, google_hyp_r0, bow_dict, map_dict[\"fisher_dev\"], display_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MT PRECISION RECALL - NOOOT BOW\n",
    "basic_precision_recall(google_s2t_refs_for_eval.values(), google_hyp_r0.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_top_K = 5\n",
    "top_K_words = [w.decode() for w,f in sorted(bow_dict['freq'].items(), reverse=True, key=lambda t: t[1])][:predict_top_K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" --- \".join(top_K_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_preds = [top_K_words for u in google_hyp_r0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_utt_refs_words_bow.values(), dummy_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_single_ref.values(), dummy_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(single_dev_ref_words.values(), dummy_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google refs vs Edin refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_count = 0\n",
    "for u in set(google_single_ref.keys()) & set(single_dev_ref_words.keys()):\n",
    "    if set(single_dev_ref_words[u][0]) != set(google_single_ref[u][0]):\n",
    "        mismatch_count += max(len(set(single_dev_ref_words[u][0]) - set(google_single_ref[u][0])), \n",
    "                              len(set(google_single_ref[u][0]) - set(single_dev_ref_words[u][0])))\n",
    "        print(u, single_dev_ref_words[u], google_single_ref[u])\n",
    "        print((set(single_dev_ref_words[u][0]) - set(google_single_ref[u][0])), \n",
    "                              (set(google_single_ref[u][0]) - set(single_dev_ref_words[u][0])))\n",
    "print(mismatch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## darkness lies beyond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.mean(mean_pos_scores), xp.mean(mean_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.std(mean_pos_scores), xp.std(mean_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_dict['w2i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bow_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           0.4,\n",
    "                                           m_cfg['max_en_pred'])\n",
    "print(basic_precision_recall(dev_utts[\"refs\"], dev_pred_words))\n",
    "print(\"using mean positive prediction threshold\")\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                               mean_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "print(basic_precision_recall(dev_utts[\"refs\"], dev_pred_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    dev_utts, train_loss = feed_model(model,\n",
    "                                optimizer=optimizer,\n",
    "                                m_dict=map_dict[m_cfg['train_set']],\n",
    "                                b_dict=bucket_dict[m_cfg['train_set']],\n",
    "                                vocab_dict=vocab_dict,\n",
    "                                bow_dict=bow_dict,\n",
    "                                batch_size=batch_size,\n",
    "                                x_key=enc_key,\n",
    "                                y_key=dec_key,\n",
    "                                train=True,\n",
    "                                input_path=os.path.join(m_cfg['data_path'], m_cfg['train_set']),\n",
    "                                max_dec=m_cfg['max_en_pred'],\n",
    "                                t_cfg=t_cfg,\n",
    "                                use_y=True)\n",
    "    print(train_loss)\n",
    "    print(\"using threshold of 0.1\")\n",
    "    train_pred_words = get_pred_words_from_probs(train_utts[\"probs\"],\n",
    "                                           0.1,\n",
    "                                           m_cfg['max_en_pred'])\n",
    "    print(basic_precision_recall(train_utts[\"refs\"], train_pred_words))\n",
    "    print(\"using mean positive prediction threshold\")\n",
    "    train_pred_words = get_pred_words_from_probs(train_utts[\"probs\"],\n",
    "                                               mean_pos_scores,\n",
    "                                               m_cfg['max_en_pred'])\n",
    "    print(basic_precision_recall(train_utts[\"refs\"], train_pred_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fil = m_cfg['model_fname']\n",
    "# print(\"Saving model\")\n",
    "# serializers.save_npz(model_fil.replace(\".model\", \"_{0:d}.model\".format(20)), model)\n",
    "# print(\"Finished saving model\")\n",
    "# print(\"Saving optimizer\")\n",
    "# serializers.save_npz(m_cfg['opt_fname'], optimizer)\n",
    "# print(\"Finished saving optimizer\")\n",
    "# print(\"Saving utterance predictions\")\n",
    "\n",
    "# pickle.dump(utts, open(os.path.join(m_cfg['model_dir'], \"model_s2t_dev_out.dict\"), \"wb\"))\n",
    "# pickle.dump(mean_pos_scores, open(os.path.join(m_cfg['model_dir'], \"mean_pos_scores.dict\"), \"wb\"))\n",
    "# pickle.dump(mean_neg_scores, open(os.path.join(m_cfg['model_dir'], \"mean_neg_scores.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utts[\"probs\"][:2,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_utts[\"probs\"][:2,:10], train_utts[\"refs\"][:2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pos_scores = xp.array([0.0 for i in bow_dict[\"i2w\"]], dtype=\"f\")\n",
    "mean_neg_scores = xp.array([0.0 for i in bow_dict[\"i2w\"]], dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, len(bow_dict[\"i2w\"])):\n",
    "    this_word = bow_dict[\"i2w\"][i]\n",
    "    print(this_word, bow_dict[\"freq\"][this_word])\n",
    "    pos_indx = [i in r[0] for r in train_utts[\"refs\"]]\n",
    "    neg_indx = [i not in r[0] for r in train_utts[\"refs\"]]\n",
    "    mean_pos_scores[i] = np.mean(F.sigmoid(train_utts[\"probs\"][:,i][pos_indx]).data)\n",
    "    mean_neg_scores[i] = np.mean(F.sigmoid(train_utts[\"probs\"][:,i][neg_indx]).data)\n",
    "    print(mean_pos_scores[i], train_utts[\"probs\"][:,i][pos_indx].shape)\n",
    "    print(mean_neg_scores[i], train_utts[\"probs\"][:,i][neg_indx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_neg_scores, mean_pos_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_index = 4\n",
    "this_word = bow_dict[\"i2w\"][check_index]\n",
    "print(this_word, bow_dict[\"freq\"][this_word])\n",
    "pos_indx = [check_index in r[0] for r in train_utts[\"refs\"]]\n",
    "neg_indx = [check_index not in r[0] for r in train_utts[\"refs\"]]\n",
    "print(np.mean(F.sigmoid(train_utts[\"probs\"][:,check_index][pos_indx]).data), train_utts[\"probs\"][:,check_index][pos_indx].shape)\n",
    "print(np.mean(F.sigmoid(train_utts[\"probs\"][:,check_index][neg_indx]).data), train_utts[\"probs\"][:,check_index][neg_indx].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.all(train_utts[\"probs\"][:10] == train_utts[\"probs\"][10:20]), train_utts[\"preds\"][:10] == train_utts[\"preds\"][10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prec, train_rec = basic_precision_recall(train_utts[\"refs\"], train_utts[\"preds\"])\n",
    "print(train_prec, train_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.1\n",
    "pred_words = []\n",
    "pred_limit = m_cfg['max_en_pred']\n",
    "for row in F.sigmoid(train_utts[\"probs\"]).data:\n",
    "    pred_inds = xp.where(row >=PRED_THRESH)[0]\n",
    "    if len(pred_inds) > pred_limit:\n",
    "        pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    pred_words.append([i for i in pred_inds.tolist() if i > 3])\n",
    "    print(row)\n",
    "    print(mean_pos_scores)\n",
    "    print(pred_inds)\n",
    "    break\n",
    "\n",
    "train_prec, train_rec = basic_precision_recall(train_utts[\"refs\"], pred_words)\n",
    "print(train_prec, train_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words = []\n",
    "pred_limit = m_cfg['max_en_pred']\n",
    "for row in F.sigmoid(train_utts[\"probs\"]).data:\n",
    "    pred_inds = xp.where(row >= mean_pos_scores)[0]\n",
    "    if len(pred_inds) > pred_limit:\n",
    "        pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    pred_words.append([i for i in pred_inds.tolist() if i > 3])\n",
    "    print(row)\n",
    "    print(mean_pos_scores)\n",
    "    print(pred_inds)\n",
    "    break\n",
    "\n",
    "train_prec, train_rec = basic_precision_recall(train_utts[\"refs\"], pred_words)\n",
    "print(train_prec, train_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words[:5], train_utts[\"refs\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if (check_index in p) else 0 for p in pred_words]), len(pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts, dev_loss = feed_model(model,\n",
    "                            optimizer=optimizer,\n",
    "                            m_dict=map_dict[dev_key],\n",
    "                            b_dict=bucket_dict[dev_key],\n",
    "                            vocab_dict=vocab_dict,\n",
    "                            bow_dict=bow_dict,\n",
    "                            batch_size=batch_size,\n",
    "                            x_key=enc_key,\n",
    "                            y_key=dec_key,\n",
    "                            train=False,\n",
    "                            input_path=input_path,\n",
    "                            max_dec=m_cfg['max_en_pred'],\n",
    "                            t_cfg=t_cfg,\n",
    "                            use_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_THRESH = 0.1\n",
    "dev_pred_words = []\n",
    "pred_limit = m_cfg['max_en_pred']\n",
    "for row in F.sigmoid(utts[\"probs\"]).data:\n",
    "    dev_pred_inds = xp.where(row >= PRED_THRESH)[0]\n",
    "    if len(dev_pred_inds) > pred_limit:\n",
    "        dev_pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    dev_pred_words.append([i for i in dev_pred_inds.tolist() if i > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(utts[\"refs\"], dev_pred_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           0.1,\n",
    "                                           m_cfg['max_en_pred'])\n",
    "print(basic_precision_recall(dev_utts[\"refs\"], dev_pred_words))\n",
    "dev_pred_words = get_pred_words_from_probs(dev_utts[\"probs\"],\n",
    "                                           mean_pos_scores,\n",
    "                                           m_cfg['max_en_pred'])\n",
    "print(basic_precision_recall(dev_utts[\"refs\"], dev_pred_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_prec, dev_rec = basic_precision_recall(dev_utts[\"refs\"], dev_pred_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_prec, dev_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_words = []\n",
    "pred_limit = m_cfg['max_en_pred']\n",
    "for row in F.sigmoid(train_utts[\"probs\"]).data:\n",
    "    pred_inds = xp.where(row >= mean_pos_scores)[0]\n",
    "    if len(pred_inds) > pred_limit:\n",
    "        pred_inds = xp.argsort(row)[-pred_limit:][::-1]\n",
    "    #pred_words.append([bow_dict['i2w'][i] for i in pred_inds.tolist()])\n",
    "    pred_words.append([i for i in pred_inds.tolist() if i > 3])\n",
    "\n",
    "train_prec, train_rec = basic_precision_recall(train_utts[\"refs\"], pred_words)\n",
    "print(train_prec, train_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.all(utts[\"probs\"][:10] == utts[\"probs\"][10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts[\"probs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid(utts[\"probs\"][:2,100:115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts[\"probs\"].T[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts[\"refs\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([p for p in utts[\"preds\"] if len(p) > 0]), len([p for p in utts[\"refs\"] if len(p) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(utts[\"refs\"], utts[\"preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.binary_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall([[[]], [[]], [[]], [[1]]], [[1], [], [], [1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts[\"refs\"][:2], utts[\"preds\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join([vocab_dict['en_c']['i2w'][i].decode() for i in pred_sents[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, 200\n",
    "# min_len, max_len = 0, 1\n",
    "displayN = 50\n",
    "m_dict=map_dict[dev_key]\n",
    "# wavs_path = os.path.join(m_cfg['data_path'], \"wavs\")\n",
    "wavs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"wavs\")\n",
    "v_dict = vocab_dict[dec_key]\n",
    "key = m_cfg['dev_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m_dict['20051009_182032_217_fsp-B-1'][dec_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsh_filt_pred, fsh_filt_utts = zip(*sorted([(p,u) for p, u in zip(pred_sents, utts) if (len(m_dict[u]['es_w']) >= min_len) and \n",
    "                                        (len(m_dict[u]['es_w']) <= max_len)], key=lambda t:t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length filtered utterances = {0:d}\".format(len(fsh_filt_utts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "813 / 3977, (50+115+15+171) / 3977, (50+115+15+171) / 813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_words(m_dict, v_dict, \n",
    "              fsh_filt_pred, \n",
    "              fsh_filt_utts, dec_key, \n",
    "              key, \n",
    "              play_audio=True, \n",
    "              displayN=displayN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_ref = []\n",
    "en_ref = []\n",
    "for u in fsh_filt_utts:\n",
    "    es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "    if type(m_dict[u][dec_key]) == list:\n",
    "        en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "    else:\n",
    "        en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "\n",
    "en_pred = []\n",
    "join_str = ' ' if dec_key.endswith('_w') else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"/afs/inf.ed.ac.uk/group/project/lowres/work/speech2text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fsh_filt_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, chrf, h, r = calc_bleu(m_dict, \n",
    "                          v_dict, \n",
    "                          fsh_filt_pred, \n",
    "                          fsh_filt_utts, \n",
    "                          dec_key, \n",
    "                          ref_index=ref_index)\n",
    "\n",
    "print(\"BLEU score on: {0:s} = {1:.2f}\".format(key, b * 100))\n",
    "print(\"-\"*60)\n",
    "\n",
    "model_refs = {u: mr for u, mr in zip(fsh_filt_utts, r)}\n",
    "model_hyps = {u: mh for u, mh in zip(fsh_filt_utts, h)}\n",
    "\n",
    "all_weights=[(1.,0.,0.,0.),\n",
    "             (0.,1.,0.,0.),\n",
    "             (0.,0.,1.,0.),\n",
    "             (0.,0.,0.,1.),\n",
    "             (1./2,1./2,0.,0.),\n",
    "             (1./3,1./3,1./3,0.),\n",
    "             (.25,.25,.25,.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bleu(r, h, smoothing_function=smooth_fun.method2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ref, en_hyp = write_predictions_to_file(m_dict, v_dict, fsh_filt_pred, fsh_filt_utts, \n",
    "                                           dec_key, key, stemmify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_refs_dict = {u:v[0].strip().split() for u, v in zip(fsh_filt_utts, en_ref)}\n",
    "model_hyps_dict = {u:v.strip().split() for u, v in zip(fsh_filt_utts, en_hyp)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_refs_dict, open(os.path.join(m_cfg['model_dir'], \"model_s2t_refs.dict\"), \"wb\"))\n",
    "pickle.dump(model_hyps_dict, open(os.path.join(m_cfg['model_dir'], \"model_s2t_hyps.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, _ = corpus_precision_recall(r, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prec_recall = {'precision' : {}, 'recall' : {}, \"tp\": 0, \"tc\": 0, \"tr\": 0}\n",
    "\n",
    "for utt_id, ref, hyp in zip(fsh_filt_utts, r, h):\n",
    "    es_ref = [w.decode() for w in m_dict[utt_id]['es_w']]\n",
    "    \n",
    "    pval, rval = modified_precision_recall(ref, hyp, n=1)\n",
    "    model_prec_recall['tc'] += pval.numerator\n",
    "    model_prec_recall['tp'] += pval.denominator\n",
    "    model_prec_recall['tr'] += rval.denominator\n",
    "\n",
    "    model_prec_recall['precision'][utt_id] = float(pval)\n",
    "    model_prec_recall['recall'][utt_id] = float(rval)\n",
    "# end for\n",
    "    \n",
    "model_prec_recall['total_precision'] = model_prec_recall['tc'] / model_prec_recall['tp']\n",
    "model_prec_recall['total_recall'] = model_prec_recall['tc'] / model_prec_recall['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tModel metrics\")\n",
    "print(\"-\"*60)\n",
    "print(\"{0:10s} = {1:0.3f}\\n{2:10s} = {3:0.3f}\".format(\"precision\",\n",
    "                                                      model_prec_recall['total_precision'],\n",
    "                                                      \"recall\",\n",
    "                                                      model_prec_recall['total_recall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model-s2t BLEU score:\")\n",
    "\"{0:0.3f}\".format(100.0 * corpus_bleu(model_refs.values(), model_hyps.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "_, _ = corpus_precision_recall(model_refs.values(), model_hyps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "metrics = {\"p\": {1: [], 2: [], 3: [], 4: []},\n",
    "           \"r\": {1: [], 2: [], 3: [], 4: []}}\n",
    "\n",
    "for ix in range(len(list(model_refs.values())[0])):\n",
    "    temp_refs = [[i[ix]] for i in model_refs.values()]\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\t\\tUsing reference = {0:d}\".format(ix+1))\n",
    "    print(\"-\"*60)\n",
    "    ps, rs = corpus_precision_recall(temp_refs, model_hyps.values())\n",
    "    for i, (p, r) in enumerate(zip(ps, rs)):\n",
    "        metrics['p'][i+1].append(p)\n",
    "        metrics['r'][i+1].append(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(vals_dict):\n",
    "#     print(\"estimated mean\")\n",
    "    k = []\n",
    "    u = []\n",
    "    s = []\n",
    "    for m, vals in vals_dict.items():\n",
    "        k.append(m)\n",
    "        u.append(np.mean(vals))\n",
    "        s.append(np.std(vals)/np.sqrt(len(vals)))\n",
    "        print(\"{0:10d}-gram = {1:.2f} Â± {2:.2f}\".format(m, np.mean(vals), np.std(vals)/np.sqrt(len(vals))))\n",
    "        \n",
    "    print(\",\".join(map(lambda x : \"{0:.2f}\".format(x), u)))\n",
    "    print(\",\".join(map(lambda x : \"{0:.2f}\".format(x), s)))\n",
    "    return k, u, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2w_p_r = {}\n",
    "\n",
    "print(\"Precision:\")\n",
    "_, s2w_p_r['p'], s2w_p_r['p_std'] = get_mean_std(metrics['p'])\n",
    "\n",
    "print(\"Recall:\")\n",
    "_, s2w_p_r['r'], s2w_p_r['r_std'] = get_mean_std(metrics['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(m_cfg['model_dir'], \"s2w_p_r.json\"), \"w\") as out_f:\n",
    "    json.dump(s2w_p_r, out_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = {}\n",
    "for u, hyp in zip(fsh_filt_utts, h):\n",
    "    model_predictions[u] = hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "min_word_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(m_dict):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u]['en_w']) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u]['en_w']])\n",
    "        else:\n",
    "            for ref in m_dict[u]['en_w']:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_only_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at utterances which our model is doing better at, and compare to Google.\n",
    "\n",
    "Are we doing better on a few calls only? Is there any particular speaker or call messing up our results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Query task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\"\n",
    "train_text_fname= \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\"\n",
    "topics_fname = \"../criseslex/fsp06_topics_in_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [ \"peace\", \"Music\", \"Marriage\", \"Religion\", \"Cell phones\", \n",
    "           \"Dating\", \"Telemarketing and SPAM\", \"Politics\", \"Travel\", \n",
    "           \"Technical devices\", \"Healthcare\", \"Advertisements\", \"Power\", \n",
    "           \"Occupations\", \"Movies\", \"Welfare\", \"Breaking up\", \"Location\", \n",
    "           \"Justice\", \"Memories\", \"Crime\", \"Violence against women\", \"Equality\", \n",
    "            \"Housing\", \"Immigration\",     \n",
    "            # new topics\n",
    "           \"Interracial\", \"Christians\", \"muslims\", \"jews\", \"e-mail\", \n",
    "           \"phone\", \"democracy\", \"Democratic\", \"Republican\", \"technology\", \n",
    "           \"leadership\", \"community\", \"jury\", \"police\", \"inequality\", \n",
    "           \"renting\", \"Violence\", \"immigrants\", \"immigrant\", \"skilled\", \n",
    "           \"Telemarketing\", \"SPAM\", \"skill\", \"job\", \"health\", \"mobile\", \n",
    "            \"ads\", \"physical\", \"emotional\", \"bubble\", \"rent\", \"economy\", \n",
    "            \"abuse\", \"women\", \"city\", \"country\", \"suburban\", \"dollar\", \n",
    "            \"united states\", \"laws\", \"phone\", \"race\", \"biracial\", \"interracial\", \n",
    "            \"marriage\", \"lyrics\", \"sexuality\", \"medicine\", \"television\", \"european\",\n",
    "            \"home\", \"protect\", \"spouse\", \"language\", \"cellphone\", \"money\",\n",
    "            \"doctor\", \"insurance\", \"cigarettes\", \"alcohol\", \"income\", \"salary\",\n",
    "            \"class\", \"censor\", \"rating\", \"programs\", \"government\",\n",
    "            \"relationship\", \"legal\", \"event\", \"life\", \"safe\", \"victim\", \"cops\",\n",
    "            \"wage\", \"illegal\"\n",
    "            ]\n",
    "topics = list(set(t.lower() for t in topics))\n",
    "topics_stem = [stem(t) for t in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words_stem = {}\n",
    "for w in dev_words:\n",
    "    stem_w = stem(w)\n",
    "    if stem_w not in dev_words_stem:\n",
    "        dev_words_stem[stem_w] = 0\n",
    "    dev_words_stem[stem_w] += dev_words[w]\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this checks for # of occurrences of the word in the specified text\n",
    "# We later use the # of utterrances, which will be lower or equal to this number\n",
    "prune_topics = [t for t in topics if dev_words_stem.get(stem(t),0) > 10 and dev_words_stem.get(stem(t),0) < 100]\n",
    "len(prune_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"selec_query_terms\")\n",
    "sel_topics = random.sample(prune_topics, min(len(prune_topics), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sel_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_terms_references(utt_text, terms, text_key=\"en_w\"):\n",
    "    terms_set = set([stem(i) for i in terms])\n",
    "    terms_search_res = {}\n",
    "    full_words = {}\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        l = (utt_text[u][text_key] if type(utt_text[u]) == dict \n",
    "             else utt_text[u])\n",
    "        for r in l:\n",
    "            for w in set(r):\n",
    "                decoded_w = w.decode() if type(w) != str else w\n",
    "                w_to_search = stem(decoded_w)\n",
    "                if w_to_search in terms_set:\n",
    "                    if w_to_search not in terms_search_res:\n",
    "                        terms_search_res[w_to_search] = set()\n",
    "                        full_words[w_to_search] = set()\n",
    "                    terms_search_res[w_to_search].add(u)\n",
    "                    full_words[w_to_search].add(decoded_w)\n",
    "                # end if found\n",
    "            # end for current reference\n",
    "        # end for all references\n",
    "    # end for all utterances\n",
    "    return terms_search_res, full_words\n",
    "\n",
    "\n",
    "def find_all_terms_predictions(utt_text, terms):\n",
    "    terms_set = set([stem(i) for i in terms])\n",
    "    terms_search_res = {}\n",
    "    full_words = {}\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        r = utt_text[u]\n",
    "        for w in set(r):\n",
    "            w_to_search = stem(w)\n",
    "            if w_to_search in terms_set:\n",
    "                if w_to_search not in terms_search_res:\n",
    "                    terms_search_res[w_to_search] = set()\n",
    "                    full_words[w_to_search] = set()\n",
    "                terms_search_res[w_to_search].add(u)\n",
    "                full_words[w_to_search].add(u)\n",
    "                # end if found\n",
    "            # end for current reference\n",
    "    # end for all utterances\n",
    "    return terms_search_res, full_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_prec_recall(preds, refs):\n",
    "    prec_recall = {}\n",
    "    prec_recall = {'t':0, 'tp':0, 'tc':0, 'terms':{}}\n",
    "    for term in refs.keys():\n",
    "        prec_recall['terms'][term] = {}\n",
    "        prec_recall['terms'][term]['t'] = len(refs[term])\n",
    "        preds_occ = preds.get(term, set())\n",
    "        prec_recall['terms'][term]['tp'] = len(preds_occ)\n",
    "        prec_recall['terms'][term]['tc'] = len(refs[term] & preds_occ)\n",
    "        prec_recall['t'] += prec_recall['terms'][term]['t']\n",
    "        prec_recall['tp'] += prec_recall['terms'][term]['tp']\n",
    "        prec_recall['tc'] += prec_recall['terms'][term]['tc']\n",
    "    # end for each term\n",
    "    return prec_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_topics, ref_topic_labels = find_all_terms_references(map_dict['fisher_dev'], sel_topics)\n",
    "pred_topics, pred_topics_topics_labels = find_all_terms_predictions(model_predictions, sel_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_topics), len(pred_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_topics_in_utts = {k:v for k, v in ref_topics.items() if len(v) >=3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prune_topics_in_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(6,10),nrows=1, ncols=1)\n",
    "\n",
    "\n",
    "sorted_counts = [t[0] for t in sorted(ref_topics.items(), reverse=True, key=lambda t:len(t[1]))]\n",
    "topic_count = [len(ref_topics[t]) for t in sorted_counts]\n",
    "\n",
    "topic_labels = [\"-\".join(list(ref_topic_labels[t])[:3]) for t in sorted_counts]\n",
    "\n",
    "ax = sns.barplot(x=topic_count, \n",
    "                 y=topic_labels, \n",
    "                 color=tableau20[6], alpha=.7)\n",
    "                 #**{\"label\":\"topic counts\", \"alpha\":0.5}, ax=ax)\n",
    "\n",
    "# ax.set_xlabel(\"# of utts in which topic occurs\", size=20)\n",
    "\n",
    "max_count_val = max([len(v) for v in ref_topics.values()])\n",
    "min_count_val = max([len(v) for v in ref_topics.values()])\n",
    "plt.xticks(list(range(0,max_count_val, 5))+[min_count_val, max_count_val])\n",
    "\n",
    "# ax.legend(loc='upper right', bbox_to_anchor=(1.1, 0.9),\n",
    "#                       ncol=1, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "sns.despine(left=True, bottom=True, top=True, right=False)\n",
    "\n",
    "for t in ax.get_yticklabels():\n",
    "    t.set_fontsize(15) \n",
    "    \n",
    "for i, t in enumerate(ax.get_xticklabels()):\n",
    "    if i > 0 and i < (len(ax.get_xticklabels())-1):\n",
    "        t.set_visible(False)\n",
    "    else:\n",
    "        t.set_fontsize(16)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig.savefig(\"../criseslex/sel_topics_new.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics_p_r = terms_prec_recall(preds = pred_topics, refs = ref_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics_p_r['precision'] = model_topics_p_r['tc'] / model_topics_p_r['tp']\n",
    "model_topics_p_r['recall'] = model_topics_p_r['tc'] / model_topics_p_r['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"-------------- Query by Text\")\n",
    "print(\"-\"*40)\n",
    "print(\"{0:20s} | {1:5.2f}%\".format(\"precision\", model_topics_p_r['precision']*100.0))\n",
    "print(\"{0:20s} | {1:5.2f}%\".format(\"recall\", model_topics_p_r['recall']*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(m_cfg['model_dir'], \"topics_p_r.json\"), \"w\") as out_f:\n",
    "    json.dump(model_topics_p_r, out_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_terms_es_references(utt_text, term):\n",
    "    terms_search_res = set()\n",
    "    total_word_count = 0\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        l = (utt_text[u][\"es_w\"] if type(utt_text[u]) == dict \n",
    "             else utt_text[u])\n",
    "        #if len(l) == 1 and l[0].decode() == term:\n",
    "        if set(l) == set(term.split()):\n",
    "            terms_search_res.add(u)\n",
    "            total_word_count += 1\n",
    "            # end if found\n",
    "        # end for current reference\n",
    "        # end for all references\n",
    "    # end for all utterances\n",
    "    return terms_search_res, total_word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_key = \"fisher_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hmm, tot_count = find_all_terms_es_references(map_dict[set_key], b'claro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_words = {}\n",
    "all_match_count = 0\n",
    "no_match_count = 0\n",
    "\n",
    "utt_refs_set = {}\n",
    "for utt in hmm:\n",
    "    #print(\"-\"*60)\n",
    "    #print(utt)\n",
    "    curr_ref = {k : \"\" for k in range(4)}\n",
    "    if \"train\" in set_key:\n",
    "        r = map_dict[set_key][utt][\"en_w\"]\n",
    "        en_ref_words = \" \".join(w.decode() for w in r)\n",
    "        curr_ref[0] = en_ref_words\n",
    "        if en_ref_words not in all_ref_words:\n",
    "            all_ref_words[en_ref_words] = 0\n",
    "        all_ref_words[en_ref_words] += 1\n",
    "    else:\n",
    "        for i, r in enumerate(map_dict[set_key][utt][\"en_w\"]):\n",
    "            en_ref_words = \" \".join(w.decode() for w in r)\n",
    "            curr_ref[i] = en_ref_words\n",
    "            #print(\" \".join(w.decode() for w in r), end=\"----\")\n",
    "            if en_ref_words not in all_ref_words:\n",
    "                all_ref_words[en_ref_words] = 0\n",
    "            all_ref_words[en_ref_words] += 1\n",
    "    #print(curr_ref.values())\n",
    "\n",
    "    \n",
    "    utt_refs_set[utt] = set(curr_ref.values())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utt_refs_set['20051023_232057_325_fsp-B-77']) if \"dev\" in set_key else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if \"dev\" in set_key:\n",
    "    start_set = utt_refs_set['20051023_232057_325_fsp-B-77']\n",
    "    for utt in utt_refs_set:\n",
    "        start_set = start_set & utt_refs_set[utt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_match_count = sum([len(utt_refs_set[utt]) == 1 for utt in utt_refs_set])\n",
    "all_dont_match_count = sum([len(utt_refs_set[utt]) == 4  for utt in utt_refs_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_match_count, all_dont_match_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_words, len(all_ref_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_refs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict = map_dict[set_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_utts = [utt for utt in utt_refs_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_batch(m_dict, enc_key, dec_key, sel_utts[:101], vocab_dict, 200, 10, input_path=input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = xp.asnumpy(batch_data['X'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "distance, path = fastdtw(A[1], A[2], dist=euclidean)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_out = np.zeros((len(A), len(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(A))):\n",
    "    for j in range(i,len(A)):\n",
    "        dtw_out[i,j], _ = fastdtw(A[i], A[j], dist=euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = dtw_out + dtw_out.T - - np.diag(np.diag(dtw_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dist_matrix), np.std(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dist_matrix, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(dist_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(dist_matrix, mask=mask, square=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "ward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X)\n",
    "elapsed_time = time.time() - st\n",
    "label = ward.labels_\n",
    "print(\"Elapsed time: %.2fs\" % elapsed_time)\n",
    "print(\"Number of points: %i\" % label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A.reshape(len(A),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "B_r = pca.fit(B).transform(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n",
    "               color=plt.cm.jet(np.float(l) / np.max(label + 1)),\n",
    "               s=20, edgecolor='k')\n",
    "plt.title('Without connectivity constraints (time %.2fs)' % elapsed_time)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Define the structure A of the data. Here a 10 nearest neighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "connectivity = kneighbors_graph(X, n_neighbors=25, include_self=False)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute clustering\n",
    "print(\"Compute structured hierarchical clustering...\")\n",
    "st = time.time()\n",
    "ward = AgglomerativeClustering(n_clusters=10, connectivity=connectivity,\n",
    "                               linkage='ward').fit(X)\n",
    "elapsed_time = time.time() - st\n",
    "label = ward.labels_\n",
    "print(\"Elapsed time: %.2fs\" % elapsed_time)\n",
    "print(\"Number of points: %i\" % label.size)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n",
    "               color=plt.cm.jet(float(l) / np.max(label + 1)),\n",
    "               s=20, edgecolor='k')\n",
    "plt.title('With connectivity constraints (time %.2fs)' % elapsed_time)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward = AgglomerativeClustering(n_clusters=5, connectivity=connectivity,\n",
    "                               linkage='ward').fit(X)\n",
    "\n",
    "labels = ward.labels_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for color, l in zip(tableau20, np.unique(labels)):\n",
    "    ax.scatter(B_r[labels == l, 0], B_r[labels == l, 1], s=100,\n",
    "               color=plt.cm.jet(float(l) / np.max(labels + 1)), edgecolor='k')\n",
    "plt.yticks(visible=False)\n",
    "plt.xticks(visible=False)\n",
    "fig.savefig(\"agglo_dtw_claro.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = KMeans(n_clusters=5)\n",
    "est.fit(B)\n",
    "labels = est.labels_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for color, l in zip(tableau20, np.unique(labels)):\n",
    "    ax.scatter(B_r[labels == l, 0], B_r[labels == l, 1], s=100,\n",
    "               color=plt.cm.jet(float(l) / np.max(labels + 1)), edgecolor='k')\n",
    "# done\n",
    "plt.yticks(visible=False)\n",
    "plt.xticks(visible=False)\n",
    "fig.savefig(\"k_means_claro.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(list(ward.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ward.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X[label == l, 0], X[label == l, 1], X[label == l, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results on an image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(X, cmap=plt.cm.gray)\n",
    "for l in range(n_clusters):\n",
    "    plt.contour(label == l, contours=1,\n",
    "                colors=[plt.cm.spectral(l / float(n_clusters)), ])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(A)\n",
    "print('pairwise dense output:\\n {}\\n'.format(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"20051021_222225_307_fsp-B-109\" in map_dict['fisher_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(m_dict, x_key, y_key, utt_list, vocab_dict,\n",
    "              max_enc, max_dec, input_path=''):\n",
    "    batch_data = {'X':[], 'y':[]}\n",
    "    # -------------------------------------------------------------------------\n",
    "    # loop through each utterance in utt list\n",
    "    # -------------------------------------------------------------------------\n",
    "    for u in utt_list:\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add X data\n",
    "        # ---------------------------------------------------------------------\n",
    "        if x_key == 'sp':\n",
    "            # -----------------------------------------------------------------\n",
    "            # for speech data\n",
    "            # -----------------------------------------------------------------\n",
    "            # get path to speech file\n",
    "            utt_sp_path = os.path.join(input_path, \"{0:s}.npy\".format(u))\n",
    "            if not os.path.exists(utt_sp_path):\n",
    "                # for training data, there are sub-folders\n",
    "                utt_sp_path = os.path.join(input_path,\n",
    "                                           u.split('_',1)[0],\n",
    "                                           \"{0:s}.npy\".format(u))\n",
    "            if os.path.exists(utt_sp_path):\n",
    "                x_data = xp.load(utt_sp_path)\n",
    "                # truncate max length\n",
    "                batch_data['X'].append(x_data[:max_enc])\n",
    "            else:\n",
    "                # -------------------------------------------------------------\n",
    "                # exception if file not found\n",
    "                # -------------------------------------------------------------\n",
    "                print(\"ERROR!! file not found: {0:s}\".format(utt_sp_path))\n",
    "                # -------------------------------------------------------------\n",
    "        else:\n",
    "            # -----------------------------------------------------------------\n",
    "            # for text data\n",
    "            # -----------------------------------------------------------------\n",
    "            x_ids = [vocab_dict[x_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][x_key]]\n",
    "            x_ids = xp.asarray(x_ids, dtype=xp.int32)\n",
    "            batch_data['X'].append(x_ids[:max_enc])\n",
    "            # -----------------------------------------------------------------\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add labels\n",
    "        # ---------------------------------------------------------------------\n",
    "        if type(m_dict[u][y_key]) == list:\n",
    "            en_ids = [vocab_dict[y_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key]]\n",
    "        else:\n",
    "            # dev and test data have multiple translations\n",
    "            # choose the first one for computing perplexity\n",
    "            en_ids = [vocab_dict[y_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key][0]]\n",
    "        y_ids = [GO_ID] + en_ids[:max_dec-2] + [EOS_ID]\n",
    "        batch_data['y'].append(xp.asarray(y_ids, dtype=xp.int32))\n",
    "        # ---------------------------------------------------------------------\n",
    "    # -------------------------------------------------------------------------\n",
    "    # end for all utterances in batch\n",
    "    # -------------------------------------------------------------------------\n",
    "    if len(batch_data['X']) > 0 and len(batch_data['y']) > 0:\n",
    "        batch_data['X'] = F.pad_sequence(batch_data['X'], padding=PAD_ID)\n",
    "        batch_data['y'] = F.pad_sequence(batch_data['y'], padding=PAD_ID)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puerto rico utterance\n",
    "# \"i 'm from puerto rico but i live here in denver colorado\"\n",
    "\" \".join(w.decode() for w in map_dict['fisher_dev']['20051023_232057_325_fsp-A-3']['en_w'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
