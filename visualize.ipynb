{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b314517c59dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmooth_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSmoothingFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_files = [f for f in os.listdir(os.path.dirname(model_fil))\n",
    "                   if os.path.basename(model_fil).replace('.model','') in f]\n",
    "# print(model_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model_fil = max(model_files, key=lambda s: int(s.split('_')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_model_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = check_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dev_fil_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_train = np.loadtxt(log_train_fil_name, delimiter=',', skiprows=False).transpose()\n",
    "log_test = np.genfromtxt(log_dev_fil_name, delimiter=',', usecols = (0,1,2)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(8,5)\n",
    "ax1.plot(log_train[0], log_train[1], color='#ff7700')\n",
    "ax1.plot(log_test[0], log_test[1], 'r--')\n",
    "ax1.set_xlabel('epoch', size=24)\n",
    "ax1.set_ylabel('loss', color='r', size=24)\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "    tl.set_fontsize(18)\n",
    "plt.legend(['train loss', 'dev loss'], bbox_to_anchor=(1.45, 0.96), framealpha=0, fontsize=20)    \n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(log_test[0], log_test[2]*100, 'b-')\n",
    "ax2.set_xlabel('iteration')\n",
    "ax2.set_ylabel('dev bleu', color='b', size=24)\n",
    "# ax1.set_xlim(0, 60)\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "    tl.set_fontsize(18) \n",
    "plt.legend(['dev bleu'], bbox_to_anchor=(1.44, 1.04), framealpha=0, fontsize=20)\n",
    "# plt.legend(['dev bleu'], bbox_to_anchor=(1.06, 0.9), framealpha=0, fontsize=20)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_utt(utt, m_dict):\n",
    "    sr, y = scipy.io.wavfile.read(os.path.join(wavs_path, utt.rsplit(\"-\",1)[0]+'.wav'))\n",
    "    start_t = min(seg['start'] for seg in m_dict[utt]['seg'])\n",
    "    end_t = max(seg['end'] for seg in m_dict[utt]['seg'])\n",
    "    print(start_t, end_t)\n",
    "    start_t_samples, end_t_samples = int(start_t*sr), int(end_t*sr)\n",
    "    display(Audio(y[start_t_samples:end_t_samples], rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_words(m_dict, v_dict, preds, utts, dec_key, key, play_audio=False, displayN=50):\n",
    "    es_ref = []\n",
    "    en_ref = []\n",
    "    for u in utts:\n",
    "        es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "        if type(m_dict[u][dec_key]) == list:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "\n",
    "    en_pred = []\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    for p in preds:\n",
    "        t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "        t_str = t_str[:t_str.find('_EOS')]\n",
    "        en_pred.append(t_str)\n",
    "\n",
    "    for u, es, en, p in zip(utts, es_ref, en_ref, en_pred):\n",
    "        # for reference, 1st word is GO_ID, no need to display\n",
    "        print(\"Utterance: {0:s}\".format(u))\n",
    "        display_pp = PrettyTable([\"cat\",\"sent\"], hrules=True)\n",
    "        display_pp.align = \"l\"\n",
    "        display_pp.header = False\n",
    "        display_pp.add_row([\"es ref\", textwrap.fill(es,50)])\n",
    "        display_pp.add_row([\"en ref\", textwrap.fill(en,50)])\n",
    "        display_pp.add_row([\"en pred\", textwrap.fill(p,50)])\n",
    "\n",
    "        print(display_pp)\n",
    "        if play_audio:\n",
    "            play_utt(u, m_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_words_latex(m_dict, v_dict, preds, utts, dec_key):\n",
    "    print(\"min length={0:d}, max length={1:d}\".format(min_len, max_len))\n",
    "    es_ref = []\n",
    "    en_ref = []\n",
    "    for u in utts:\n",
    "        es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "        if type(m_dict[u][dec_key]) == list:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "\n",
    "    en_pred = []\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    for p in preds:\n",
    "        t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "        t_str = t_str[:t_str.find('_EOS')]\n",
    "        en_pred.append(t_str)\n",
    "\n",
    "    total_matching_len = 0\n",
    "\n",
    "    for u, es, en, p in zip(utts, es_ref, en_ref, en_pred):\n",
    "        # for reference, 1st word is GO_ID, no need to display\n",
    "        print(\"Utterance: {0:s}\".format(u))\n",
    "        print(\"{0:d} & {1:s} & {2:s} & {3:s} \\\\\\\\\".format(total_matching_len, es, en, p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_predictions_to_file(m_dict, v_dict, preds, utts, dec_key, key, stemmify=False):\n",
    "    en_hyp = []\n",
    "    en_ref = []\n",
    "    ref_key = 'en_w' if 'en_' in dec_key else 'es_w'\n",
    "    src_key = 'es_w'\n",
    "    \n",
    "    for u in tqdm(utts, ncols=80):\n",
    "        if type(m_dict[u][ref_key]) == list:\n",
    "            if stemmify:\n",
    "                en_ref.append(\" \".join([stem(w.decode()) for w in m_dict[u]['en_w']]))\n",
    "            else:\n",
    "                en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_r_list = []\n",
    "            for r in m_dict[u][ref_key]:\n",
    "                if stemmify:\n",
    "                    en_r_list.append(\" \".join([stem(w.decode()) for w in r]))\n",
    "                else:\n",
    "                    en_r_list.append(\" \".join([w.decode() for w in r]))\n",
    "            en_ref.append(en_r_list)\n",
    "\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    total_matching_len = 0\n",
    "\n",
    "    for u, p in zip(utts, preds):\n",
    "        if stemmify:\n",
    "            t_str = join_str.join([stem(v_dict['i2w'][i].decode()) if i != EOS_ID else EOS.decode() for i in p])\n",
    "        else:\n",
    "            t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "        t_str = t_str[:t_str.find('_EOS')]\n",
    "        en_hyp.append(t_str)\n",
    "\n",
    "    \n",
    "    print(\"writing to file: {0:s}\".format(\"{0:s}_mt-output\".format(key)))\n",
    "    with open(\"{0:s}_mt-output\".format(key), \"w\") as pred_f:\n",
    "        for p in en_hyp:\n",
    "            pred_f.write(\"{0:s}\\n\".format(p))\n",
    "        # end for\n",
    "    # end while\n",
    "    \n",
    "    if type(m_dict[utts[0]][ref_key]) == list:\n",
    "        with open(\"{0:s}.ref0\".format(key), \"w\") as ref_f:\n",
    "            for r in en_ref:\n",
    "                ref_f.write(\"{0:s}\\n\".format(r))\n",
    "    else:\n",
    "        num_ref = len(m_dict[u][ref_key])\n",
    "        for i in range(num_ref):\n",
    "            with open(\"{0:s}_en.ref{1:d}\".format(key,i), \"w\") as ref_f:\n",
    "                for r in en_ref:\n",
    "                    ref_f.write(\"{0:s}\\n\".format(r[i]))\n",
    "                # end for each utt\n",
    "            # end with\n",
    "        # end for reference\n",
    "    # end else\n",
    "    print(\"done\")\n",
    "    return en_ref, en_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key='fisher_dev'\n",
    "train=False\n",
    "m_dict = map_dict[key]\n",
    "v_dict = vocab_dict[dec_key]\n",
    "n=len(map_dict[key])\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_speech_path = os.path.join(out_path, key)\n",
    "wavs_path = os.path.join(out_path, \"wavs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*80)\n",
    "print(\"EPOCH = {0:d}\".format(last_epoch+1))\n",
    "fsh_pred_sents, fsh_utts, loss = feed_model(map_dict[key],\n",
    "                  b_dict=bucket_dict[key],\n",
    "                  vocab_dict=vocab_dict,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  x_key=enc_key,\n",
    "                  y_key=dec_key,\n",
    "                  train=train,\n",
    "                  cat_speech_path=cat_speech_path, use_y=False, mini=False)\n",
    "\n",
    "print(\"{0:s} {1:s} mean loss={2:.4f}\".format(\"*\" * 10,\n",
    "                                    \"train\" if train else \"dev\",\n",
    "                                    loss))\n",
    "print(\"-\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, MAX_EN_LEN\n",
    "# min_len, max_len = 0, 5\n",
    "displayN = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsh_filt_pred, fsh_filt_utts = zip(*sorted([(p,u) for p, u in zip(fsh_pred_sents, fsh_utts) if (len(m_dict[u]['es_w']) >= min_len) and \n",
    "                                        (len(m_dict[u]['es_w']) <= max_len)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length filtered utterances = {0:d}\".format(len(fsh_filt_utts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_words(m_dict, v_dict, fsh_filt_pred[:20], fsh_filt_utts[:20], \n",
    "              dec_key, key, play_audio=False, displayN=displayN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"/afs/inf.ed.ac.uk/group/project/lowres/work/chainer2/speech2text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, chrf, h, r = calc_bleu(m_dict, \n",
    "                          v_dict, \n",
    "                          fsh_filt_pred, \n",
    "                          fsh_filt_utts, \n",
    "                          dec_key, \n",
    "                          ref_index=ref_index)\n",
    "\n",
    "print(\"BLEU score on: {0:s} = {1:.2f}\".format(key, b * 100))\n",
    "print(\"-\"*60)\n",
    "\n",
    "model_refs = {u: mr for u, mr in zip(fsh_filt_utts, r)}\n",
    "model_hyps = {u: mh for u, mh in zip(fsh_filt_utts, h)}\n",
    "\n",
    "all_weights=[(1.,0.,0.,0.),\n",
    "             (0.,1.,0.,0.),\n",
    "             (0.,0.,1.,0.),\n",
    "             (0.,0.,0.,1.),\n",
    "             (1./2,1./2,0.,0.),\n",
    "             (1./3,1./3,1./3,0.),\n",
    "             (.25,.25,.25,.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ref, en_hyp = write_predictions_to_file(m_dict, v_dict, fsh_filt_pred, fsh_filt_utts, \n",
    "                                           dec_key, key, stemmify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, _ = corpus_precision_recall(r, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callhome dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_callhome_metrics():\n",
    "    key='callhome_devtest'\n",
    "    train=False\n",
    "    m_dict = map_dict[key]\n",
    "    v_dict = vocab_dict[dec_key]\n",
    "    n=len(map_dict[key])\n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "    cat_speech_path = os.path.join(out_path, key)\n",
    "\n",
    "    print(\"-\"*80)\n",
    "    print(\"EPOCH = {0:d}\".format(last_epoch+1))\n",
    "    call_pred_sents, call_utts, loss = feed_model(map_dict[key],\n",
    "                      b_dict=bucket_dict[key],\n",
    "                      vocab_dict=vocab_dict,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      x_key=enc_key,\n",
    "                      y_key=dec_key,\n",
    "                      train=train,\n",
    "                      cat_speech_path=cat_speech_path, use_y=True)\n",
    "\n",
    "    print(\"{0:s} {1:s} mean loss={2:.4f}\".format(\"*\" * 10,\n",
    "                                        \"train\" if train else \"dev\",\n",
    "                                        loss))\n",
    "    print(\"-\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    display_words(m_dict, v_dict, call_pred_sents[:displayN], call_utts[:displayN],\n",
    "                  dec_key, key, min_len=min_len, max_len=max_len, play_audio=False)\n",
    "\n",
    "    b, _, h, r = calc_bleu(m_dict, \n",
    "                                  v_dict, \n",
    "                                  call_pred_sents, \n",
    "                                  call_utts, \n",
    "                                  dec_key, \n",
    "                                  min_len=min_len, \n",
    "                                  max_len=max_len, \n",
    "                                  ref_index=ref_index)\n",
    "\n",
    "    print(\"BLEU score on: {0:s} = {1:.2f}\".format(key, b * 100))\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    all_weights=[(1.,0.,0.,0.),\n",
    "                 (0.,1.,0.,0.),\n",
    "                 (0.,0.,1.,0.),\n",
    "                 (0.,0.,0.,1.),\n",
    "                 (1./2,1./2,0.,0.),\n",
    "                 (1./3,1./3,1./3,0.),\n",
    "                 (.25,.25,.25,.25)]\n",
    "\n",
    "    print(\"{0:>20s} | {1:20s}\".format(\"bleu score (0-100)\", \"uni-bi-tri-quad\"))\n",
    "    for weights in all_weights:\n",
    "        b = corpus_bleu(r, h, weights=weights, smoothing_function=smooth_fun.method2)\n",
    "        print(\"{0:20.2f} | {1:20s}\".format(b * 100, \"-\".join(map(\"{0:.2f}\".format, weights))))\n",
    "\n",
    "    _, _ = corpus_precision_recall(r, h)\n",
    "\n",
    "    call_en_ref, call_en_hyp = write_predictions_to_file(m_dict, v_dict, call_pred_sents, call_utts, \n",
    "                                                         dec_key, key, min_len=min_len, max_len=max_len, stemmify=False)\n",
    "\n",
    "    print(len(call_en_ref),len(call_en_hyp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_s2t_path_hyps = \"./google-s2t/e2e_ast_decode/hyps/fisher_dev/\"\n",
    "google_s2t_path_refs = \"./google-s2t/e2e_ast_decode/refs/fisher_dev/\"\n",
    "google_hyp_fname = 'fisher_spa_eng_ast_003_base_r0.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Google data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaldi_segment_map_path = os.path.join(out_path,'kaldi_segment_map.dict')\n",
    "rev_map_dict_path = os.path.join(out_path,'rev_map.dict')\n",
    "kaldi_segment_map = pickle.load(open(kaldi_segment_map_path, \"rb\"))\n",
    "rev_map_dict = pickle.load(open(rev_map_dict_path, \"rb\"))\n",
    "\n",
    "\n",
    "google_s2t_path_mapping = \"./google-s2t/e2e_ast_decode/mapping/fisher_dev/\"\n",
    "google_s2t_ref0 = os.path.join(google_s2t_path_mapping, \"sorted-normalized-fisher_dev.en.0.mapping\")\n",
    "\n",
    "google_s2t_map_path = os.path.join(out_path, \"google_s2t_map.dict\")\n",
    "google_s2t_rev_map_path = os.path.join(out_path, \"google_s2t_rev_map.dict\")\n",
    "google_s2t_refs_path = os.path.join(out_path, \"google_s2t_refs.dict\")\n",
    "google_s2t_hyps_path = os.path.join(out_path, \"google_s2t_hyps.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_google_data():\n",
    "\n",
    "    google_s2t_map = {}\n",
    "    google_s2t_rev_map = {}\n",
    "\n",
    "    r_m_dict = rev_map_dict['fisher_dev']\n",
    "    with open(google_s2t_ref0, \"r\") as in_f:\n",
    "        for i, line in enumerate(in_f, start=1):\n",
    "            wav_f = line.split(\".wav\")[0].replace(\"fisher_dev/\", \"\")\n",
    "            key_1 = wav_f[:wav_f.find(\"_fsp\")+len(\"_fsp\")]\n",
    "            key_2 = wav_f.rsplit(\"_fsp\",1)[1].split(\"_\")[0]\n",
    "            google_s2t_map[i] = r_m_dict[key_1+key_2]\n",
    "            google_s2t_rev_map[r_m_dict[key_1+key_2]] = i\n",
    "        # end for\n",
    "    # end with\n",
    "    pickle.dump(google_s2t_map, open(google_s2t_map_path, \"wb\"))\n",
    "    pickle.dump(google_s2t_rev_map, open(google_s2t_rev_map_path, \"wb\"))\n",
    "    \n",
    "    google_s2t_refs = {}\n",
    "    for fname in os.listdir(google_s2t_path_refs):\n",
    "        with open(os.path.join(google_s2t_path_refs,fname), \"r\") as in_f:\n",
    "            f_key = fname.rsplit(\"-\",1)[-1].replace(\".en.\",\"_ref_\")\n",
    "            google_s2t_refs[f_key] = {}\n",
    "            for i, line in enumerate(in_f, start=1):\n",
    "                utt_key = google_s2t_map[i]\n",
    "                google_s2t_refs[f_key][utt_key] = line.rstrip('\\n').split()\n",
    "            # end for\n",
    "        # end with\n",
    "    # end for\n",
    "    pickle.dump(google_s2t_refs, open(google_s2t_refs_path, \"wb\"))\n",
    "    \n",
    "    google_s2t_hyps = {}\n",
    "    for fname in os.listdir(google_s2t_path_hyps):\n",
    "        with open(os.path.join(google_s2t_path_hyps,fname), \"r\") as in_f:\n",
    "            f_key = \"fisher_dev_{0:s}\".format(fname.rsplit(\"_\")[-1].replace(\".txt\", \"\"))\n",
    "            google_s2t_hyps[f_key] = {}\n",
    "            for i, line in enumerate(in_f, start=1):\n",
    "                utt_key = google_s2t_map[i]\n",
    "                google_s2t_hyps[f_key][utt_key] = line.rstrip('\\n').split()\n",
    "            # end for\n",
    "        # end with\n",
    "    # end for\n",
    "    pickle.dump(google_s2t_hyps, open(google_s2t_hyps_path, \"wb\"))\n",
    "    return google_s2t_refs, google_s2t_hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\".\" in fsh_utts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_google_s2t_refs_to_file(google_dict, utts):\n",
    "    data_lines = {}\n",
    "    for r in google_dict.keys():\n",
    "        curr_file_lines = []\n",
    "        out_fname = \"google_\"+r+\".en\"\n",
    "        with open(out_fname, \"w\") as out_f:\n",
    "            print(\"writing to file: {0:s}\".format(out_fname))\n",
    "            for utt_id in utts:\n",
    "                words = \" \".join(google_dict[r][utt_id])\n",
    "                out_line = \"{0:s}\\n\".format(words)\n",
    "                out_f.write(out_line)\n",
    "                if utt_id not in data_lines:\n",
    "                    if len(google_dict) > 1:\n",
    "                        data_lines[utt_id] = [google_dict[r][utt_id]]\n",
    "                    else:\n",
    "                        data_lines[utt_id] = google_dict[r][utt_id][:]\n",
    "                else:\n",
    "                    data_lines[utt_id].append(google_dict[r][utt_id])\n",
    "            # end for over utterances\n",
    "        # end for file        \n",
    "    # end with\n",
    "    return data_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_s2t_refs, google_s2t_hyps = prep_google_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_key = 'fisher_dev_r0'\n",
    "google_refs = write_google_s2t_refs_to_file(google_s2t_refs, fsh_filt_utts)\n",
    "google_hyps = write_google_s2t_refs_to_file({hyp_key : google_s2t_hyps[hyp_key]}, fsh_filt_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\" \".join(map(lambda t: t.decode(), m_dict[rev_map_dict['fisher_dev']['20051009_182032_217_fsp.108']]['en_w'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join([w for w in google_s2t_refs['fisher_dev_ref_0'][fsh_utts[0]]]))\n",
    "print(\" \".join([w for w in google_s2t_hyps['fisher_dev_r0'][fsh_utts[0]]]))\n",
    "print(\" \".join([w for w in h[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fsh_utts), len(google_s2t_refs['fisher_dev_ref_0']))\n",
    "\n",
    "for key in google_s2t_refs['fisher_dev_ref_0']:\n",
    "    if key not in fsh_utts:\n",
    "        print(key)\n",
    "        print(\" \".join([w.decode() for w in m_dict[key]['en_w'][0]]))\n",
    "        print(google_s2t_hyps['fisher_dev_r0']['20051017_220530_275_fsp-B-26'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_s2t_refs.keys(), google_s2t_hyps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_prec_recall = {'precision' : {}, 'recall' : {}, \"tp\": 0, \"tc\": 0, \"tr\": 0}\n",
    "model_prec_recall = {'precision' : {}, 'recall' : {}, \"tp\": 0, \"tc\": 0, \"tr\": 0}\n",
    "\n",
    "\n",
    "for utt_id, ref, hyp in zip(fsh_filt_utts, r, h):\n",
    "    es_ref = [w.decode() for w in m_dict[utt_id]['es_w']]\n",
    "    \n",
    "    pval, rval = modified_precision_recall([google_s2t_refs[i][utt_id] for i in google_s2t_refs], \n",
    "                                           google_s2t_hyps['fisher_dev_r0'][utt_id], n=1)\n",
    "\n",
    "    google_prec_recall['precision'][utt_id] = float(pval)\n",
    "    google_prec_recall['recall'][utt_id] = float(rval)\n",
    "\n",
    "    google_prec_recall['tc'] += pval.numerator\n",
    "    google_prec_recall['tp'] += pval.denominator\n",
    "    google_prec_recall['tr'] += rval.denominator\n",
    "\n",
    "    if rval > 1:\n",
    "        print(\"ouch\", utt_id)\n",
    "        break\n",
    "\n",
    "    pval, rval = modified_precision_recall(ref, hyp, n=1)\n",
    "    model_prec_recall['tc'] += pval.numerator\n",
    "    model_prec_recall['tp'] += pval.denominator\n",
    "    model_prec_recall['tr'] += rval.denominator\n",
    "\n",
    "    model_prec_recall['precision'][utt_id] = float(pval)\n",
    "    model_prec_recall['recall'][utt_id] = float(rval)\n",
    "# end for\n",
    "    \n",
    "google_prec_recall['total_precision'] = google_prec_recall['tc'] / google_prec_recall['tp']\n",
    "google_prec_recall['total_recall'] = google_prec_recall['tc'] / google_prec_recall['tr']\n",
    "model_prec_recall['total_precision'] = model_prec_recall['tc'] / model_prec_recall['tp']\n",
    "model_prec_recall['total_recall'] = model_prec_recall['tc'] / model_prec_recall['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\tGoogle metrics\")\n",
    "print(\"-\"*60)\n",
    "print(\"{0:10s} = {1:0.3f}\\n{2:10s} = {3:0.3f}\".format(\"precision\",\n",
    "                                                      google_prec_recall['total_precision'],\n",
    "                                                      \"recall\",\n",
    "                                                      google_prec_recall['total_recall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tModel metrics\")\n",
    "print(\"-\"*60)\n",
    "print(\"{0:10s} = {1:0.3f}\\n{2:10s} = {3:0.3f}\".format(\"precision\",\n",
    "                                                      model_prec_recall['total_precision'],\n",
    "                                                      \"recall\",\n",
    "                                                      model_prec_recall['total_recall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"google-s2t BLEU score:\")\n",
    "\"{0:0.3f}\".format(corpus_bleu(google_refs.values(), google_hyps.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model-s2t BLEU score:\")\n",
    "\"{0:0.3f}\".format(corpus_bleu(model_refs.values(), model_hyps.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(len(list(google_refs.values())[0])):\n",
    "        print(\"chrf score using ref:{0:d} = {1:5.3f}\".format(i+1, corpus_chrf([ref[i] \n",
    "                                                                               for ref in google_refs.values()], \n",
    "                                                                              google_hyps.values())))\n",
    "except:\n",
    "    print(\"bleh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in range(len(list(model_refs.values())[0])):\n",
    "        print(\"chrf score using ref:{0:d} = {1:5.3f}\".format(i+1, corpus_chrf([x[i] for x in model_refs.values()], \n",
    "                                                                              model_hyps.values())))\n",
    "except:\n",
    "    print(\"bleh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tGOOGLE\")\n",
    "print(\"-\"*60)\n",
    "_, _ = corpus_precision_recall(google_refs.values(), google_hyps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "_, _ = corpus_precision_recall(model_refs.values(), model_hyps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tGOOGLE\")\n",
    "print(\"-\"*60)\n",
    "for ix in range(len(list(google_refs.values())[0])):\n",
    "    temp_refs = [[i[ix]] for i in google_refs.values()]\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\t\\tUsing reference = {0:d}\".format(ix+1))\n",
    "    print(\"-\"*60)\n",
    "    _, _ = corpus_precision_recall(temp_refs, google_hyps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "for ix in range(len(list(model_refs.values())[0])):\n",
    "    temp_refs = [[i[ix]] for i in model_refs.values()]\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\t\\tUsing reference = {0:d}\".format(ix+1))\n",
    "    print(\"-\"*60)\n",
    "    _, _ = corpus_precision_recall(temp_refs, model_hyps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "better_utts = {'precision' : [], 'recall' : []}\n",
    "for utt_id in fsh_filt_utts:\n",
    "    if model_prec_recall['precision'][utt_id] > google_prec_recall['precision'][utt_id]:\n",
    "        better_utts['precision'].append(utt_id)\n",
    "    if model_prec_recall['recall'][utt_id] > google_prec_recall['recall'][utt_id]:\n",
    "        better_utts['recall'].append(utt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(better_utts['precision']), len(better_utts['recall']), len(model_prec_recall['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_predictions = {}\n",
    "for u, hyp in zip(fsh_filt_utts, h):\n",
    "    model_predictions[u] = hyp\n",
    "# print(model_predictions['20051017_220530_275_fsp-A-6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"better_precision.txt\", \"w\") as out_f:\n",
    "    for utt_id in better_utts['precision']:\n",
    "        ref_line = \" ||| \".join([\" \".join([w.decode() for w in m_dict[utt_id]['en_w'][i]]) for i in range(4)])\n",
    "        model_pred = \" \".join(model_predictions[utt_id])\n",
    "        google_pred = \" \".join(google_s2t_hyps['fisher_dev_r0'][utt_id])\n",
    "        out_line = \"{0:s}---{1:s}---{2:s}---{3:.3f}---{4:s}---{5:.3f}\\n\".format(utt_id, \n",
    "                                                                                ref_line,\n",
    "                                                                                model_pred,\n",
    "                                                                                model_prec_recall['precision'][utt_id], \n",
    "                                                                                google_pred,\n",
    "                                                                                google_prec_recall['precision'][utt_id])\n",
    "        out_f.write(out_line)\n",
    "    # end for\n",
    "# end with\n",
    "with open(\"better_recall.txt\", \"w\") as out_f:\n",
    "    out_line = \"{0:s}---{1:s}---{2:s}---{3:s}\\n\".format('utt_id', 'ref_line', 'model_pred', 'google_pred')\n",
    "    out_f.write(out_line)\n",
    "    for utt_id in better_utts['recall']:\n",
    "        ref_line = \" ||| \".join([\" \".join([w.decode() for w in m_dict[utt_id]['en_w'][i]]) for i in range(4)])\n",
    "        model_pred = \" \".join(model_predictions[utt_id])\n",
    "        google_pred = \" \".join(google_s2t_hyps['fisher_dev_r0'][utt_id])\n",
    "        out_line = \"{0:s}---{1:s}---{2:s}---{3:.3f}---{4:s}---{5:.3f}\\n\".format(utt_id, \n",
    "                                                                                ref_line,\n",
    "                                                                                model_pred,\n",
    "                                                                                model_prec_recall['recall'][utt_id], \n",
    "                                                                                google_pred,\n",
    "                                                                                google_prec_recall['recall'][utt_id])\n",
    "        out_f.write(out_line)\n",
    "    # end for\n",
    "# end with\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(google_prec_recall['recall'].values()), max(model_prec_recall['recall'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_jitter(arr, jitter=0.1):\n",
    "    stdev= jitter * np.std(arr)\n",
    "    return arr + np.random.randn(len(arr)) * stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "x_vals = np.array(list(google_prec_recall['recall'].values()))\n",
    "y_vals = np.array(list(google_prec_recall['precision'].values()))\n",
    "\n",
    "plt.scatter(rand_jitter(x_vals), rand_jitter(y_vals), c=[tableau20[4]], label='google', alpha=0.5)\n",
    "\n",
    "x_vals = np.array(list(model_prec_recall['recall'].values()))\n",
    "y_vals = np.array(list(model_prec_recall['precision'].values()))\n",
    "\n",
    "plt.scatter(rand_jitter(x_vals), rand_jitter(y_vals), c=[tableau20[8]], label='model', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Recall\", size=20)\n",
    "plt.ylabel(\"Precision\", size=20)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)        \n",
    "\n",
    "\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 0.9),\n",
    "                      ncol=1, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,12))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "google_vals = np.array(list(google_prec_recall['precision'].values()))\n",
    "model_vals = np.array(list(model_prec_recall['precision'].values()))\n",
    "x_vals = np.arange(0,len(model_prec_recall['precision']))\n",
    "\n",
    "plt.scatter(x_vals, rand_jitter(google_vals), c=[tableau20[4]], label='google', alpha=0.5)\n",
    "\n",
    "plt.scatter(x_vals, rand_jitter(model_vals), c=[tableau20[8]], label='model', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"utterance\", size=24)\n",
    "plt.ylabel(\"Precision\", size=24)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)        \n",
    "\n",
    "\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 0.9),\n",
    "                      ncol=1, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,12))\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "google_vals = np.array(list(google_prec_recall['recall'].values()))\n",
    "model_vals = np.array(list(model_prec_recall['recall'].values()))\n",
    "x_vals = np.arange(0,len(model_prec_recall['recall']))\n",
    "\n",
    "plt.scatter(x_vals, rand_jitter(google_vals), c=[tableau20[4]], label='google', alpha=0.5)\n",
    "\n",
    "plt.scatter(x_vals, rand_jitter(model_vals), c=[tableau20[8]], label='model', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"utterance\", size=24)\n",
    "plt.ylabel(\"recall\", size=24)\n",
    "\n",
    "plt.yticks(rotation=0, size=18)\n",
    "plt.xticks(rotation=0, size=18)        \n",
    "\n",
    "\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 0.9),\n",
    "                      ncol=1, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5),nrows=1, ncols=2, sharey=True)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "hist_vals = [\n",
    "    (np.array(list(google_prec_recall['precision'].values())), 'google-prec', tableau20[4]),\n",
    "    (np.array(list(google_prec_recall['recall'].values())), 'google-rec', tableau20[4]),\n",
    "    (np.array(list(model_prec_recall['precision'].values())), 'model-prec', tableau20[8]),\n",
    "    (np.array(list(model_prec_recall['recall'].values())), 'model-rec', tableau20[8]),\n",
    "]\n",
    "\n",
    "for i, (vals, lbl, c) in enumerate(hist_vals):\n",
    "    curr_ax = ax[i%2]\n",
    "    curr_ax.hist(vals, label=lbl, color=c, alpha=0.5, bins=20)\n",
    "\n",
    "    curr_ax.set_xlabel(lbl, size=20)\n",
    "    for i in curr_ax.get_xticklabels()+curr_ax.get_yticklabels():\n",
    "        i.set_fontsize(14) \n",
    "#     curr_ax.set_yticks(size=18)\n",
    "#     curr_ax.set_xticks(size=18)        \n",
    "\n",
    "\n",
    "ax[0].legend(loc='upper center', bbox_to_anchor=(0.5, 0.9),\n",
    "                      ncol=4, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "ax[1].legend(loc='upper center', bbox_to_anchor=(0.5, 0.9),\n",
    "                      ncol=4, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_ref = [[\"ha ha lol hue\".split()], [\"ha ha ja ha\".split()], [\"ha ha ja ha\".split()]]\n",
    "# test_h = [\"lol ja\".split(), \"ha he\".split(), \"ha ja\".split()]\n",
    "# _, _ = corpus_precision_recall(test_ref, test_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_better_metrics(metrics_1, metrics_2, preds_1, preds_2, factor, fname, m_dict):\n",
    "    out_fname = \"{0:s}_factor-{1:.2f}.txt\".format(fname, factor)\n",
    "    \n",
    "    better_utts = []\n",
    "    \n",
    "    with open(out_fname, \"w\") as out_f:\n",
    "        for utt_id in metrics_1:\n",
    "            if (metrics_1[utt_id] >= (factor * metrics_2[utt_id])) and metrics_1[utt_id] > 0:\n",
    "                ref_line = \" ||| \".join([\" \".join([w.decode() for w in m_dict[utt_id]['en_w'][i]]) for i in range(4)][:1])\n",
    "                pred_1 = \" \".join(preds_1[utt_id])\n",
    "                pred_2 = \" \".join(preds_2[utt_id])\n",
    "                out_line = \"{0:s}---{1:s}---{2:s}---{3:.3f}---{4:s}---{5:.3f}\\n\".format(utt_id, \n",
    "                                                                                        ref_line,\n",
    "                                                                                        pred_1,\n",
    "                                                                                        metrics_1[utt_id], \n",
    "                                                                                        pred_2,\n",
    "                                                                                        metrics_2[utt_id])\n",
    "                out_f.write(out_line)\n",
    "                better_utts.append(utt_id)\n",
    "            # end if\n",
    "        # end for\n",
    "    # end with\n",
    "    print(\"Writing to file - {0:s}\".format(out_fname))\n",
    "    !wc $out_fname\n",
    "    return sorted([u.rsplit('-',1)[0] for u in better_utts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 3\n",
    "\n",
    "better_utts_prec_model = check_better_metrics(model_prec_recall['precision'], \n",
    "                         google_prec_recall['precision'], \n",
    "                         model_predictions, \n",
    "                         google_s2t_hyps['fisher_dev_r0'],\n",
    "                         factor=factor, \n",
    "                         fname=\"model-google-prec\", \n",
    "                         m_dict=m_dict)\n",
    "\n",
    "better_utts_prec_google = check_better_metrics(google_prec_recall['precision'], \n",
    "                         model_prec_recall['precision'], \n",
    "                         google_s2t_hyps['fisher_dev_r0'],\n",
    "                         model_predictions, \n",
    "                         factor=factor, \n",
    "                         fname=\"google-model-prec\", \n",
    "                         m_dict=m_dict)\n",
    "\n",
    "\n",
    "better_utts_rec_google = check_better_metrics(google_prec_recall['recall'], \n",
    "                         model_prec_recall['recall'],\n",
    "                         google_s2t_hyps['fisher_dev_r0'],\n",
    "                         model_predictions, \n",
    "                         factor=factor, \n",
    "                         fname=\"google-model-rec\", \n",
    "                         m_dict=m_dict)\n",
    "\n",
    "better_utts_rec_model = check_better_metrics(model_prec_recall['recall'], \n",
    "                         google_prec_recall['recall'], \n",
    "                         model_predictions, \n",
    "                         google_s2t_hyps['fisher_dev_r0'],\n",
    "                         factor=factor, \n",
    "                         fname=\"model-google-rec\", \n",
    "                         m_dict=m_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8),nrows=1, ncols=1)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "ax = sns.countplot(y=better_utts_prec_google, color=tableau20[4], **{\"label\":\"google\", \"alpha\":0.5}, ax=ax)\n",
    "ax = sns.countplot(y=better_utts_prec_model, color=tableau20[8], **{\"label\":\"model\", \"alpha\":0.5}, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"precision\", size=20)\n",
    "for i in ax.get_xticklabels()+ax.get_yticklabels():\n",
    "    i.set_fontsize(12) \n",
    "\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.25, 0.9),\n",
    "                      ncol=1, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8),nrows=1, ncols=1)\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "ax = sns.countplot(y=better_utts_rec_google, color=tableau20[4], **{\"label\":\"google\", \"alpha\":0.5}, ax=ax)\n",
    "ax = sns.countplot(y=better_utts_rec_model, color=tableau20[8], **{\"label\":\"model\", \"alpha\":0.5}, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"recall\", size=20)\n",
    "for i in ax.get_xticklabels()+ax.get_yticklabels():\n",
    "    i.set_fontsize(12) \n",
    "\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.25, 0.9),\n",
    "                      ncol=1, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "min_word_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words(m_dict):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u]['en_w']) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u]['en_w']])\n",
    "        else:\n",
    "            for ref in m_dict[u]['en_w']:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_only_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at utterances which our model is doing better at, and compare to Google.\n",
    "\n",
    "Are we doing better on a few calls only? Is there any particular speaker or call messing up our results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
