{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_path = \"./sp2enw_mel-80_vocab-nltk/sp_1.0\"\n",
    "cfg_path = \"sp2enw/sp_1.0_h512_rnn4_l2e-4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train = np.loadtxt(os.path.join(cfg_path, \"train.log\"), delimiter=',', skiprows=False).transpose()\n",
    "log_test = np.genfromtxt(os.path.join(cfg_path, \"dev.log\"), delimiter=',', usecols = (0,1,2)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(8,5)\n",
    "ax1.plot(log_train[0], log_train[1], color='#ff7700')\n",
    "ax1.plot(log_test[0], log_test[1], 'r--')\n",
    "ax1.set_xlabel('epoch', size=24)\n",
    "ax1.set_ylabel('loss', color='r', size=24)\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "    tl.set_fontsize(18)\n",
    "plt.legend(['train loss', 'dev loss'], bbox_to_anchor=(1.45, 0.96), framealpha=0, fontsize=20)    \n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(log_test[0], log_test[2]*100, 'b-')\n",
    "ax2.set_xlabel('iteration')\n",
    "ax2.set_ylabel('dev bleu', color='b', size=24)\n",
    "# ax1.set_xlim(0, 60)\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "    tl.set_fontsize(18) \n",
    "plt.legend(['dev bleu'], bbox_to_anchor=(1.44, 1.04), framealpha=0, fontsize=20)\n",
    "# plt.legend(['dev bleu'], bbox_to_anchor=(1.06, 0.9), framealpha=0, fontsize=20)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_utt(utt, m_dict):\n",
    "    sr, y = scipy.io.wavfile.read(os.path.join(wavs_path, utt.rsplit(\"-\",1)[0]+'.wav'))\n",
    "    start_t = min(seg['start'] for seg in m_dict[utt]['seg'])\n",
    "    end_t = max(seg['end'] for seg in m_dict[utt]['seg'])\n",
    "    print(start_t, end_t)\n",
    "    start_t_samples, end_t_samples = int(start_t*sr), int(end_t*sr)\n",
    "    display(Audio(y[start_t_samples:end_t_samples], rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words(m_dict, v_dict, preds, utts, dec_key, key, play_audio=False, displayN=-1):\n",
    "    if displayN == -1:\n",
    "        displayN = len(utts)\n",
    "    es_ref = []\n",
    "    en_ref = []\n",
    "    en_ref2 = []\n",
    "    en_ref3 = []\n",
    "    en_ref4 = []\n",
    "    for u in utts:\n",
    "        es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "        if type(m_dict[u][dec_key]) == list:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "            en_ref2.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][1]]))\n",
    "            en_ref3.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][2]]))\n",
    "            en_ref4.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][3]]))\n",
    "\n",
    "    en_pred = []\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "    print(\"JOIN _STR:\", dec_key+join_str+dec_key)\n",
    "\n",
    "    for p in preds:\n",
    "        if type(p) == list:\n",
    "            t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "            t_str = t_str[:t_str.find('_EOS')]\n",
    "            en_pred.append(t_str)\n",
    "        else:\n",
    "            en_pred.append(\"\")\n",
    "            \n",
    "    cnt = 0\n",
    "    for u, es, en, en2, en3, en4, p in sorted(list(zip(utts, es_ref, en_ref, en_ref2, en_ref3, en_ref4, en_pred)))[:displayN]:\n",
    "        # for reference, 1st word is GO_ID, no need to display\n",
    "        print(\"Utterance: {0:s}\".format(u))\n",
    "        display_pp = PrettyTable([\"cat\",\"sent\"], hrules=True)\n",
    "        display_pp.align = \"l\"\n",
    "        display_pp.header = False\n",
    "        display_pp.add_row([\"es ref\", textwrap.fill(es,50)])\n",
    "        display_pp.add_row([\"en ref\", textwrap.fill(en,50)])\n",
    "        display_pp.add_row([\"en ref2\", textwrap.fill(en2,50)])\n",
    "        display_pp.add_row([\"en ref3\", textwrap.fill(en3,50)])\n",
    "        display_pp.add_row([\"en ref4\", textwrap.fill(en4,50)])\n",
    "        \n",
    "        \n",
    "        display_pp.add_row([\"en pred\", textwrap.fill(p,50)])\n",
    "        \n",
    "        \n",
    "        cnt += 1\n",
    "\n",
    "        print(display_pp)\n",
    "        if play_audio:\n",
    "            play_utt(u, m_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words_latex(m_dict, v_dict, preds, utts, dec_key):\n",
    "    print(\"min length={0:d}, max length={1:d}\".format(min_len, max_len))\n",
    "    es_ref = []\n",
    "    en_ref = []\n",
    "    for u in utts:\n",
    "        es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "        if type(m_dict[u][dec_key]) == list:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "\n",
    "    en_pred = []\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    for p in preds:\n",
    "        t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "        t_str = t_str[:t_str.find('_EOS')]\n",
    "        en_pred.append(t_str)\n",
    "\n",
    "    total_matching_len = 0\n",
    "\n",
    "    for u, es, en, p in zip(utts, es_ref, en_ref, en_pred):\n",
    "        # for reference, 1st word is GO_ID, no need to display\n",
    "        print(\"Utterance: {0:s}\".format(u))\n",
    "        print(\"{0:d} & {1:s} & {2:s} & {3:s} \\\\\\\\\".format(total_matching_len, es, en, p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg['model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_to_file(m_dict, v_dict, preds, utts, dec_key, key, stemmify=False):\n",
    "    en_hyp = []\n",
    "    en_ref = []\n",
    "    ref_key = 'en_w' if 'en_' in dec_key else 'es_w'\n",
    "    src_key = 'es_w'\n",
    "    \n",
    "    for u in tqdm(utts, ncols=80):\n",
    "        if type(m_dict[u][ref_key]) == list:\n",
    "            if stemmify:\n",
    "                en_ref.append(\" \".join([stem(w.decode()) for w in m_dict[u]['en_w']]))\n",
    "            else:\n",
    "                en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_r_list = []\n",
    "            for r in m_dict[u][ref_key]:\n",
    "                if stemmify:\n",
    "                    en_r_list.append(\" \".join([stem(w.decode()) for w in r]))\n",
    "                else:\n",
    "                    en_r_list.append(\" \".join([w.decode() for w in r]))\n",
    "            en_ref.append(en_r_list)\n",
    "\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    total_matching_len = 0\n",
    "\n",
    "    for u, p in zip(utts, preds):\n",
    "        if stemmify:\n",
    "            t_str = join_str.join([stem(v_dict['i2w'][i].decode()) if i != EOS_ID else EOS.decode() for i in p])\n",
    "        else:\n",
    "            if type(p) == list:\n",
    "                t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "            else:\n",
    "                t_str = \"\"\n",
    "        t_str = t_str[:t_str.find('_EOS')]\n",
    "        en_hyp.append(t_str)\n",
    "\n",
    "    out_fname = os.path.join(m_cfg['model_dir'], \"{0:s}_mt-output\".format(key))\n",
    "    print(\"writing to file: {0:s}\".format(out_fname))\n",
    "    \n",
    "    with open(out_fname, \"w\") as pred_f:\n",
    "        for p in en_hyp:\n",
    "            pred_f.write(\"{0:s}\\n\".format(p))\n",
    "        # end for\n",
    "    # end while\n",
    "    \n",
    "    if type(m_dict[utts[0]][ref_key]) == list:\n",
    "        with open(os.path.join(m_cfg['model_dir'], \"{0:s}.ref0\".format(key)), \"w\") as ref_f:\n",
    "            for r in en_ref:\n",
    "                ref_f.write(\"{0:s}\\n\".format(r))\n",
    "    else:\n",
    "        num_ref = len(m_dict[u][ref_key])\n",
    "        for i in range(num_ref):\n",
    "            with open(os.path.join(m_cfg['model_dir'], \"{0:s}_en.ref{1:d}\".format(key,i)), \"w\") as ref_f:\n",
    "                for r in en_ref:\n",
    "                    ref_f.write(\"{0:s}\\n\".format(r[i]))\n",
    "                # end for each utt\n",
    "            # end with\n",
    "        # end for reference\n",
    "    # end else\n",
    "    print(\"done\")\n",
    "    return en_ref, en_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict = get_data_dicts(m_cfg)\n",
    "batch_size = {'max': 128, 'med': 128, 'min': 128, 'scale': 1}\n",
    "# batch_size = {'max': 1, 'med': 1, 'min': 1, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict[\"fisher_dev\"]['20051009_182032_217_fsp-B-1'][\"en_w\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edin_s2t_refs_for_eval_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"edin_s2t_refs_for_eval.dict\")\n",
    "\n",
    "# if os.path.exists(edin_s2t_refs_for_eval_path):\n",
    "#     print(\"eval refs found, loading\")\n",
    "#     edin_s2t_refs_for_eval = pickle.load(open(edin_s2t_refs_for_eval_path, \"rb\"))\n",
    "# else:\n",
    "#     print(\"eval refs not found, creating\")\n",
    "#     edin_s2t_refs_for_eval = {}\n",
    "#     for u in map_dict[\"fisher_dev\"]:\n",
    "#         edin_s2t_refs_for_eval[u] = []\n",
    "#         for ref in map_dict[\"fisher_dev\"][u][\"en_w\"]:\n",
    "#             edin_s2t_refs_for_eval[u].append([w.decode() for w in ref])\n",
    "\n",
    "#     pickle.dump(edin_s2t_refs_for_eval, open(edin_s2t_refs_for_eval_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sents, utts, dev_loss = feed_model(model,\n",
    "                                        optimizer=optimizer,\n",
    "                                        m_dict=map_dict[dev_key],\n",
    "                                        b_dict=bucket_dict[dev_key],\n",
    "                                        vocab_dict=vocab_dict,\n",
    "                                        batch_size=batch_size,\n",
    "                                        x_key=enc_key,\n",
    "                                        y_key=dec_key,\n",
    "                                        train=False,\n",
    "                                        input_path=input_path,\n",
    "                                        max_dec=m_cfg['max_en_pred'],\n",
    "                                        t_cfg=t_cfg,\n",
    "                                        use_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([vocab_dict['en_w']['i2w'][i].decode() for i in pred_sents[100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, 200\n",
    "# min_len, max_len = 0, 1\n",
    "displayN = 50\n",
    "m_dict=map_dict[dev_key]\n",
    "# wavs_path = os.path.join(m_cfg['data_path'], \"wavs\")\n",
    "wavs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"wavs\")\n",
    "v_dict = vocab_dict[dec_key]\n",
    "key = m_cfg['dev_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m_dict['20051009_182032_217_fsp-B-1'][dec_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsh_filt_pred, fsh_filt_utts = zip(*sorted([(p,u) for p, u in zip(pred_sents, utts) if (len(m_dict[u]['es_w']) >= min_len) and \n",
    "                                        (len(m_dict[u]['es_w']) <= max_len)], key=lambda t:t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length filtered utterances = {0:d}\".format(len(fsh_filt_utts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "813 / 3977, (50+115+15+171) / 3977, (50+115+15+171) / 813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_words(m_dict, v_dict, \n",
    "              fsh_filt_pred, \n",
    "              fsh_filt_utts, dec_key, \n",
    "              key, \n",
    "              play_audio=True, \n",
    "              displayN=displayN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_ref = []\n",
    "en_ref = []\n",
    "for u in fsh_filt_utts:\n",
    "    es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "    if type(m_dict[u][dec_key]) == list:\n",
    "        en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "    else:\n",
    "        en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "\n",
    "en_pred = []\n",
    "join_str = ' ' if dec_key.endswith('_w') else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"/afs/inf.ed.ac.uk/group/project/lowres/work/speech2text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fsh_filt_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, chrf, h, r = calc_bleu(m_dict, \n",
    "                          v_dict, \n",
    "                          fsh_filt_pred, \n",
    "                          fsh_filt_utts, \n",
    "                          dec_key, \n",
    "                          ref_index=ref_index)\n",
    "\n",
    "print(\"BLEU score on: {0:s} = {1:.2f}\".format(key, b * 100))\n",
    "print(\"-\"*60)\n",
    "\n",
    "model_refs = {u: mr for u, mr in zip(fsh_filt_utts, r)}\n",
    "model_hyps = {u: mh for u, mh in zip(fsh_filt_utts, h)}\n",
    "\n",
    "all_weights=[(1.,0.,0.,0.),\n",
    "             (0.,1.,0.,0.),\n",
    "             (0.,0.,1.,0.),\n",
    "             (0.,0.,0.,1.),\n",
    "             (1./2,1./2,0.,0.),\n",
    "             (1./3,1./3,1./3,0.),\n",
    "             (.25,.25,.25,.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bleu(r, h, smoothing_function=smooth_fun.method2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ref, en_hyp = write_predictions_to_file(m_dict, v_dict, fsh_filt_pred, fsh_filt_utts, \n",
    "                                           dec_key, key, stemmify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_refs_dict = {u:v[0].strip().split() for u, v in zip(fsh_filt_utts, en_ref)}\n",
    "model_hyps_dict = {u:v.strip().split() for u, v in zip(fsh_filt_utts, en_hyp)}\n",
    "\n",
    "model_refs_for_eval = {}\n",
    "for u, r in zip(fsh_filt_utts, en_ref):\n",
    "    model_refs_for_eval[u] = []\n",
    "    for curr_r in r:\n",
    "        model_refs_for_eval[u].append(curr_r.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model_refs_for_eval.values())[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_refs_dict, open(os.path.join(m_cfg['model_dir'], \"model_s2t_refs.dict\"), \"wb\"))\n",
    "pickle.dump(model_hyps_dict, open(os.path.join(m_cfg['model_dir'], \"model_s2t_hyps.dict\"), \"wb\"))\n",
    "pickle.dump(model_refs_for_eval, open(os.path.join(m_cfg['model_dir'], \"model_s2t_refs_for_eval.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, _ = corpus_precision_recall(r, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prec_recall = {'precision' : {}, 'recall' : {}, \"tp\": 0, \"tc\": 0, \"tr\": 0}\n",
    "\n",
    "for utt_id, ref, hyp in zip(fsh_filt_utts, r, h):\n",
    "    es_ref = [w.decode() for w in m_dict[utt_id]['es_w']]\n",
    "    \n",
    "    pval, rval = modified_precision_recall(ref, hyp, n=1)\n",
    "    model_prec_recall['tc'] += pval.numerator\n",
    "    model_prec_recall['tp'] += pval.denominator\n",
    "    model_prec_recall['tr'] += rval.denominator\n",
    "\n",
    "    model_prec_recall['precision'][utt_id] = float(pval)\n",
    "    model_prec_recall['recall'][utt_id] = float(rval)\n",
    "# end for\n",
    "    \n",
    "model_prec_recall['total_precision'] = model_prec_recall['tc'] / model_prec_recall['tp']\n",
    "model_prec_recall['total_recall'] = model_prec_recall['tc'] / model_prec_recall['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tModel metrics\")\n",
    "print(\"-\"*60)\n",
    "print(\"{0:10s} = {1:0.3f}\\n{2:10s} = {3:0.3f}\".format(\"precision\",\n",
    "                                                      model_prec_recall['total_precision'],\n",
    "                                                      \"recall\",\n",
    "                                                      model_prec_recall['total_recall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model-s2t BLEU score:\")\n",
    "\"{0:0.3f}\".format(100.0 * corpus_bleu(model_refs.values(), model_hyps.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "_, _ = corpus_precision_recall(model_refs.values(), model_hyps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "metrics = {\"p\": {1: [], 2: [], 3: [], 4: []},\n",
    "           \"r\": {1: [], 2: [], 3: [], 4: []}}\n",
    "\n",
    "for ix in range(len(list(model_refs.values())[0])):\n",
    "    temp_refs = [[i[ix]] for i in model_refs.values()]\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\t\\tUsing reference = {0:d}\".format(ix+1))\n",
    "    print(\"-\"*60)\n",
    "    ps, rs = corpus_precision_recall(temp_refs, model_hyps.values())\n",
    "    for i, (p, r) in enumerate(zip(ps, rs)):\n",
    "        metrics['p'][i+1].append(p)\n",
    "        metrics['r'][i+1].append(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(vals_dict):\n",
    "#     print(\"estimated mean\")\n",
    "    k = []\n",
    "    u = []\n",
    "    s = []\n",
    "    for m, vals in vals_dict.items():\n",
    "        k.append(m)\n",
    "        u.append(np.mean(vals))\n",
    "        s.append(np.std(vals)/np.sqrt(len(vals)))\n",
    "        print(\"{0:10d}-gram = {1:.2f} Â± {2:.2f}\".format(m, np.mean(vals), np.std(vals)/np.sqrt(len(vals))))\n",
    "        \n",
    "    print(\",\".join(map(lambda x : \"{0:.2f}\".format(x), u)))\n",
    "    print(\",\".join(map(lambda x : \"{0:.2f}\".format(x), s)))\n",
    "    return k, u, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2w_p_r = {}\n",
    "\n",
    "print(\"Precision:\")\n",
    "_, s2w_p_r['p'], s2w_p_r['p_std'] = get_mean_std(metrics['p'])\n",
    "\n",
    "print(\"Recall:\")\n",
    "_, s2w_p_r['r'], s2w_p_r['r_std'] = get_mean_std(metrics['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(m_cfg['model_dir'], \"s2w_p_r.json\"), \"w\") as out_f:\n",
    "    json.dump(s2w_p_r, out_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = {}\n",
    "for u, hyp in zip(fsh_filt_utts, h):\n",
    "    model_predictions[u] = hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_s2t_refs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"google_s2t_refs.dict\")\n",
    "google_s2t_hyps_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"google_s2t_hyps.dict\")\n",
    "google_s2t_refs_for_eval_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"google_s2t_refs_for_eval.dict\")\n",
    "\n",
    "google_s2t_hyps = pickle.load(open(google_s2t_hyps_path, \"rb\"))\n",
    "google_hyp_r0 = google_s2t_hyps['fisher_dev_r0']\n",
    "\n",
    "google_s2t_refs = pickle.load(open(google_s2t_refs_path, \"rb\"))\n",
    "google_dev_ref_0 = google_s2t_refs['fisher_dev_ref_0']\n",
    "\n",
    "if os.path.exists(google_s2t_refs_for_eval_path):\n",
    "    print(\"eval refs found, loading\")\n",
    "    google_s2t_refs_for_eval = pickle.load(open(google_s2t_refs_for_eval_path, \"rb\"))\n",
    "else:\n",
    "    print(\"eval refs not found, creating\")\n",
    "    google_s2t_refs_for_eval = {}\n",
    "    for u in google_dev_ref_0:\n",
    "        google_s2t_refs_for_eval[u] = []\n",
    "        for ref in google_s2t_refs:\n",
    "            google_s2t_refs_for_eval[u].append(google_s2t_refs[ref][u])\n",
    "\n",
    "    google_s2t_refs_for_eval = pickle.dump(google_s2t_refs_for_eval, open(google_s2t_refs_for_eval_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_precision_recall(google_all_dev_refs.values(), google_hyp_r0.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "min_word_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(m_dict):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u]['en_w']) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u]['en_w']])\n",
    "        else:\n",
    "            for ref in m_dict[u]['en_w']:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_only_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at utterances which our model is doing better at, and compare to Google.\n",
    "\n",
    "Are we doing better on a few calls only? Is there any particular speaker or call messing up our results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Query task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\"\n",
    "train_text_fname= \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\"\n",
    "topics_fname = \"../criseslex/fsp06_topics_in_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [ \"peace\", \"Music\", \"Marriage\", \"Religion\", \"Cell phones\", \n",
    "           \"Dating\", \"Telemarketing and SPAM\", \"Politics\", \"Travel\", \n",
    "           \"Technical devices\", \"Healthcare\", \"Advertisements\", \"Power\", \n",
    "           \"Occupations\", \"Movies\", \"Welfare\", \"Breaking up\", \"Location\", \n",
    "           \"Justice\", \"Memories\", \"Crime\", \"Violence against women\", \"Equality\", \n",
    "            \"Housing\", \"Immigration\",     \n",
    "            # new topics\n",
    "           \"Interracial\", \"Christians\", \"muslims\", \"jews\", \"e-mail\", \n",
    "           \"phone\", \"democracy\", \"Democratic\", \"Republican\", \"technology\", \n",
    "           \"leadership\", \"community\", \"jury\", \"police\", \"inequality\", \n",
    "           \"renting\", \"Violence\", \"immigrants\", \"immigrant\", \"skilled\", \n",
    "           \"Telemarketing\", \"SPAM\", \"skill\", \"job\", \"health\", \"mobile\", \n",
    "            \"ads\", \"physical\", \"emotional\", \"bubble\", \"rent\", \"economy\", \n",
    "            \"abuse\", \"women\", \"city\", \"country\", \"suburban\", \"dollar\", \n",
    "            \"united states\", \"laws\", \"phone\", \"race\", \"biracial\", \"interracial\", \n",
    "            \"marriage\", \"lyrics\", \"sexuality\", \"medicine\", \"television\", \"european\",\n",
    "            \"home\", \"protect\", \"spouse\", \"language\", \"cellphone\", \"money\",\n",
    "            \"doctor\", \"insurance\", \"cigarettes\", \"alcohol\", \"income\", \"salary\",\n",
    "            \"class\", \"censor\", \"rating\", \"programs\", \"government\",\n",
    "            \"relationship\", \"legal\", \"event\", \"life\", \"safe\", \"victim\", \"cops\",\n",
    "            \"wage\", \"illegal\"\n",
    "            ]\n",
    "topics = list(set(t.lower() for t in topics))\n",
    "topics_stem = [stem(t) for t in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words_stem = {}\n",
    "for w in dev_words:\n",
    "    stem_w = stem(w)\n",
    "    if stem_w not in dev_words_stem:\n",
    "        dev_words_stem[stem_w] = 0\n",
    "    dev_words_stem[stem_w] += dev_words[w]\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this checks for # of occurrences of the word in the specified text\n",
    "# We later use the # of utterrances, which will be lower or equal to this number\n",
    "prune_topics = [t for t in topics if dev_words_stem.get(stem(t),0) > 10 and dev_words_stem.get(stem(t),0) < 100]\n",
    "len(prune_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"selec_query_terms\")\n",
    "sel_topics = random.sample(prune_topics, min(len(prune_topics), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sel_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_terms_references(utt_text, terms, text_key=\"en_w\"):\n",
    "    terms_set = set([stem(i) for i in terms])\n",
    "    terms_search_res = {}\n",
    "    full_words = {}\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        l = (utt_text[u][text_key] if type(utt_text[u]) == dict \n",
    "             else utt_text[u])\n",
    "        for r in l:\n",
    "            for w in set(r):\n",
    "                decoded_w = w.decode() if type(w) != str else w\n",
    "                w_to_search = stem(decoded_w)\n",
    "                if w_to_search in terms_set:\n",
    "                    if w_to_search not in terms_search_res:\n",
    "                        terms_search_res[w_to_search] = set()\n",
    "                        full_words[w_to_search] = set()\n",
    "                    terms_search_res[w_to_search].add(u)\n",
    "                    full_words[w_to_search].add(decoded_w)\n",
    "                # end if found\n",
    "            # end for current reference\n",
    "        # end for all references\n",
    "    # end for all utterances\n",
    "    return terms_search_res, full_words\n",
    "\n",
    "\n",
    "def find_all_terms_predictions(utt_text, terms):\n",
    "    terms_set = set([stem(i) for i in terms])\n",
    "    terms_search_res = {}\n",
    "    full_words = {}\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        r = utt_text[u]\n",
    "        for w in set(r):\n",
    "            w_to_search = stem(w)\n",
    "            if w_to_search in terms_set:\n",
    "                if w_to_search not in terms_search_res:\n",
    "                    terms_search_res[w_to_search] = set()\n",
    "                    full_words[w_to_search] = set()\n",
    "                terms_search_res[w_to_search].add(u)\n",
    "                full_words[w_to_search].add(u)\n",
    "                # end if found\n",
    "            # end for current reference\n",
    "    # end for all utterances\n",
    "    return terms_search_res, full_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_prec_recall(preds, refs):\n",
    "    prec_recall = {}\n",
    "    prec_recall = {'t':0, 'tp':0, 'tc':0, 'terms':{}}\n",
    "    for term in refs.keys():\n",
    "        prec_recall['terms'][term] = {}\n",
    "        prec_recall['terms'][term]['t'] = len(refs[term])\n",
    "        preds_occ = preds.get(term, set())\n",
    "        prec_recall['terms'][term]['tp'] = len(preds_occ)\n",
    "        prec_recall['terms'][term]['tc'] = len(refs[term] & preds_occ)\n",
    "        prec_recall['t'] += prec_recall['terms'][term]['t']\n",
    "        prec_recall['tp'] += prec_recall['terms'][term]['tp']\n",
    "        prec_recall['tc'] += prec_recall['terms'][term]['tc']\n",
    "    # end for each term\n",
    "    return prec_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_topics, ref_topic_labels = find_all_terms_references(map_dict['fisher_dev'], sel_topics)\n",
    "pred_topics, pred_topics_topics_labels = find_all_terms_predictions(model_predictions, sel_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_topics), len(pred_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_topics_in_utts = {k:v for k, v in ref_topics.items() if len(v) >=3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prune_topics_in_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(6,10),nrows=1, ncols=1)\n",
    "\n",
    "\n",
    "sorted_counts = [t[0] for t in sorted(ref_topics.items(), reverse=True, key=lambda t:len(t[1]))]\n",
    "topic_count = [len(ref_topics[t]) for t in sorted_counts]\n",
    "\n",
    "topic_labels = [\"-\".join(list(ref_topic_labels[t])[:3]) for t in sorted_counts]\n",
    "\n",
    "ax = sns.barplot(x=topic_count, \n",
    "                 y=topic_labels, \n",
    "                 color=tableau20[6], alpha=.7)\n",
    "                 #**{\"label\":\"topic counts\", \"alpha\":0.5}, ax=ax)\n",
    "\n",
    "# ax.set_xlabel(\"# of utts in which topic occurs\", size=20)\n",
    "\n",
    "max_count_val = max([len(v) for v in ref_topics.values()])\n",
    "min_count_val = max([len(v) for v in ref_topics.values()])\n",
    "plt.xticks(list(range(0,max_count_val, 5))+[min_count_val, max_count_val])\n",
    "\n",
    "# ax.legend(loc='upper right', bbox_to_anchor=(1.1, 0.9),\n",
    "#                       ncol=1, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "sns.despine(left=True, bottom=True, top=True, right=False)\n",
    "\n",
    "for t in ax.get_yticklabels():\n",
    "    t.set_fontsize(15) \n",
    "    \n",
    "for i, t in enumerate(ax.get_xticklabels()):\n",
    "    if i > 0 and i < (len(ax.get_xticklabels())-1):\n",
    "        t.set_visible(False)\n",
    "    else:\n",
    "        t.set_fontsize(16)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig.savefig(\"../criseslex/sel_topics_new.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics_p_r = terms_prec_recall(preds = pred_topics, refs = ref_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics_p_r['precision'] = model_topics_p_r['tc'] / model_topics_p_r['tp']\n",
    "model_topics_p_r['recall'] = model_topics_p_r['tc'] / model_topics_p_r['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"-------------- Query by Text\")\n",
    "print(\"-\"*40)\n",
    "print(\"{0:20s} | {1:5.2f}%\".format(\"precision\", model_topics_p_r['precision']*100.0))\n",
    "print(\"{0:20s} | {1:5.2f}%\".format(\"recall\", model_topics_p_r['recall']*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(m_cfg['model_dir'], \"topics_p_r.json\"), \"w\") as out_f:\n",
    "    json.dump(model_topics_p_r, out_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_terms_es_references(utt_text, term):\n",
    "    terms_search_res = set()\n",
    "    total_word_count = 0\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        l = (utt_text[u][\"es_w\"] if type(utt_text[u]) == dict \n",
    "             else utt_text[u])\n",
    "        #if len(l) == 1 and l[0].decode() == term:\n",
    "        if set(l) == set(term.split()):\n",
    "            terms_search_res.add(u)\n",
    "            total_word_count += 1\n",
    "            # end if found\n",
    "        # end for current reference\n",
    "        # end for all references\n",
    "    # end for all utterances\n",
    "    return terms_search_res, total_word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_key = \"fisher_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hmm, tot_count = find_all_terms_es_references(map_dict[set_key], b'claro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_words = {}\n",
    "all_match_count = 0\n",
    "no_match_count = 0\n",
    "\n",
    "utt_refs_set = {}\n",
    "for utt in hmm:\n",
    "    #print(\"-\"*60)\n",
    "    #print(utt)\n",
    "    curr_ref = {k : \"\" for k in range(4)}\n",
    "    if \"train\" in set_key:\n",
    "        r = map_dict[set_key][utt][\"en_w\"]\n",
    "        en_ref_words = \" \".join(w.decode() for w in r)\n",
    "        curr_ref[0] = en_ref_words\n",
    "        if en_ref_words not in all_ref_words:\n",
    "            all_ref_words[en_ref_words] = 0\n",
    "        all_ref_words[en_ref_words] += 1\n",
    "    else:\n",
    "        for i, r in enumerate(map_dict[set_key][utt][\"en_w\"]):\n",
    "            en_ref_words = \" \".join(w.decode() for w in r)\n",
    "            curr_ref[i] = en_ref_words\n",
    "            #print(\" \".join(w.decode() for w in r), end=\"----\")\n",
    "            if en_ref_words not in all_ref_words:\n",
    "                all_ref_words[en_ref_words] = 0\n",
    "            all_ref_words[en_ref_words] += 1\n",
    "    #print(curr_ref.values())\n",
    "\n",
    "    \n",
    "    utt_refs_set[utt] = set(curr_ref.values())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utt_refs_set['20051023_232057_325_fsp-B-77']) if \"dev\" in set_key else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if \"dev\" in set_key:\n",
    "    start_set = utt_refs_set['20051023_232057_325_fsp-B-77']\n",
    "    for utt in utt_refs_set:\n",
    "        start_set = start_set & utt_refs_set[utt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_match_count = sum([len(utt_refs_set[utt]) == 1 for utt in utt_refs_set])\n",
    "all_dont_match_count = sum([len(utt_refs_set[utt]) == 4  for utt in utt_refs_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_match_count, all_dont_match_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ref_words, len(all_ref_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_refs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dict = map_dict[set_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_utts = [utt for utt in utt_refs_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_batch(m_dict, enc_key, dec_key, sel_utts[:101], vocab_dict, 200, 10, input_path=input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = xp.asnumpy(batch_data['X'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "distance, path = fastdtw(A[1], A[2], dist=euclidean)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_out = np.zeros((len(A), len(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(A))):\n",
    "    for j in range(i,len(A)):\n",
    "        dtw_out[i,j], _ = fastdtw(A[i], A[j], dist=euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = dtw_out + dtw_out.T - - np.diag(np.diag(dtw_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dist_matrix), np.std(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dist_matrix, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(dist_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "with sns.axes_style(\"white\"):\n",
    "    ax = sns.heatmap(dist_matrix, mask=mask, square=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "ward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X)\n",
    "elapsed_time = time.time() - st\n",
    "label = ward.labels_\n",
    "print(\"Elapsed time: %.2fs\" % elapsed_time)\n",
    "print(\"Number of points: %i\" % label.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A.reshape(len(A),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "B_r = pca.fit(B).transform(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n",
    "               color=plt.cm.jet(np.float(l) / np.max(label + 1)),\n",
    "               s=20, edgecolor='k')\n",
    "plt.title('Without connectivity constraints (time %.2fs)' % elapsed_time)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Define the structure A of the data. Here a 10 nearest neighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "connectivity = kneighbors_graph(X, n_neighbors=25, include_self=False)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute clustering\n",
    "print(\"Compute structured hierarchical clustering...\")\n",
    "st = time.time()\n",
    "ward = AgglomerativeClustering(n_clusters=10, connectivity=connectivity,\n",
    "                               linkage='ward').fit(X)\n",
    "elapsed_time = time.time() - st\n",
    "label = ward.labels_\n",
    "print(\"Elapsed time: %.2fs\" % elapsed_time)\n",
    "print(\"Number of points: %i\" % label.size)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "fig = plt.figure()\n",
    "ax = p3.Axes3D(fig)\n",
    "ax.view_init(7, -80)\n",
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n",
    "               color=plt.cm.jet(float(l) / np.max(label + 1)),\n",
    "               s=20, edgecolor='k')\n",
    "plt.title('With connectivity constraints (time %.2fs)' % elapsed_time)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward = AgglomerativeClustering(n_clusters=5, connectivity=connectivity,\n",
    "                               linkage='ward').fit(X)\n",
    "\n",
    "labels = ward.labels_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for color, l in zip(tableau20, np.unique(labels)):\n",
    "    ax.scatter(B_r[labels == l, 0], B_r[labels == l, 1], s=100,\n",
    "               color=plt.cm.jet(float(l) / np.max(labels + 1)), edgecolor='k')\n",
    "plt.yticks(visible=False)\n",
    "plt.xticks(visible=False)\n",
    "fig.savefig(\"agglo_dtw_claro.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = KMeans(n_clusters=5)\n",
    "est.fit(B)\n",
    "labels = est.labels_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for color, l in zip(tableau20, np.unique(labels)):\n",
    "    ax.scatter(B_r[labels == l, 0], B_r[labels == l, 1], s=100,\n",
    "               color=plt.cm.jet(float(l) / np.max(labels + 1)), edgecolor='k')\n",
    "# done\n",
    "plt.yticks(visible=False)\n",
    "plt.xticks(visible=False)\n",
    "fig.savefig(\"k_means_claro.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(list(ward.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ward.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in np.unique(label):\n",
    "    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X[label == l, 0], X[label == l, 1], X[label == l, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results on an image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(X, cmap=plt.cm.gray)\n",
    "for l in range(n_clusters):\n",
    "    plt.contour(label == l, contours=1,\n",
    "                colors=[plt.cm.spectral(l / float(n_clusters)), ])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(A)\n",
    "print('pairwise dense output:\\n {}\\n'.format(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"20051021_222225_307_fsp-B-109\" in map_dict['fisher_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(m_dict, x_key, y_key, utt_list, vocab_dict,\n",
    "              max_enc, max_dec, input_path=''):\n",
    "    batch_data = {'X':[], 'y':[]}\n",
    "    # -------------------------------------------------------------------------\n",
    "    # loop through each utterance in utt list\n",
    "    # -------------------------------------------------------------------------\n",
    "    for u in utt_list:\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add X data\n",
    "        # ---------------------------------------------------------------------\n",
    "        if x_key == 'sp':\n",
    "            # -----------------------------------------------------------------\n",
    "            # for speech data\n",
    "            # -----------------------------------------------------------------\n",
    "            # get path to speech file\n",
    "            utt_sp_path = os.path.join(input_path, \"{0:s}.npy\".format(u))\n",
    "            if not os.path.exists(utt_sp_path):\n",
    "                # for training data, there are sub-folders\n",
    "                utt_sp_path = os.path.join(input_path,\n",
    "                                           u.split('_',1)[0],\n",
    "                                           \"{0:s}.npy\".format(u))\n",
    "            if os.path.exists(utt_sp_path):\n",
    "                x_data = xp.load(utt_sp_path)\n",
    "                # truncate max length\n",
    "                batch_data['X'].append(x_data[:max_enc])\n",
    "            else:\n",
    "                # -------------------------------------------------------------\n",
    "                # exception if file not found\n",
    "                # -------------------------------------------------------------\n",
    "                print(\"ERROR!! file not found: {0:s}\".format(utt_sp_path))\n",
    "                # -------------------------------------------------------------\n",
    "        else:\n",
    "            # -----------------------------------------------------------------\n",
    "            # for text data\n",
    "            # -----------------------------------------------------------------\n",
    "            x_ids = [vocab_dict[x_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][x_key]]\n",
    "            x_ids = xp.asarray(x_ids, dtype=xp.int32)\n",
    "            batch_data['X'].append(x_ids[:max_enc])\n",
    "            # -----------------------------------------------------------------\n",
    "        # ---------------------------------------------------------------------\n",
    "        #  add labels\n",
    "        # ---------------------------------------------------------------------\n",
    "        if type(m_dict[u][y_key]) == list:\n",
    "            en_ids = [vocab_dict[y_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key]]\n",
    "        else:\n",
    "            # dev and test data have multiple translations\n",
    "            # choose the first one for computing perplexity\n",
    "            en_ids = [vocab_dict[y_key]['w2i'].get(w, UNK_ID) for w in m_dict[u][y_key][0]]\n",
    "        y_ids = [GO_ID] + en_ids[:max_dec-2] + [EOS_ID]\n",
    "        batch_data['y'].append(xp.asarray(y_ids, dtype=xp.int32))\n",
    "        # ---------------------------------------------------------------------\n",
    "    # -------------------------------------------------------------------------\n",
    "    # end for all utterances in batch\n",
    "    # -------------------------------------------------------------------------\n",
    "    if len(batch_data['X']) > 0 and len(batch_data['y']) > 0:\n",
    "        batch_data['X'] = F.pad_sequence(batch_data['X'], padding=PAD_ID)\n",
    "        batch_data['y'] = F.pad_sequence(batch_data['y'], padding=PAD_ID)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puerto rico utterance\n",
    "# \"i 'm from puerto rico but i live here in denver colorado\"\n",
    "\" \".join(w.decode() for w in map_dict['fisher_dev']['20051023_232057_325_fsp-A-3']['en_w'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
