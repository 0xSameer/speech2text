{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "# Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "for i in range(len(tableau20)):    \n",
    "    r, g, b = tableau20[i]    \n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_fun = nltk.translate.bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"./sp2enw/sp_.50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train = np.loadtxt(os.path.join(cfg_path, \"train.log\"), delimiter=',', skiprows=False).transpose()\n",
    "log_test = np.genfromtxt(os.path.join(cfg_path, \"dev.log\"), delimiter=',', usecols = (0,1,2)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "fig.set_size_inches(8,5)\n",
    "ax1.plot(log_train[0], log_train[1], color='#ff7700')\n",
    "ax1.plot(log_test[0], log_test[1], 'r--')\n",
    "ax1.set_xlabel('epoch', size=24)\n",
    "ax1.set_ylabel('loss', color='r', size=24)\n",
    "for tl in ax1.get_yticklabels():\n",
    "    tl.set_color('r')\n",
    "    tl.set_fontsize(18)\n",
    "plt.legend(['train loss', 'dev loss'], bbox_to_anchor=(1.45, 0.96), framealpha=0, fontsize=20)    \n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(log_test[0], log_test[2]*100, 'b-')\n",
    "ax2.set_xlabel('iteration')\n",
    "ax2.set_ylabel('dev bleu', color='b', size=24)\n",
    "# ax1.set_xlim(0, 60)\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('b')\n",
    "    tl.set_fontsize(18) \n",
    "plt.legend(['dev bleu'], bbox_to_anchor=(1.44, 1.04), framealpha=0, fontsize=20)\n",
    "# plt.legend(['dev bleu'], bbox_to_anchor=(1.06, 0.9), framealpha=0, fontsize=20)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_utt(utt, m_dict):\n",
    "    sr, y = scipy.io.wavfile.read(os.path.join(wavs_path, utt.rsplit(\"-\",1)[0]+'.wav'))\n",
    "    start_t = min(seg['start'] for seg in m_dict[utt]['seg'])\n",
    "    end_t = max(seg['end'] for seg in m_dict[utt]['seg'])\n",
    "    print(start_t, end_t)\n",
    "    start_t_samples, end_t_samples = int(start_t*sr), int(end_t*sr)\n",
    "    display(Audio(y[start_t_samples:end_t_samples], rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words(m_dict, v_dict, preds, utts, dec_key, key, play_audio=False, displayN=-1):\n",
    "    if displayN == -1:\n",
    "        displayN = len(utts)\n",
    "    es_ref = []\n",
    "    en_ref = []\n",
    "    for u in utts:\n",
    "        es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "        if type(m_dict[u][dec_key]) == list:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "\n",
    "    en_pred = []\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    for p in preds:\n",
    "        t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "        t_str = t_str[:t_str.find('_EOS')]\n",
    "        en_pred.append(t_str)\n",
    "\n",
    "    for u, es, en, p in sorted(list(zip(utts, es_ref, en_ref, en_pred)))[:displayN]:\n",
    "        # for reference, 1st word is GO_ID, no need to display\n",
    "        print(\"Utterance: {0:s}\".format(u))\n",
    "        display_pp = PrettyTable([\"cat\",\"sent\"], hrules=True)\n",
    "        display_pp.align = \"l\"\n",
    "        display_pp.header = False\n",
    "        display_pp.add_row([\"es ref\", textwrap.fill(es,50)])\n",
    "        display_pp.add_row([\"en ref\", textwrap.fill(en,50)])\n",
    "        display_pp.add_row([\"en pred\", textwrap.fill(p,50)])\n",
    "\n",
    "        print(display_pp)\n",
    "        if play_audio:\n",
    "            play_utt(u, m_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words_latex(m_dict, v_dict, preds, utts, dec_key):\n",
    "    print(\"min length={0:d}, max length={1:d}\".format(min_len, max_len))\n",
    "    es_ref = []\n",
    "    en_ref = []\n",
    "    for u in utts:\n",
    "        es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "        if type(m_dict[u][dec_key]) == list:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "\n",
    "    en_pred = []\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    for p in preds:\n",
    "        t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "        t_str = t_str[:t_str.find('_EOS')]\n",
    "        en_pred.append(t_str)\n",
    "\n",
    "    total_matching_len = 0\n",
    "\n",
    "    for u, es, en, p in zip(utts, es_ref, en_ref, en_pred):\n",
    "        # for reference, 1st word is GO_ID, no need to display\n",
    "        print(\"Utterance: {0:s}\".format(u))\n",
    "        print(\"{0:d} & {1:s} & {2:s} & {3:s} \\\\\\\\\".format(total_matching_len, es, en, p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cfg['model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_to_file(m_dict, v_dict, preds, utts, dec_key, key, stemmify=False):\n",
    "    en_hyp = []\n",
    "    en_ref = []\n",
    "    ref_key = 'en_w' if 'en_' in dec_key else 'es_w'\n",
    "    src_key = 'es_w'\n",
    "    \n",
    "    for u in tqdm(utts, ncols=80):\n",
    "        if type(m_dict[u][ref_key]) == list:\n",
    "            if stemmify:\n",
    "                en_ref.append(\" \".join([stem(w.decode()) for w in m_dict[u]['en_w']]))\n",
    "            else:\n",
    "                en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_r_list = []\n",
    "            for r in m_dict[u][ref_key]:\n",
    "                if stemmify:\n",
    "                    en_r_list.append(\" \".join([stem(w.decode()) for w in r]))\n",
    "                else:\n",
    "                    en_r_list.append(\" \".join([w.decode() for w in r]))\n",
    "            en_ref.append(en_r_list)\n",
    "\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    total_matching_len = 0\n",
    "\n",
    "    for u, p in zip(utts, preds):\n",
    "        if stemmify:\n",
    "            t_str = join_str.join([stem(v_dict['i2w'][i].decode()) if i != EOS_ID else EOS.decode() for i in p])\n",
    "        else:\n",
    "            t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "        t_str = t_str[:t_str.find('_EOS')]\n",
    "        en_hyp.append(t_str)\n",
    "\n",
    "    out_fname = os.path.join(m_cfg['model_dir'], \"{0:s}_mt-output\".format(key))\n",
    "    print(\"writing to file: {0:s}\".format(out_fname))\n",
    "    \n",
    "    with open(out_fname, \"w\") as pred_f:\n",
    "        for p in en_hyp:\n",
    "            pred_f.write(\"{0:s}\\n\".format(p))\n",
    "        # end for\n",
    "    # end while\n",
    "    \n",
    "    if type(m_dict[utts[0]][ref_key]) == list:\n",
    "        with open(os.path.join(m_cfg['model_dir'], \"{0:s}.ref0\".format(key)), \"w\") as ref_f:\n",
    "            for r in en_ref:\n",
    "                ref_f.write(\"{0:s}\\n\".format(r))\n",
    "    else:\n",
    "        num_ref = len(m_dict[u][ref_key])\n",
    "        for i in range(num_ref):\n",
    "            with open(os.path.join(m_cfg['model_dir'], \"{0:s}_en.ref{1:d}\".format(key,i)), \"w\") as ref_f:\n",
    "                for r in en_ref:\n",
    "                    ref_f.write(\"{0:s}\\n\".format(r[i]))\n",
    "                # end for each utt\n",
    "            # end with\n",
    "        # end for reference\n",
    "    # end else\n",
    "    print(\"done\")\n",
    "    return en_ref, en_hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict = get_data_dicts(m_cfg)\n",
    "batch_size = {'max': 96, 'med': 128, 'min': 256, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sents, utts, dev_loss = feed_model(model,\n",
    "                                        optimizer=optimizer,\n",
    "                                        m_dict=map_dict[dev_key],\n",
    "                                        b_dict=bucket_dict[dev_key],\n",
    "                                        vocab_dict=vocab_dict,\n",
    "                                        batch_size=batch_size,\n",
    "                                        x_key=enc_key,\n",
    "                                        y_key=dec_key,\n",
    "                                        train=False,\n",
    "                                        input_path=input_path,\n",
    "                                        max_dec=m_cfg['max_en_pred'],\n",
    "                                        t_cfg=t_cfg,\n",
    "                                        use_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, m_cfg['max_en_pred']\n",
    "# min_len, max_len = 0, 5\n",
    "displayN = 50\n",
    "m_dict=map_dict[dev_key]\n",
    "# wavs_path = os.path.join(m_cfg['data_path'], \"wavs\")\n",
    "wavs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"wavs\")\n",
    "v_dict = vocab_dict['en_w']\n",
    "key = m_cfg['dev_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsh_filt_pred, fsh_filt_utts = zip(*sorted([(p,u) for p, u in zip(pred_sents, utts) if (len(m_dict[u]['es_w']) >= min_len) and \n",
    "                                        (len(m_dict[u]['es_w']) <= max_len)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length filtered utterances = {0:d}\".format(len(fsh_filt_utts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_words(m_dict, v_dict, \n",
    "              fsh_filt_pred, \n",
    "              fsh_filt_utts, dec_key, \n",
    "              key, \n",
    "              play_audio=True, \n",
    "              displayN=displayN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_ref = []\n",
    "en_ref = []\n",
    "for u in fsh_filt_utts:\n",
    "    es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "    if type(m_dict[u][dec_key]) == list:\n",
    "        en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "    else:\n",
    "        en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "\n",
    "en_pred = []\n",
    "join_str = ' ' if dec_key.endswith('_w') else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"/afs/inf.ed.ac.uk/group/project/lowres/work/speech2text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, chrf, h, r = calc_bleu(m_dict, \n",
    "                          v_dict, \n",
    "                          fsh_filt_pred, \n",
    "                          fsh_filt_utts, \n",
    "                          dec_key, \n",
    "                          ref_index=ref_index)\n",
    "\n",
    "print(\"BLEU score on: {0:s} = {1:.2f}\".format(key, b * 100))\n",
    "print(\"-\"*60)\n",
    "\n",
    "model_refs = {u: mr for u, mr in zip(fsh_filt_utts, r)}\n",
    "model_hyps = {u: mh for u, mh in zip(fsh_filt_utts, h)}\n",
    "\n",
    "all_weights=[(1.,0.,0.,0.),\n",
    "             (0.,1.,0.,0.),\n",
    "             (0.,0.,1.,0.),\n",
    "             (0.,0.,0.,1.),\n",
    "             (1./2,1./2,0.,0.),\n",
    "             (1./3,1./3,1./3,0.),\n",
    "             (.25,.25,.25,.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ref, en_hyp = write_predictions_to_file(m_dict, v_dict, fsh_filt_pred, fsh_filt_utts, \n",
    "                                           dec_key, key, stemmify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, _ = corpus_precision_recall(r, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prec_recall = {'precision' : {}, 'recall' : {}, \"tp\": 0, \"tc\": 0, \"tr\": 0}\n",
    "\n",
    "for utt_id, ref, hyp in zip(fsh_filt_utts, r, h):\n",
    "    es_ref = [w.decode() for w in m_dict[utt_id]['es_w']]\n",
    "    \n",
    "    pval, rval = modified_precision_recall(ref, hyp, n=1)\n",
    "    model_prec_recall['tc'] += pval.numerator\n",
    "    model_prec_recall['tp'] += pval.denominator\n",
    "    model_prec_recall['tr'] += rval.denominator\n",
    "\n",
    "    model_prec_recall['precision'][utt_id] = float(pval)\n",
    "    model_prec_recall['recall'][utt_id] = float(rval)\n",
    "# end for\n",
    "    \n",
    "model_prec_recall['total_precision'] = model_prec_recall['tc'] / model_prec_recall['tp']\n",
    "model_prec_recall['total_recall'] = model_prec_recall['tc'] / model_prec_recall['tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\t\\tModel metrics\")\n",
    "print(\"-\"*60)\n",
    "print(\"{0:10s} = {1:0.3f}\\n{2:10s} = {3:0.3f}\".format(\"precision\",\n",
    "                                                      model_prec_recall['total_precision'],\n",
    "                                                      \"recall\",\n",
    "                                                      model_prec_recall['total_recall']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model-s2t BLEU score:\")\n",
    "\"{0:0.3f}\".format(100.0 * corpus_bleu(model_refs.values(), model_hyps.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "_, _ = corpus_precision_recall(model_refs.values(), model_hyps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"\\t\\tMODEL\")\n",
    "print(\"-\"*60)\n",
    "metrics = {\"p\": {1: [], 2: [], 3: [], 4: []},\n",
    "           \"r\": {1: [], 2: [], 3: [], 4: []}}\n",
    "\n",
    "for ix in range(len(list(model_refs.values())[0])):\n",
    "    temp_refs = [[i[ix]] for i in model_refs.values()]\n",
    "    print(\"-\"*60)\n",
    "    print(\"\\t\\tUsing reference = {0:d}\".format(ix+1))\n",
    "    print(\"-\"*60)\n",
    "    ps, rs = corpus_precision_recall(temp_refs, model_hyps.values())\n",
    "    for i, (p, r) in enumerate(zip(ps, rs)):\n",
    "        metrics['p'][i+1].append(p)\n",
    "        metrics['r'][i+1].append(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(vals_dict):\n",
    "#     print(\"estimated mean\")\n",
    "    k = []\n",
    "    u = []\n",
    "    s = []\n",
    "    for m, vals in vals_dict.items():\n",
    "        k.append(m)\n",
    "        u.append(np.mean(vals))\n",
    "        s.append(np.std(vals)/np.sqrt(len(vals)))\n",
    "        print(\"{0:10d}-gram = {1:.2f} Â± {2:.2f}\".format(m, np.mean(vals), np.std(vals)/np.sqrt(len(vals))))\n",
    "        \n",
    "    print(\",\".join(map(lambda x : \"{0:.2f}\".format(x), u)))\n",
    "    print(\",\".join(map(lambda x : \"{0:.2f}\".format(x), s)))\n",
    "    return k, u, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2w_p_r = {}\n",
    "\n",
    "print(\"Precision:\")\n",
    "_, s2w_p_r['p'], s2w_p_r['p_std'] = get_mean_std(metrics['p'])\n",
    "\n",
    "print(\"Recall:\")\n",
    "_, s2w_p_r['r'], s2w_p_r['r_std'] = get_mean_std(metrics['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(m_cfg['model_dir'], \"s2w_p_r.json\"), \"w\") as out_f:\n",
    "    json.dump(s2w_p_r, out_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = {}\n",
    "for u, hyp in zip(fsh_filt_utts, h):\n",
    "    model_predictions[u] = hyp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "min_word_len = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(m_dict):\n",
    "    words = []\n",
    "    for u in m_dict:\n",
    "        if type(m_dict[u]['en_w']) == list:\n",
    "            words.extend([w.decode() for w in m_dict[u]['en_w']])\n",
    "        else:\n",
    "            for ref in m_dict[u]['en_w']:\n",
    "                words.extend([w.decode() for w in ref])\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in train\n",
    "train_words = get_words(map_dict['fisher_train'])\n",
    "train_words_top_k = [(w,f) for w, f in sorted(train_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "train_only_words = set(train_words.keys())\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word types\", len(train_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# train word tokens\", sum(train_words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(w,f) for w,f in train_words_top_k if \"'\" in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words = get_words(map_dict['fisher_dev'])\n",
    "dev_words_top_k = [(w,f) for w, f in sorted(dev_words.items(), reverse=True, key=lambda t:t[1]) \n",
    "                     if w not in stop_words and len(w) >= min_word_len][:top_k]\n",
    "\n",
    "dev_only_words = set(dev_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words_top_k[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_words = {w:f for w,f in dev_words.items() if w not in train_only_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word types\", len(dev_only_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# dev word tokens\", sum(dev_words.values())))\n",
    "\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word types\", len(oov_words)))\n",
    "print(\"{0:20s} | {1:10d}\".format(\"# oov word tokens\", sum(oov_words.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{0:.1f}%\".format(sum(oov_words.values()) / sum(dev_words.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at utterances which our model is doing better at, and compare to Google.\n",
    "\n",
    "Are we doing better on a few calls only? Is there any particular speaker or call messing up our results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Query task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crises_lex_fname = \"../criseslex/CrisisLexLexicon/CrisisLexRec.txt\"\n",
    "train_text_fname= \"../installs/fisher-callhome-corpus/corpus/ldc/fisher_train.en\"\n",
    "topics_fname = \"../criseslex/fsp06_topics_in_english.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [ \"peace\", \"Music\", \"Marriage\", \"Religion\", \"Cell phones\", \n",
    "           \"Dating\", \"Telemarketing and SPAM\", \"Politics\", \"Travel\", \n",
    "           \"Technical devices\", \"Healthcare\", \"Advertisements\", \"Power\", \n",
    "           \"Occupations\", \"Movies\", \"Welfare\", \"Breaking up\", \"Location\", \n",
    "           \"Justice\", \"Memories\", \"Crime\", \"Violence against women\", \"Equality\", \n",
    "            \"Housing\", \"Immigration\",     \n",
    "            # new topics\n",
    "           \"Interracial\", \"Christians\", \"muslims\", \"jews\", \"e-mail\", \n",
    "           \"phone\", \"democracy\", \"Democratic\", \"Republican\", \"technology\", \n",
    "           \"leadership\", \"community\", \"jury\", \"police\", \"inequality\", \n",
    "           \"renting\", \"Violence\", \"immigrants\", \"immigrant\", \"skilled\", \n",
    "           \"Telemarketing\", \"SPAM\", \"skill\", \"job\", \"health\", \"mobile\", \n",
    "            \"ads\", \"physical\", \"emotional\", \"bubble\", \"rent\", \"economy\", \n",
    "            \"abuse\", \"women\", \"city\", \"country\", \"suburban\", \"dollar\", \n",
    "            \"united states\", \"laws\", \"phone\", \"race\", \"biracial\", \"interracial\", \n",
    "            \"marriage\", \"lyrics\", \"sexuality\", \"medicine\", \"television\", \"european\",\n",
    "            \"home\", \"protect\", \"spouse\", \"language\", \"cellphone\", \"money\",\n",
    "            \"doctor\", \"insurance\", \"cigarettes\", \"alcohol\", \"income\", \"salary\",\n",
    "            \"class\", \"censor\", \"rating\", \"programs\", \"shows\", \"government\",\n",
    "            \"relationship\", \"legal\", \"event\", \"life\", \"safe\", \"victim\", \"cops\",\n",
    "            \"wage\", \"illegal\"\n",
    "            ]\n",
    "topics = list(set(t.lower() for t in topics))\n",
    "topics_stem = [stem(t) for t in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words_stem = {}\n",
    "for w in dev_words:\n",
    "    stem_w = stem(w)\n",
    "    if stem_w not in dev_words_stem:\n",
    "        dev_words_stem[stem_w] = 0\n",
    "    dev_words_stem[stem_w] += dev_words[w]\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this checks for # of occurrences of the word in the specified text\n",
    "# We later use the # of utterrances, which will be lower or equal to this number\n",
    "prune_topics = [t for t in topics if dev_words_stem.get(stem(t),0) > 5 and dev_words_stem.get(stem(t),0) < 100]\n",
    "len(prune_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"selec_query_terms\")\n",
    "sel_topics = random.sample(prune_topics, min(len(prune_topics), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sel_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_terms_references(utt_text, terms):\n",
    "    terms_set = set([stem(i) for i in terms])\n",
    "    terms_search_res = {}\n",
    "    full_words = {}\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        l = (utt_text[u]['en_w'] if type(utt_text[u]) == dict \n",
    "             else utt_text[u])\n",
    "        for r in l:\n",
    "            for w in set(r):\n",
    "                decoded_w = w.decode() if type(w) != str else w\n",
    "                w_to_search = stem(decoded_w)\n",
    "                if w_to_search in terms_set:\n",
    "                    if w_to_search not in terms_search_res:\n",
    "                        terms_search_res[w_to_search] = set()\n",
    "                        full_words[w_to_search] = set()\n",
    "                    terms_search_res[w_to_search].add(u)\n",
    "                    full_words[w_to_search].add(decoded_w)\n",
    "                # end if found\n",
    "            # end for current reference\n",
    "        # end for all references\n",
    "    # end for all utterances\n",
    "    return terms_search_res, full_words\n",
    "\n",
    "\n",
    "def find_all_terms_predictions(utt_text, terms):\n",
    "    terms_set = set([stem(i) for i in terms])\n",
    "    terms_search_res = {}\n",
    "    full_words = {}\n",
    "\n",
    "    for u in tqdm(utt_text, ncols=80):\n",
    "        r = utt_text[u]\n",
    "        for w in set(r):\n",
    "            w_to_search = stem(w)\n",
    "            if w_to_search in terms_set:\n",
    "                if w_to_search not in terms_search_res:\n",
    "                    terms_search_res[w_to_search] = set()\n",
    "                    full_words[w_to_search] = set()\n",
    "                terms_search_res[w_to_search].add(u)\n",
    "                full_words[w_to_search].add(u)\n",
    "                # end if found\n",
    "            # end for current reference\n",
    "    # end for all utterances\n",
    "    return terms_search_res, full_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms_prec_recall(preds, refs):\n",
    "    prec_recall = {}\n",
    "    prec_recall = {'t':0, 'tp':0, 'tc':0, 'terms':{}}\n",
    "    for term in refs.keys():\n",
    "        prec_recall['terms'][term] = {}\n",
    "        prec_recall['terms'][term]['t'] = len(refs[term])\n",
    "        preds_occ = preds.get(term, set())\n",
    "        prec_recall['terms'][term]['tp'] = len(preds_occ)\n",
    "        prec_recall['terms'][term]['tc'] = len(refs[term] & preds_occ)\n",
    "        prec_recall['t'] += prec_recall['terms'][term]['t']\n",
    "        prec_recall['tp'] += prec_recall['terms'][term]['tp']\n",
    "        prec_recall['tc'] += prec_recall['terms'][term]['tc']\n",
    "    # end for each term\n",
    "    return prec_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_topics, ref_topic_labels = find_all_terms_references(map_dict['fisher_dev'], sel_topics)\n",
    "pred_topics, pred_topics_topics_labels = find_all_terms_predictions(model_predictions, sel_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_topics), len(pred_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_topics_in_utts = {k:v for k, v in ref_topics.items() if len(v) >=3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prune_topics_in_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "fig, ax = plt.subplots(figsize=(6,10),nrows=1, ncols=1)\n",
    "\n",
    "\n",
    "sorted_counts = [t[0] for t in sorted(ref_topics.items(), reverse=True, key=lambda t:len(t[1]))]\n",
    "topic_count = [len(ref_topics[t]) for t in sorted_counts]\n",
    "\n",
    "topic_labels = [\"-\".join(list(ref_topic_labels[t])[:3]) for t in sorted_counts]\n",
    "\n",
    "ax = sns.barplot(x=topic_count, \n",
    "                 y=topic_labels, \n",
    "                 color=tableau20[6], alpha=.7)\n",
    "                 #**{\"label\":\"topic counts\", \"alpha\":0.5}, ax=ax)\n",
    "\n",
    "# ax.set_xlabel(\"# of utts in which topic occurs\", size=20)\n",
    "\n",
    "max_count_val = max([len(v) for v in ref_topics.values()])\n",
    "min_count_val = max([len(v) for v in ref_topics.values()])\n",
    "plt.xticks(list(range(0,max_count_val, 5))+[min_count_val, max_count_val])\n",
    "\n",
    "# ax.legend(loc='upper right', bbox_to_anchor=(1.1, 0.9),\n",
    "#                       ncol=1, fancybox=True, shadow=True, fontsize=20)\n",
    "\n",
    "sns.despine(left=True, bottom=True, top=True, right=False)\n",
    "\n",
    "for t in ax.get_yticklabels():\n",
    "    t.set_fontsize(15) \n",
    "    \n",
    "for i, t in enumerate(ax.get_xticklabels()):\n",
    "    if i > 0 and i < (len(ax.get_xticklabels())-1):\n",
    "        t.set_visible(False)\n",
    "    else:\n",
    "        t.set_fontsize(16)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "fig.savefig(\"../criseslex/sel_topics.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics_p_r = terms_prec_recall(preds = pred_topics, refs = ref_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics_p_r['precision'] = model_topics_p_r['tc'] / model_topics_p_r['tp']\n",
    "model_topics_p_r['recall'] = model_topics_p_r['tc'] / model_topics_p_r['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*40)\n",
    "print(\"-------------- Query by Text\")\n",
    "print(\"-\"*40)\n",
    "print(\"{0:20s} | {1:5.2f}%\".format(\"precision\", model_topics_p_r['precision']*100.0))\n",
    "print(\"{0:20s} | {1:5.2f}%\".format(\"recall\", model_topics_p_r['recall']*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(m_cfg['model_dir'], \"topics_p_r.json\"), \"w\") as out_f:\n",
    "    json.dump(model_topics_p_r, out_f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
