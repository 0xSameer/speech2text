{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, modified_precision\n",
    "from nltk.translate.chrf_score import sentence_chrf, corpus_chrf\n",
    "from nltk.metrics import scores\n",
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "from nltk.stem import *\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from stemming.porter2 import stem\n",
    "import stemming\n",
    "from nltk.metrics.scores import recall\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_words(m_dict, v_dict, preds, utts, dec_key, key, play_audio=False, displayN=-1):\n",
    "    if displayN == -1:\n",
    "        displayN = len(utts)\n",
    "    es_ref = []\n",
    "    en_ref = []\n",
    "    google_ref = []\n",
    "    google_pred = []\n",
    "    for u in utts:\n",
    "        es_ref.append(\" \".join([w.decode() for w in m_dict[u]['es_w']]))\n",
    "        if type(m_dict[u][dec_key]) == list:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w']]))\n",
    "        else:\n",
    "            en_ref.append(\" \".join([w.decode() for w in m_dict[u]['en_w'][0]]))\n",
    "        google_pred.append(\" \".join(google_hyp_r0[u]))\n",
    "        google_ref.append(\" \".join(google_dev_ref_0[u]))\n",
    "\n",
    "    en_pred = []\n",
    "    join_str = ' ' if dec_key.endswith('_w') else ''\n",
    "\n",
    "    for p in preds:\n",
    "        if type(p) == list:\n",
    "            t_str = join_str.join([v_dict['i2w'][i].decode() for i in p])\n",
    "            t_str = t_str[:t_str.find('_EOS')]\n",
    "            en_pred.append(t_str)\n",
    "        else:\n",
    "            en_pred.append(\"\")\n",
    "        \n",
    "\n",
    "    for u, es, en, p, g, gr in sorted(list(zip(utts, es_ref, en_ref, en_pred, google_pred, google_ref)))[:displayN]:\n",
    "        # for reference, 1st word is GO_ID, no need to display\n",
    "        print(\"Utterance: {0:s}\".format(u))\n",
    "        display_pp = PrettyTable([\"cat\",\"sent\"], hrules=True)\n",
    "        display_pp.align = \"l\"\n",
    "        display_pp.header = False\n",
    "        display_pp.add_row([\"es ref\", textwrap.fill(es,50)])\n",
    "        display_pp.add_row([\"en ref\", textwrap.fill(en,50)])\n",
    "        display_pp.add_row([\"model pred\", textwrap.fill(p,50)])\n",
    "        display_pp.add_row([\"model bleu\", \"{0:.2f}\".format(sentence_bleu([en], p, smoothing_function=smooth_fun.method2))])\n",
    "        display_pp.add_row([\"google pred\", textwrap.fill(g,50)])\n",
    "        display_pp.add_row([\"google bleu\", \"{0:.2f}\".format(sentence_bleu([gr], g, smoothing_function=smooth_fun.method2))])\n",
    "    \n",
    "\n",
    "        print(display_pp)\n",
    "        if play_audio:\n",
    "            play_utt(u, m_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(utt, X, y=None, display_limit=10):\n",
    "    # get shape\n",
    "    batch_size = X.shape[0]\n",
    "    # encode input\n",
    "    model.forward_enc(X)\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # initialize decoder LSTM to final encoder state\n",
    "    # ---------------------------------------------------------------------\n",
    "    model.set_decoder_state()\n",
    "    # ---------------------------------------------------------------------\n",
    "    # swap axes of the decoder batch\n",
    "    if y is not None:\n",
    "        y = F.swapaxes(y, 0, 1)\n",
    "    # -----------------------------------------------------------------\n",
    "    # predict\n",
    "    # -----------------------------------------------------------------\n",
    "    # make return statements consistent\n",
    "    return(decode_display(utt, batch_size=batch_size,\n",
    "                          pred_limit=model.m_cfg['max_en_pred'],\n",
    "#                           pred_limit=20,\n",
    "                          y=y, display_limit=display_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_display(utt, batch_size, pred_limit, y=None, display_limit=10):\n",
    "    xp = cuda.cupy if model.gpuid >= 0 else np\n",
    "    # max number of predictions to make\n",
    "    # if labels are provided, this variable is not used\n",
    "    stop_limit = pred_limit\n",
    "    # to track number of predictions made\n",
    "    npred = 0\n",
    "    # to store loss\n",
    "    loss = 0\n",
    "    # if labels are provided, use them for computing loss\n",
    "    compute_loss = True if y is not None else False\n",
    "    # ---------------------------------------------------------------------\n",
    "    if compute_loss:\n",
    "        stop_limit = len(y)-1\n",
    "        # get starting word to initialize decoder\n",
    "        curr_word = y[0]\n",
    "    else:\n",
    "        # intialize starting word to GO_ID symbol\n",
    "        curr_word = Variable(xp.full((batch_size,), GO_ID, dtype=xp.int32))\n",
    "    # ---------------------------------------------------------------------\n",
    "    # flag to track if all sentences in batch have predicted EOS\n",
    "    # ---------------------------------------------------------------------\n",
    "    with cupy.cuda.Device(model.gpuid):\n",
    "        check_if_all_eos = xp.full((batch_size,), False, dtype=xp.bool_)\n",
    "    # ---------------------------------------------------------------------\n",
    "    a_units = m_cfg['attn_units']\n",
    "    ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "    # ---------------------------------------------------------------------\n",
    "    prob_out = {}\n",
    "    prob_print_str = []\n",
    "    while npred < (stop_limit):\n",
    "        # -----------------------------------------------------------------\n",
    "        # decode and predict\n",
    "        #print(\"decoding with word: {0:s}\".format(vocab_dict['en_w']['i2w'][curr_word.data[0].tolist()].decode()))\n",
    "        pred_out, ht = model.decode(curr_word, ht)\n",
    "        pred_word = F.argmax(pred_out, axis=1)\n",
    "        # -----------------------------------------------------------------\n",
    "        # printing conditional probabilities\n",
    "        # -----------------------------------------------------------------\n",
    "        pred_probs = xp.asnumpy(F.log_softmax(pred_out).data[0])\n",
    "        top_n_probs = xp.argsort(pred_probs)[-display_limit:]\n",
    "        #print(\"-\"*60)\n",
    "        #print(\"predicting word : {0:d}\".format(npred))\n",
    "        prob_print_str.append(\"-\" * 60)\n",
    "        prob_print_str.append(\"predicting word : {0:d}\".format(npred))\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "#         if npred == 0:\n",
    "#             sample_word = np.random.choice(range(len(pred_probs)), p=pred_probs)\n",
    "#             sample_word = np.argsort(pred_probs)[-2]\n",
    "#             print(np.argsort(pred_probs)[-2], np.argsort(pred_probs)[-1])\n",
    "#             pred_word = Variable(xp.asarray([sample_word], dtype=xp.int32))\n",
    "        # -----------------------------------------------------------------\n",
    "        \n",
    "        prob_out[npred] = {}\n",
    "        for pi in top_n_probs[::-1]:\n",
    "            prob_out[npred][v_dict['i2w'][pi].decode()] = \"{0:.3f}\".format(pred_probs[pi])\n",
    "            #print(\"{0:10s} = {1:5.3f}\".format(v_dict['i2w'][pi].decode(), pred_probs[pi]))\n",
    "            prob_print_str.append(\"{0:10s} = {1:5.3f}\".format(v_dict['i2w'][pi].decode(), pred_probs[pi]))\n",
    "            \n",
    "        # -----------------------------------------------------------------\n",
    "        # save prediction at this time step\n",
    "        # -----------------------------------------------------------------\n",
    "        if npred == 0:\n",
    "            pred_sents = pred_word.data\n",
    "        else:\n",
    "            pred_sents = xp.vstack((pred_sents, pred_word.data))\n",
    "        # -----------------------------------------------------------------\n",
    "        if compute_loss:\n",
    "            # compute loss\n",
    "            loss += F.softmax_cross_entropy(pred_out, y[npred+1],\n",
    "                                               class_weight=model.mask_pad_id)\n",
    "        # -----------------------------------------------------------------\n",
    "        curr_word = pred_word\n",
    "        # -----------------------------------------------------------------\n",
    "        # check if EOS is predicted for all sentences\n",
    "        # -----------------------------------------------------------------\n",
    "        check_if_all_eos[pred_word.data == EOS_ID] = True\n",
    "        if xp.all(check_if_all_eos):\n",
    "            break\n",
    "        # -----------------------------------------------------------------\n",
    "        # increment number of predictions made\n",
    "        npred += 1\n",
    "        # -----------------------------------------------------------------\n",
    "    \n",
    "#     out_fname = os.path.join(m_cfg['model_dir'], \"probs\", \"{0:s}_probs.json\".format(utt))\n",
    "#     with open(out_fname, \"w\") as out_f:\n",
    "#         json.dump(prob_out, out_f, indent=4)\n",
    "#     print(\"saved probs in : {0:s}\".format(out_fname))\n",
    "    return pred_sents.T, loss, \"\\n\".join(prob_print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utt_data(eg_utt, curr_set):\n",
    "    # get shape\n",
    "    local_input_path = os.path.join(m_cfg['data_path'], curr_set)\n",
    "        \n",
    "    width_b = bucket_dict[dev_key][\"width_b\"]\n",
    "    num_b = bucket_dict[dev_key][\"num_b\"]\n",
    "    utt_list = [eg_utt]\n",
    "    \n",
    "    batch_data = get_batch(map_dict[curr_set], \n",
    "                           enc_key,\n",
    "                           dec_key,\n",
    "                           utt_list,\n",
    "                           vocab_dict,\n",
    "                           num_b * width_b,\n",
    "                           200,\n",
    "                           input_path=local_input_path)\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"interspeech/sp_80hrs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_out_dim = rnn_in_units =  1280\n",
      "using ADAM optimizer\n",
      "--------------------------------------------------------------------------------\n",
      "model found = \n",
      "interspeech/sp_80hrs/seq2seq_83.model\n",
      "finished loading ..\n",
      "optimizer found = interspeech/sp_80hrs/train.opt\n",
      "finished loading optimizer ...\n"
     ]
    }
   ],
   "source": [
    "last_epoch, model, optimizer, m_cfg, t_cfg = check_model(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_key = m_cfg['train_set']\n",
    "dev_key = m_cfg['dev_set']\n",
    "batch_size=t_cfg['batch_size']\n",
    "enc_key=m_cfg['enc_key']\n",
    "dec_key=m_cfg['dec_key']\n",
    "input_path = os.path.join(m_cfg['data_path'], m_cfg['dev_set'])\n",
    "# -------------------------------------------------------------------------\n",
    "# get data dictionaries\n",
    "# -------------------------------------------------------------------------\n",
    "map_dict, vocab_dict, bucket_dict = get_data_dicts(m_cfg)\n",
    "batch_size = {'max': 1, 'med': 1, 'min': 1, 'scale': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"meh\")\n",
    "# random.seed(\"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval parameters\n",
    "ref_index = -1\n",
    "min_len, max_len= 0, m_cfg['max_en_pred']\n",
    "# min_len, max_len = 0, 10\n",
    "displayN = 50\n",
    "m_dict=map_dict[dev_key]\n",
    "# wavs_path = os.path.join(m_cfg['data_path'], \"wavs\")\n",
    "wavs_path = os.path.join(\"../chainer2/speech2text/both_fbank_out/\", \"wavs\")\n",
    "v_dict = vocab_dict['en_w']\n",
    "key = m_cfg['dev_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"/afs/inf.ed.ac.uk/group/project/lowres/work/speech2text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L0_dec', 'L1_dec', 'L2_dec']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rnn_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_states():\n",
    "    rnn_states = {\"c\": [], \"h\": []}\n",
    "    # ---------------------------------------------------------------------\n",
    "    # get the hidden and cell state (LSTM) of the first RNN in the decoder\n",
    "    # ---------------------------------------------------------------------\n",
    "    if model.m_cfg['bi_rnn']:\n",
    "        for i, (enc, rev_enc) in enumerate(zip(model.rnn_enc,\n",
    "                                     model.rnn_rev_enc)):\n",
    "            h_state = F.concat((model[enc].h, model[rev_enc].h))\n",
    "            rnn_states[\"h\"].append(h_state)\n",
    "            if model.m_cfg['rnn_unit'] == RNN_LSTM:\n",
    "                c_state = F.concat((model[enc].c, model[rev_enc].c))\n",
    "                rnn_states[\"c\"].append(c_state)\n",
    "    else:\n",
    "        for enc, dec in zip(model.rnn_enc, model.rnn_dec):\n",
    "            rnn_states[\"h\"].append(model[enc].h)\n",
    "            if model.m_cfg['rnn_unit'] == RNN_LSTM:\n",
    "                rnn_states[\"c\"].append(model[enc].c)\n",
    "            # end if\n",
    "        # end for all layers\n",
    "    # end if bi-rnn\n",
    "    return rnn_states\n",
    "    # ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_states():\n",
    "    rnn_states = {\"c\": [], \"h\": []}\n",
    "    # ---------------------------------------------------------------------\n",
    "    # get the hidden and cell state (LSTM) of the first RNN in the decoder\n",
    "    # ---------------------------------------------------------------------\n",
    "    for i, dec in enumerate(model.rnn_dec):\n",
    "        rnn_states[\"h\"].append(model[dec].h)\n",
    "        if model.m_cfg['rnn_unit'] == RNN_LSTM:\n",
    "            rnn_states[\"c\"].append(model[dec].c)\n",
    "        # end if\n",
    "    # end for all layers\n",
    "    return rnn_states\n",
    "    # ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_decoder_states(rnn_states):\n",
    "    # ---------------------------------------------------------------------\n",
    "    # set the hidden and cell state (LSTM) for the decoder\n",
    "    # ---------------------------------------------------------------------\n",
    "    for i, dec in enumerate(model.rnn_dec):\n",
    "        if model.m_cfg['rnn_unit'] == RNN_LSTM:\n",
    "            model[dec].set_state(rnn_states[\"c\"][i], rnn_states[\"h\"][i])\n",
    "        else:\n",
    "            model[dec].set_state(rnn_states[\"h\"][i])\n",
    "        # end if\n",
    "    # end for all layers\n",
    "    # ---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_utt_data(X):\n",
    "    # get shape\n",
    "    batch_size = X.shape[0]\n",
    "    # encode input\n",
    "    model.forward_enc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hyp():\n",
    "    beam_entry = {\"hyp\": [GO_ID], \"score\": 0}\n",
    "    beam_entry[\"dec_state\"] = get_encoder_states()\n",
    "    a_units = m_cfg['attn_units']\n",
    "    ht = Variable(xp.zeros((1, a_units), dtype=xp.float32))\n",
    "    beam_entry[\"attn_v\"] = ht\n",
    "    return beam_entry\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_decoder_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.L0_dec.c[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_state[\"c\"][0][0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 393, 80)\n",
      "[ 0.07863344  0.32638404 -0.40837365 -0.2300286  -0.2424436 ]\n",
      "[-0.47259966 -0.47259966  7.5808063  -0.47259963  2.0798721 ]\n",
      "[ 0.03225665 -0.02414448  0.03373034  0.09144363 -0.13371721]\n",
      "variable([ 0.07863344  0.32638404 -0.40837365 -0.2300286  -0.2424436 ])\n",
      "[-7.9359665  -3.6814308  -0.02831078]\n",
      "['it', 'and', 'i']\n"
     ]
    }
   ],
   "source": [
    "with chainer.using_config('train', False):\n",
    "    batch_data = get_utt_data(eg_utt, \"fisher_dev\")\n",
    "    model.forward_enc(batch_data['X'])\n",
    "    print(batch_data['X'].shape)\n",
    "    print(model.L2_enc.h.data[0,:5])\n",
    "\n",
    "    decode_entry = init_hyp()\n",
    "    word_id, dec_state, attn_v = (decode_entry[\"hyp\"][-1], \n",
    "                                            decode_entry[\"dec_state\"], \n",
    "                                            decode_entry[\"attn_v\"])\n",
    "\n",
    "    # set decoder state\n",
    "    set_decoder_states(dec_state)\n",
    "#     model.set_decoder_state()\n",
    "\n",
    "    curr_word = Variable(xp.full((1,), word_id, dtype=xp.int32))\n",
    "#     a_units = m_cfg['attn_units']\n",
    "#     ht = Variable(xp.zeros((1, a_units), dtype=xp.float32))\n",
    "    embed_id = model.embed_dec(curr_word)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # apply rnn - input feeding, use previous ht\n",
    "    # ---------------------------------------------------------------------\n",
    "    rnn_in = F.concat((embed_id, attn_v), axis=1)\n",
    "    h = model.feed_rnn(rnn_in, model.rnn_dec)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # compute context vector\n",
    "    # ---------------------------------------------------------------------\n",
    "    cv, _ = model.compute_context_vector(h)\n",
    "    cv_hdec = F.concat((cv, h), axis=1)\n",
    "    # ---------------------------------------------------------------------\n",
    "    # compute attentional hidden state\n",
    "    # ---------------------------------------------------------------------\n",
    "    ht = F.tanh(model.context(cv_hdec))\n",
    "    # ---------------------------------------------------------------------\n",
    "    # make prediction\n",
    "    # ---------------------------------------------------------------------\n",
    "    predicted_out = model.out(ht)\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(predicted_out.data[0,:5])\n",
    "    print(h.data[0,:5])\n",
    "    print(model.L2_enc.h[0,:5])\n",
    "    pred_probs = xp.asnumpy(F.log_softmax(predicted_out).data[0])\n",
    "    top_n_probs = xp.argsort(pred_probs)[-3:]\n",
    "    print(pred_probs[top_n_probs])\n",
    "    print([v_dict['i2w'][pi].decode() for pi in top_n_probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_beam_step(decode_entry, beam_width=3):\n",
    "    xp = cuda.cupy if model.gpuid >= 0 else np\n",
    "    \n",
    "    with chainer.using_config('train', False):\n",
    "    \n",
    "        word_id, dec_state, attn_v = (decode_entry[\"hyp\"][-1], \n",
    "                                        decode_entry[\"dec_state\"], \n",
    "                                        decode_entry[\"attn_v\"])\n",
    "\n",
    "        # set decoder state\n",
    "        set_decoder_states(dec_state)\n",
    "        #model.set_decoder_state()\n",
    "\n",
    "        # intialize starting word symbol\n",
    "        #print(\"beam step curr word\", v_dict['i2w'][word_id].decode())\n",
    "        curr_word = Variable(xp.full((1,), word_id, dtype=xp.int32))\n",
    "\n",
    "        prob_out = {}\n",
    "        prob_print_str = []\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # decode and predict\n",
    "        pred_out, ht = model.decode(curr_word, attn_v)    \n",
    "        # -----------------------------------------------------------------\n",
    "        # printing conditional probabilities\n",
    "        # -----------------------------------------------------------------\n",
    "        pred_probs = xp.asnumpy(F.log_softmax(pred_out).data[0])\n",
    "        top_n_probs = xp.argsort(pred_probs)[-beam_width:]\n",
    "        \n",
    "        new_entries = []\n",
    "        \n",
    "        curr_dec_state = get_decoder_states()\n",
    "\n",
    "        for pi in top_n_probs[::-1]:\n",
    "            #print(\"{0:10s} = {1:5.4f}\".format(v_dict['i2w'][pi].decode(), pred_probs[pi]))\n",
    "            new_entry = {}\n",
    "            new_entry[\"hyp\"] = decode_entry[\"hyp\"] + [pi]\n",
    "            #print(new_entry[\"hyp\"])\n",
    "            new_entry[\"score\"] = decode_entry[\"score\"] + pred_probs[pi]\n",
    "            new_entry[\"dec_state\"] = curr_dec_state\n",
    "            new_entry[\"attn_v\"] = ht\n",
    "            \n",
    "            new_entries.append(new_entry)\n",
    "            \n",
    "    # end with chainer test mode\n",
    "    return new_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_beam(utt, curr_set, stop_limit=10, max_n=5, beam_width=3):\n",
    "    with chainer.using_config('train', False):\n",
    "        batch_data = get_utt_data(utt, curr_set)\n",
    "        model.forward_enc(batch_data['X'])\n",
    "\n",
    "        n_best = []\n",
    "        n_best.append(init_hyp())\n",
    "\n",
    "        for i in range(stop_limit):\n",
    "            #print(\"-\"*40)\n",
    "            #print(i)\n",
    "            #print(\"-\"*40)\n",
    "            all_non_eos = [1 if e[\"hyp\"][-1] != EOS_ID else 0 for e in n_best]\n",
    "            if sum(all_non_eos) == 0:\n",
    "                print(\"all eos at step={0:d}\".format(i))\n",
    "                break\n",
    "\n",
    "            curr_entries = []\n",
    "            for e in n_best:\n",
    "                if e[\"hyp\"][-1] != EOS_ID:\n",
    "                    #print(\"feeding\", v_dict[\"i2w\"][e[\"hyp\"][-1]])\n",
    "                    curr_entries.extend(decode_beam_step(e, beam_width=beam_width))\n",
    "                else:\n",
    "                    curr_entries.append(e)\n",
    "\n",
    "            n_best = sorted(curr_entries, reverse=True, key=lambda t: t[\"score\"])[:max_n]\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 'm from puerto rico but i live here in denver colorado\n"
     ]
    }
   ],
   "source": [
    "eg_utt = '20051023_232057_325_fsp-A-3'\n",
    "print(\" \".join(map(bytes.decode, m_dict[eg_utt][\"en_w\"][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all eos at step=13\n"
     ]
    }
   ],
   "source": [
    "with chainer.using_config('train', False):\n",
    "    n_best = decode_beam(eg_utt, \"fisher_dev\", stop_limit=20, max_n=8, beam_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_GO i 'm from puerto rico but i live here in denver colorado _EOS\n",
      "-1.732635498046875\n",
      "_GO i 'm from puerto rico but i live here in denver _EOS\n",
      "-2.7813310623168945\n",
      "_GO i 'm from puerto rico but i live here in canada _EOS\n",
      "-2.857640266418457\n",
      "_GO i 'm from puerto rico but i live here in toronto _EOS\n",
      "-3.0372314453125\n",
      "_GO i 'm from puerto rico but i live here in toronto colorado _EOS\n",
      "-3.4406538009643555\n",
      "_GO i am from puerto rico but i live here in denver colorado _EOS\n",
      "-3.9291458129882812\n",
      "_GO i 'm from puerto rico but i live here in denver canada _EOS\n",
      "-3.9817543029785156\n",
      "_GO i 'm from puerto rico but live here in denver colorado _EOS\n",
      "-4.247453689575195\n"
     ]
    }
   ],
   "source": [
    "for e in n_best:\n",
    "    print(\" \".join([v_dict['i2w'][i].decode() for i in e[\"hyp\"]]))\n",
    "    print(e[\"score\"])\n",
    "#     print(e[\"hyp\"])\n",
    "#     print(\" \".join([w.decode() for i in e[\"hyp\"] for w in v_dict[\"i2w\"][i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "predicting word : 0\n",
      "i          = -0.028\n",
      "------------------------------------------------------------\n",
      "predicting word : 1\n",
      "'m         = -0.128\n",
      "------------------------------------------------------------\n",
      "predicting word : 2\n",
      "from       = -0.010\n",
      "------------------------------------------------------------\n",
      "predicting word : 3\n",
      "puerto     = -0.022\n",
      "------------------------------------------------------------\n",
      "predicting word : 4\n",
      "rico       = -0.003\n",
      "------------------------------------------------------------\n",
      "predicting word : 5\n",
      "but        = -0.063\n",
      "------------------------------------------------------------\n",
      "predicting word : 6\n",
      "i          = -0.098\n",
      "------------------------------------------------------------\n",
      "predicting word : 7\n",
      "live       = -0.009\n",
      "------------------------------------------------------------\n",
      "predicting word : 8\n",
      "here       = -0.085\n",
      "------------------------------------------------------------\n",
      "predicting word : 9\n",
      "in         = -0.064\n",
      "------------------------------------------------------------\n",
      "predicting word : 10\n",
      "denver     = -0.733\n",
      "------------------------------------------------------------\n",
      "predicting word : 11\n",
      "colorado   = -0.487\n",
      "------------------------------------------------------------\n",
      "predicting word : 12\n",
      "_EOS       = -0.002\n"
     ]
    }
   ],
   "source": [
    "with chainer.using_config('train', False):\n",
    "    batch_data = get_utt_data(eg_utt, curr_set=\"fisher_dev\")\n",
    "    _, _, ha = make_pred(eg_utt, batch_data['X'], y=None, display_limit=1)\n",
    "    print(ha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_utt_data(eg_utt, curr_set=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward_enc(batch_data['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_states = get_encoder_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_states[\"c\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_states[\"c\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_decoder_states(rnn_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.L0_enc.h[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.L0_dec.h[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data['X'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### beam data structure\n",
    "\n",
    "hidden states for decoder:\n",
    "\n",
    "-- get_rnn_states(layer_names): returns list with decoder hidden states\n",
    "\n",
    "-- set_rnn_states(layer_names, hidden_states)\n",
    "\n",
    "-- get_e\n",
    "\n",
    "decoder_layer_name: {\"c\": , \"h\" :}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translate_probs(eg_utt, curr_set=\"fisher_dev\", display_limit=5, display_probs=True):\n",
    "    \n",
    "    batch_data = get_batch(map_dict[curr_set], \n",
    "                           enc_key,\n",
    "                           dec_key,\n",
    "                           utt_list,\n",
    "                           vocab_dict,\n",
    "                           (eg_utt_bucket+1) * width_b,\n",
    "                           200,\n",
    "                           input_path=local_input_path)\n",
    "    \n",
    "    with chainer.using_config('train', False):\n",
    "        cuda.get_device(t_cfg['gpuid']).use()\n",
    "        preds, _, probs_str = make_pred(eg_utt, X=batch_data['X'], display_limit=display_limit)\n",
    "        #preds, _ = make_pred(eg_utt, X=batch_data['X'][:,-150:,:], display_limit=10)\n",
    "        loss_val = 0.0\n",
    "    \n",
    "    display_words(map_dict[curr_set], v_dict, \n",
    "                  preds.tolist(), \n",
    "                  utt_list, dec_key, \n",
    "                  key, \n",
    "                  play_audio=play_audio, \n",
    "                  displayN=displayN)\n",
    "    \n",
    "    if display_probs:\n",
    "        print(probs_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_utts_with_word(word, text_key=\"en_w\", set_key=\"fisher_dev\", show_max_found=10):\n",
    "    total_found = 0\n",
    "    out_str = []\n",
    "    for utt, entry in map_dict[set_key].items():\n",
    "        if \"train\" in set_key or text_key == \"es_w\":\n",
    "            words_in_utt = \" \".join([w.decode() for w in entry[text_key]])\n",
    "        else:\n",
    "            words_in_utt = \" \".join([w.decode() for w in entry[text_key][0]])\n",
    "        es_words_in_utt = \" \".join([w.decode() for w in entry[\"es_w\"]])        \n",
    "        #if \"puerto\" in words_in_utt:\n",
    "        if word in words_in_utt:\n",
    "            out_str.append(\"{0:s} | {1:s} | {2:s}\".format(utt, words_in_utt, es_words_in_utt))\n",
    "            total_found += 1\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"total instances found = {0:d}\".format(total_found))\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\n\".join(out_str[:show_max_found]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_utts_with_word(\"claro\", text_key=\"es_w\", set_key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eg_utt = \"20051010_212418_225_fsp-A-32\"\n",
    "generate_translate_probs(eg_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051023_232057_325_fsp-A-3\"\n",
    "generate_translate_probs(eg_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LEN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"aha\")\n",
    "sel_utts = random.sample([u for u in google_dev_ref_0.keys() if len(google_dev_ref_0[u]) > MIN_LEN], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_utts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sel_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, u in enumerate(sel_utts):\n",
    "    print(\"-\"*80)\n",
    "    print(\"{0:d}\".format(i))\n",
    "    print(\"-\"*80)\n",
    "    generate_translate_probs(u, curr_set='fisher_dev', display_limit=3, display_probs=True)\n",
    "    loss_v, loss_by_w = check_loss(u, curr_set='fisher_dev')\n",
    "    #print(\"{0:20s} ||| {1:5.2f} ||| {2:5.2f} ||| {3:5.2f}\".format(u, l, loss_v, loss_by_w))\n",
    "    print(\"{0:20s} ||| {1:5.2f}\".format(u, loss_by_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, u in enumerate(sel_utts):\n",
    "    print(\"-\"*80)\n",
    "    print(\"{0:d}\".format(i))\n",
    "    print(\"-\"*80)\n",
    "    generate_translate_probs(u, curr_set='fisher_dev', display_limit=3, display_probs=True)\n",
    "    loss_v, loss_by_w = check_loss(u, curr_set='fisher_dev')\n",
    "    #print(\"{0:20s} ||| {1:5.2f} ||| {2:5.2f} ||| {3:5.2f}\".format(u, l, loss_v, loss_by_w))\n",
    "    print(\"{0:20s} ||| {1:5.2f}\".format(u, loss_by_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_fname = os.path.join(m_cfg['model_dir'], \"{0:s}_probs.json\".format(eg_utt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eg_utt = \"20051017_234550_276_fsp-B-34\"\n",
    "print(check_loss(eg_utt, curr_set='fisher_dev'))\n",
    "generate_translate_probs(eg_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict['en_w']['i2w'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051026_180724_341_fsp-A-26\"\n",
    "generate_translate_probs(eg_utt)\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051017_234550_276_fsp-A-13\"\n",
    "generate_translate_probs(eg_utt)\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051018_210220_279_fsp-A-26\"\n",
    "generate_translate_probs(eg_utt)\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_utts_with_word(\"mhm\", set_key=\"fisher_dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051019_210146_289_fsp-A-54\"\n",
    "generate_translate_probs(eg_utt, curr_set='fisher_dev')\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_loss = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051017_220530_275_fsp-B-21.npy\"\n",
    "try:\n",
    "    check_loss(eg_utt, curr_set='fisher_dev')\n",
    "except:\n",
    "    print(\"{0:s} not found\".format(utt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "utt_loss = {}\n",
    "for utt in tqdm(map_dict['fisher_dev'], ncols=50):\n",
    "    if utt not in utt_loss:\n",
    "        try:\n",
    "            loss = check_loss(utt, curr_set='fisher_dev')\n",
    "            utt_loss[utt] = loss.data.tolist()\n",
    "        except:\n",
    "            print(\"{0:s} not found\".format(utt))\n",
    "    #     print(utt, \"{0:5.3f}\".format(loss.data.tolist()))\n",
    "#     i += 1\n",
    "#     if i > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize by length\n",
    "utt_loss_normalize = {}\n",
    "for utt in tqdm(utt_loss, ncols=50):\n",
    "    utt_loss_normalize[utt] = utt_loss[utt] / (len(map_dict['fisher_dev'][utt]['en_w'][0])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(utt_loss.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(utt_loss, open(os.path.join(cfg_path, \"dev_utts_loss.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(utt_loss_normalize, open(os.path.join(cfg_path, \"dev_utts_loss_normalized.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_loss = pickle.load(open(os.path.join(cfg_path, \"dev_utts_loss.dict\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_loss_normalize = pickle.load(open(os.path.join(cfg_path, \"dev_utts_loss_normalized.dict\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_utts = sorted(utt_loss_normalize.items(), reverse=True, key=lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BAD_UTTS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = '20051026_180724_341_fsp-A-26'\n",
    "generate_translate_probs(eg_utt, curr_set='fisher_dev')\n",
    "check_loss(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_utts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_utts[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*utt_loss_normalize.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 if i < 1 else 0 for i in y]), sum([1 if i > 5 else 0 for i in y]), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev utts - avg loss per word in utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8,5)\n",
    "\n",
    "ax = sns.distplot(y, kde=False, rug=False, ax=ax, color=tableau20[0]);\n",
    "ax.set_xlabel(\"dev utts - avg loss per word in utt\", size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (u, l) in enumerate(sorted(utt_loss_normalize.items(), reverse=True, key=lambda t: t[1])[:50]):\n",
    "    print(\"-\"*80)\n",
    "    print(\"{0:d}\".format(i))\n",
    "    print(\"-\"*80)\n",
    "    generate_translate_probs(u, curr_set='fisher_dev', display_limit=3, display_probs=True)\n",
    "    loss_v, loss_by_w = check_loss(u, curr_set='fisher_dev')\n",
    "    #print(\"{0:20s} ||| {1:5.2f} ||| {2:5.2f} ||| {3:5.2f}\".format(u, l, loss_v, loss_by_w))\n",
    "    print(\"{0:20s} ||| {3:5.2f}\".format(u, l, loss_v, loss_by_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To share:\n",
    "Several utterrance labels have typos, giving a misleading signal about the prediction quality\n",
    "\n",
    "\"20051017_180712_270_fsp-B-62\"\n",
    "dogs barking\n",
    "\n",
    "\"20051018_210220_279_fsp-A-71\"\n",
    "monopoly money -- monopoly occurs only 5 times in the train set, and never in the context of the game\n",
    "\n",
    "\"20051017_220530_275_fsp-B-61\"\n",
    "the decode probabilities show that maybe beam decoding (? or probably language model) will help catch up to Google. the Google model outputs Texas. We have Texas as the second most probable word as per the acoustic model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[ .759,  0.141,  .053]], dtype=np.float32)\n",
    "t = np.array([1]).astype('i')\n",
    "y = F.softmax_cross_entropy(x, t)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_utt_data(eg_utt, curr_set='fisher_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = batch_data['X'], batch_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[vocab_dict['en_w']['i2w'][i] for i in xp.asnumpy(y.data[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_dict['fisher_train']['buckets'][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_list = bucket_dict['fisher_train']['buckets'][0][:5]\n",
    "width_b = bucket_dict['fisher_train']['width_b']\n",
    "local_input_path = os.path.join(m_cfg['data_path'], m_cfg['train_set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = get_batch(map_dict[\"fisher_train\"], \n",
    "                       enc_key,\n",
    "                       dec_key,\n",
    "                       utt_list,\n",
    "                       vocab_dict,\n",
    "                       (0+1) * width_b,\n",
    "                       200,\n",
    "                       input_path=local_input_path)\n",
    "    \n",
    "X, y = batch_data['X'], batch_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input\n",
    "model.forward_enc(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = F.swapaxes(y, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros(shape=(len(next_word),10), dtype='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0,[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict = pickle.load(open(\"../speech2text/fbanks_80dim_nltk/sim.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['i'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for next_word in y_t:\n",
    "    print(next_word)\n",
    "    t = np.zeros(shape=(len(next_word.data), 17000), dtype='i')\n",
    "    print(next_word.data.tolist())\n",
    "    for i,w in enumerate(next_word.data.tolist()):\n",
    "        t[i,sim_dict['i'][w]] = 1\n",
    "        print(t[i,[4,1044, 1045, 2477]])\n",
    "    #print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = xp.zeros((5,10)).astype('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(next_word):\n",
    "    print(i, w)\n",
    "    labels[i,[w]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word.data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1,17000).astype('f')\n",
    "x = np.zeros((1,17000)).astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,2] = 10.0\n",
    "x[0,0] = -10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.log_softmax(x).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0 * np.ones((1,17000), dtype='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0,2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(x, t, reduce='no', normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(x, t, reduce='no', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(x, t, reduce='mean', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(x, t, reduce='mean', normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(x, t, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[-2.0, 3.0, -2.0], [-2.0, 3.0, -2.0]]).astype('f')\n",
    "t = np.array([[-1, -1, -1], [0, 1, 0]]).astype('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((1,10), dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,[2]] = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,list(range(2))+list(range(3,10))] = -.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros((1,10), dtype=\"i\")\n",
    "t[0,5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_out[:1], xp.argmax(predicted_out[:1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 0 * xp.ones(predicted_out[:1].shape).astype('i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = xp.zeros(predicted_out[:1].shape).astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0,[2,5]] = 50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0,[2,5]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(t, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid_cross_entropy(F.softmax(predicted_out[:1]), xp.expand_dims(labels, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['w'][b'rico'], sim_dict['i'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp.random.choice(sim_dict['i'][4], 1), sim_dict['i'][w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(t_alt)):\n",
    "    print(t_alt, i)\n",
    "    print(t_alt[i])\n",
    "    #t_alt[i] = xp.random.sample(sim_dict['i'][t_alt[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_alt)\n",
    "for i in range(len(t_alt)):\n",
    "    print(xp.random.choice(sim_dict['i'][int(t_alt[i])],1))\n",
    "    print(t_alt[i],type(t_alt[i]), int(t_alt[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_batch = y_t\n",
    "batch_size = decoder_batch.shape[1]\n",
    "loss = 0\n",
    "# ---------------------------------------------------------------------\n",
    "# initialize hidden states as a zero vector\n",
    "# ---------------------------------------------------------------------\n",
    "a_units = model.m_cfg['attn_units']\n",
    "ht = Variable(xp.zeros((batch_size, a_units), dtype=xp.float32))\n",
    "# ---------------------------------------------------------------------\n",
    "decoder_input = decoder_batch[0]\n",
    "# for all sequences in the batch, feed the characters one by one\n",
    "for curr_word, next_word in zip(decoder_batch, decoder_batch[1:]):\n",
    "    print(curr_word, next_word)\n",
    "    decoder_input = curr_word\n",
    "    # -----------------------------------------------------------------\n",
    "    # encode tokens\n",
    "    # -----------------------------------------------------------------\n",
    "    predicted_out, ht = model.decode(decoder_input, ht)\n",
    "    decoder_input = F.argmax(predicted_out, axis=1)\n",
    "    #print(decoder_input)\n",
    "    # -----------------------------------------------------------------\n",
    "    # compute loss\n",
    "    # -----------------------------------------------------------------\n",
    "    t_alt = xp.copy(next_word.data)\n",
    "    print(t_alt)\n",
    "    for i in range(len(t_alt)):\n",
    "        t_alt[i] = xp.random.choice(sim_dict['i'][int(t_alt[i])],1)\n",
    "        #print(t[i,[4,1044, 1045, 2477]])\n",
    "    print(t_alt)\n",
    "\n",
    "#     t = xp.zeros(shape=predicted_out.shape, dtype='i')\n",
    "#     print(next_word.data.tolist())\n",
    "#     print(next_word.shape)\n",
    "#     for i,w in enumerate(next_word.data.tolist()):\n",
    "#         if w == PAD_ID:\n",
    "#             t[i,:] = -1\n",
    "#         else:\n",
    "#             t[i,sim_dict['i'][w]] = 1\n",
    "#         #print(t[i,[4,1044, 1045, 2477]])\n",
    "#     loss_arr = F.sigmoid_cross_entropy(predicted_out, t, normalize=True)\n",
    "    loss_arr = F.softmax_cross_entropy(predicted_out, t_alt, normalize=True)\n",
    "    print(\"softmax cross entropy:\", F.softmax_cross_entropy(predicted_out, next_word), \"sigmoid:\", loss_arr.data.tolist())\n",
    "    loss += loss_arr\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "#print(loss, loss / (y.shape[0]-2), y.shape)\n",
    "print(loss.data.tolist(), (loss / (y.shape[0]-1)).data.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare BLEU scores at utterance level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051023_232057_325_fsp-A-3\"\n",
    "print(sentence_bleu([google_dev_ref_0[eg_utt]], google_hyp_r0[eg_utt], smoothing_function=smooth_fun.method2))\n",
    "print(sentence_bleu([model_s2t_refs[eg_utt]], model_s2t_hyps[eg_utt], smoothing_function=smooth_fun.method2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051019_190221_288_fsp-B-1\"\n",
    "print(sentence_bleu([google_dev_ref_0[eg_utt]], google_hyp_r0[eg_utt], smoothing_function=smooth_fun.method2))\n",
    "print(sentence_bleu([model_s2t_refs[eg_utt]], model_s2t_hyps[eg_utt], smoothing_function=smooth_fun.method2))\n",
    "print(google_dev_ref_0[eg_utt], google_hyp_r0[eg_utt])\n",
    "print(model_s2t_refs[eg_utt], model_s2t_hyps[eg_utt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(\"haha\")\n",
    "dev_utts = list(model_s2t_refs.keys())\n",
    "random.shuffle(dev_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(m_cfg['model_dir'], \"probs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"google beats model by factor of 2\")\n",
    "\n",
    "count = 0\n",
    "# print(\"-\"*80)\n",
    "# print(\"{0:>5s} ||| {1:30s} ||| {2:>15s} || {3:>15s}\".format(\"sn\", \"utt\", \"google utt bleu\", \"model utt bleu\"))\n",
    "# print(\"-\"*80)\n",
    "for utt in dev_utts:\n",
    "    if len(model_s2t_refs[utt]) < 10:\n",
    "        google_utt_bleu = sentence_bleu([google_dev_ref_0[utt]], google_hyp_r0[utt], smoothing_function=smooth_fun.method2)\n",
    "        model_utt_bleu = sentence_bleu([model_s2t_refs[utt]], model_s2t_hyps[utt], smoothing_function=smooth_fun.method2)\n",
    "        if google_utt_bleu >= (2 * model_utt_bleu) and google_utt_bleu >= 0.5:\n",
    "            count += 1\n",
    "            #print(\"{0:5d} ||| {1:30s} ||| {2:15.2f} || {3:15.2f}\".format(count, utt, google_utt_bleu, model_utt_bleu))\n",
    "            print(\"-\"*80)\n",
    "            print(count)\n",
    "            print(\"-\"*80)\n",
    "    #         display_pp = PrettyTable([\"cat\",\"sent\"], hrules=True)\n",
    "    #         display_pp.align = \"l\"\n",
    "    #         display_pp.header = False\n",
    "    #         display_pp.add_row([\"en ref\", textwrap.fill(\" \".join(model_s2t_refs[utt]),50)])\n",
    "    #         display_pp.add_row([\"model pred\", textwrap.fill(\" \".join(model_s2t_hyps[utt]),50)])\n",
    "    #         display_pp.add_row([\"model utt bleu\", \"{0:.2f}\".format(model_utt_bleu)])\n",
    "    #         display_pp.add_row([\"google pred\", textwrap.fill(\" \".join(google_hyp_r0[utt]),50)])\n",
    "    #         display_pp.add_row([\"google utt bleu\", \"{0:.2f}\".format(google_utt_bleu)])\n",
    "    #         print(display_pp)\n",
    "    #         play_utt(utt, map_dict['fisher_dev'])\n",
    "            generate_translate_probs(utt)\n",
    "        if count > 50:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict['en_w']['i2w'][494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_utt = \"20051009_210519_219_fsp-A-16\"\n",
    "generate_translate_probs(eg_utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model beats google by factor of 2\")\n",
    "\n",
    "count = 0\n",
    "# print(\"-\"*80)\n",
    "# print(\"{0:>5s} ||| {1:30s} ||| {2:>15s} || {3:>15s}\".format(\"sn\", \"utt\", \"google utt bleu\", \"model utt bleu\"))\n",
    "# print(\"-\"*80)\n",
    "for utt in dev_utts:\n",
    "    if len(model_s2t_refs[utt]) > 3 and len(model_s2t_refs[utt]) < 20:\n",
    "        google_utt_bleu = sentence_bleu([google_dev_ref_0[utt]], google_hyp_r0[utt], smoothing_function=smooth_fun.method2)\n",
    "        model_utt_bleu = sentence_bleu([model_s2t_refs[utt]], model_s2t_hyps[utt], smoothing_function=smooth_fun.method2)\n",
    "        if model_utt_bleu >= (1.5 * google_utt_bleu) and model_utt_bleu >= 0.5:\n",
    "            count += 1\n",
    "            #print(\"{0:5d} ||| {1:30s} ||| {2:15.2f} || {3:15.2f}\".format(count, utt, google_utt_bleu, model_utt_bleu))\n",
    "            print(\"-\"*80)\n",
    "            print(count)\n",
    "            print(\"-\"*80)\n",
    "    #         display_pp = PrettyTable([\"cat\",\"sent\"], hrules=True)\n",
    "    #         display_pp.align = \"l\"\n",
    "    #         display_pp.header = False\n",
    "    #         display_pp.add_row([\"en ref\", textwrap.fill(\" \".join(model_s2t_refs[utt]),50)])\n",
    "    #         display_pp.add_row([\"model pred\", textwrap.fill(\" \".join(model_s2t_hyps[utt]),50)])\n",
    "    #         display_pp.add_row([\"model utt bleu\", \"{0:.2f}\".format(model_utt_bleu)])\n",
    "    #         display_pp.add_row([\"google pred\", textwrap.fill(\" \".join(google_hyp_r0[utt]),50)])\n",
    "    #         display_pp.add_row([\"google utt bleu\", \"{0:.2f}\".format(google_utt_bleu)])\n",
    "    #         print(display_pp)\n",
    "    #         play_utt(utt, map_dict['fisher_dev'])\n",
    "            generate_translate_probs(utt)\n",
    "        if count > 50:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_dict['es_w']['w2i']), len(vocab_dict['en_w']['w2i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_words = set(vocab_dict['es_w']['w2i'].keys())\n",
    "en_words = set(vocab_dict['en_w']['w2i'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(es_words), len(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = es_words & en_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_common_es = {w: vocab_dict['es_w']['freq'][w] for w in common_words}\n",
    "freq_common_en = {w: vocab_dict['en_w']['freq'][w] for w in common_words}\n",
    "freq_common_both = {w: (vocab_dict['en_w']['freq'][w], vocab_dict['es_w']['freq'][w]) for w in common_words}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(freq_common_es), len(freq_common_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(freq_common_es.values()), sum(freq_common_en.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_dict['es_w']['freq']), sum(vocab_dict['es_w']['freq'].values()), len(vocab_dict['en_w']['freq']), sum(vocab_dict['en_w']['freq'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "844202 / 1496796, 1282482 / 1497356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_common_en[b'que']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(freq_common_es.items(), reverse=True, key= lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(freq_common_en.items(), reverse=True, key= lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(freq_common_both.items(), reverse=True, key= lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 20\n",
    "common_in_both = [w for w, (c1, c2) in freq_common_both.items() if c1 >= C and c2 >= C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_in_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
