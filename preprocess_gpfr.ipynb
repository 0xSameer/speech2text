{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import bisect\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "import random\n",
    "import math\n",
    "import scipy as sp\n",
    "import scipy.io.wavfile as wav\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator, \\\n",
    "     FormatStrFormatter, AutoMinorLocator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_map = pickle.load(open(\"./mfcc_13dim/mboshi_map.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab = pickle.load(open(\"./mfcc_13dim/mboshi_train_vocab.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpfr_out_path = \"./gp/FR\"\n",
    "cfg_path = \"./mfcc_13dim/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['mboshi_train', 'mboshi_dev', 'mboshi_test']),\n",
       " dict_keys(['en_w', 'en_c', 'bpe_w']))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mboshi_map.keys(), mboshi_map[\"mboshi_train\"]['abiayi_2015-09-08-11-18-39_samsung-SM-T530_mdw_elicit_Dico18_1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_sets = ['fr_train', 'fr_dev', 'fr_eval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr_train\n",
      "fr_dev\n",
      "fr_eval\n"
     ]
    }
   ],
   "source": [
    "oov = {}\n",
    "map_dict = {}\n",
    "for c in fr_sets:\n",
    "    oov[c] = []\n",
    "    print(c)\n",
    "    map_dict[c] = {}\n",
    "    all_words = []\n",
    "    utt2words = {}\n",
    "    \n",
    "    with open(os.path.join(gpfr_out_path, \"{0:s}.BPE_1000.fr\".format(c)), \"rb\") as text_f, \\\n",
    "         open(os.path.join(gpfr_out_path, \"{0:s}.ids\".format(c)), \"r\") as id_f, \\\n",
    "         open(os.path.join(gpfr_out_path, \"{0:s}.clean.fr\".format(c)), \"rb\") as words_f:\n",
    "        for i, t, e in zip(id_f, text_f, words_f):\n",
    "            map_dict[c][i.strip()] = {}\n",
    "            map_dict[c][i.strip()][\"bpe_w\"] = t.strip().split()\n",
    "            map_dict[c][i.strip()][\"en_w\"] = e.strip().split()\n",
    "            for w in t.strip().split():\n",
    "                if w not in fr_vocab[\"bpe_w\"][\"w2i\"]:\n",
    "                    oov[c].append(w)\n",
    "    # end with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(map_dict, open(\"{0:s}/gpfr_map.dict\".format(cfg_path), \"wb\"))\n",
    "# pickle.dump(fr_vocab, open(\"{0:s}/gpfr_train_vocab.dict\".format(cfg_path), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1067"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_train_bpe_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bpe_text(c):\n",
    "    all_words = []\n",
    "    utt2words = {}\n",
    "    with open(os.path.join(gpfr_out_path, \"{0:s}.BPE_1000.fr\".format(c)), \"r\") as text_f, \\\n",
    "         open(os.path.join(gpfr_out_path, \"{0:s}.ids\".format(c)), \"r\") as id_f:\n",
    "        for u, line in tqdm(zip(id_f, text_f)):\n",
    "            t = line.strip().split()\n",
    "            utt2words[u.strip()] = t\n",
    "            all_words.extend(t)\n",
    "        # end for line\n",
    "    # end with\n",
    "    return utt2words, dict(Counter(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8818it [00:00, 73817.65it/s]\n"
     ]
    }
   ],
   "source": [
    "fr_train_bpe_text, fr_train_bpe_words = read_bpe_text(\"fr_train\")\n",
    "# fr_dev_bpe_text, fr_dev_bpe_words = read_bpe_text(\"fr_dev\")\n",
    "# fr_eval_bpe_text, fr_eval_bpe_words = read_bpe_text(\"fr_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_vocab(words):\n",
    "    out = {\"w2i\":{}, \"i2w\":{}, \"freq\":{}}\n",
    "    START_VOCAB = [PAD, GO, EOS, UNK]\n",
    "    for w in START_VOCAB:\n",
    "        out['w2i'][w] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][w] = 1\n",
    "    #for w in words_list['words']:\n",
    "    sorted_w = sorted(words.items(), reverse=True, key=lambda t: t[1])\n",
    "    for w in sorted_w:\n",
    "        encoded_word = w[0].encode()\n",
    "        out[\"w2i\"][encoded_word] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][encoded_word] = w[1]\n",
    "\n",
    "    out[\"i2w\"] = {val:key for key, val in out[\"w2i\"].items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab[\"bpe_w\"] = create_new_vocab(fr_train_bpe_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_vocab['bpe_w']['w2i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bpe_w': [b'ce',\n",
       "  b'num@@',\n",
       "  b'\\xc3\\xa9r@@',\n",
       "  b'o',\n",
       "  b'est',\n",
       "  b'com@@',\n",
       "  b'pos@@',\n",
       "  b'\\xc3\\xa9',\n",
       "  b'de',\n",
       "  b'quatre',\n",
       "  b'nomb@@',\n",
       "  b'res',\n",
       "  b's\\xc3\\xa9@@',\n",
       "  b'pa@@',\n",
       "  b'r\\xc3\\xa9s',\n",
       "  b'par',\n",
       "  b'des',\n",
       "  b'poin@@',\n",
       "  b'ts',\n",
       "  b'et',\n",
       "  b'dont',\n",
       "  b'la',\n",
       "  b'val@@',\n",
       "  b'eur',\n",
       "  b'est',\n",
       "  b'com@@',\n",
       "  b'prise',\n",
       "  b'entre',\n",
       "  b'z@@',\n",
       "  b'\\xc3\\xa9r@@',\n",
       "  b'o',\n",
       "  b'et',\n",
       "  b'deux',\n",
       "  b'cent',\n",
       "  b'cin@@',\n",
       "  b'qu@@',\n",
       "  b'ante',\n",
       "  b'cinq',\n",
       "  b'par',\n",
       "  b'exemple',\n",
       "  b'cent',\n",
       "  b'quatre',\n",
       "  b'vingt',\n",
       "  b'qu@@',\n",
       "  b'at@@',\n",
       "  b'or@@',\n",
       "  b'ze'],\n",
       " 'en_w': [b'ce',\n",
       "  b'num\\xc3\\xa9ro',\n",
       "  b'est',\n",
       "  b'compos\\xc3\\xa9',\n",
       "  b'de',\n",
       "  b'quatre',\n",
       "  b'nombres',\n",
       "  b's\\xc3\\xa9par\\xc3\\xa9s',\n",
       "  b'par',\n",
       "  b'des',\n",
       "  b'points',\n",
       "  b'et',\n",
       "  b'dont',\n",
       "  b'la',\n",
       "  b'valeur',\n",
       "  b'est',\n",
       "  b'comprise',\n",
       "  b'entre',\n",
       "  b'z\\xc3\\xa9ro',\n",
       "  b'et',\n",
       "  b'deux',\n",
       "  b'cent',\n",
       "  b'cinquante',\n",
       "  b'cinq',\n",
       "  b'par',\n",
       "  b'exemple',\n",
       "  b'cent',\n",
       "  b'quatre',\n",
       "  b'vingt',\n",
       "  b'quatorze']}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_dict['fr_train']['FR001_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(set(oov[c])) for c in oov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(map_dict, open(\"{0:s}/gpfr_bpe_map.dict\".format(cfg_path), \"wb\"))\n",
    "# pickle.dump(fr_vocab, open(\"{0:s}/gpfr_bpe_train_vocab.dict\".format(cfg_path), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8819/8819 [00:04<00:00, 2120.83it/s]\n",
      "100%|██████████| 840/840 [00:00<00:00, 2345.84it/s]\n",
      "100%|██████████| 822/822 [00:00<00:00, 2324.08it/s]\n"
     ]
    }
   ],
   "source": [
    "gpfr_data = {}\n",
    "for c in fr_sets:\n",
    "    gpfr_data[c] = {}\n",
    "    for x in tqdm(os.listdir(os.path.join(gpfr_out_path, c))):\n",
    "        if x.endswith(\".np\"):\n",
    "            temp = np.load(os.path.join(gpfr_out_path, c, x))\n",
    "            for k in temp:\n",
    "                gpfr_data[c][k] = temp[k]\n",
    "        # end for\n",
    "    # end for\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FR087_19',\n",
       " 'FR085_18',\n",
       " 'FR088_5',\n",
       " 'FR085_25',\n",
       " 'FR082_29',\n",
       " 'FR083_36',\n",
       " 'FR084_97',\n",
       " 'FR087_2',\n",
       " 'FR084_75',\n",
       " 'FR083_54']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gpfr_data[\"fr_dev\"].keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.064710096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gpfr_data['fr_dev']['FR087_19'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fr_train', 'fr_dev', 'fr_eval'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpfr_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 8818/8818 [00:00<00:00, 164473.73it/s]\n",
      "100%|█████████████████████████████████████| 839/839 [00:00<00:00, 218031.04it/s]\n",
      "100%|█████████████████████████████████████| 821/821 [00:00<00:00, 222896.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr_train\n",
      "fr_dev\n",
      "fr_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "info = {}\n",
    "durs = {}\n",
    "for c in fr_sets:\n",
    "    print(c)\n",
    "    info[c] = {}\n",
    "    durs[c] = []\n",
    "    for x in tqdm(map_dict[c], ncols=80):\n",
    "        info[c][x] = {}\n",
    "        t_data = gpfr_data[c][x]\n",
    "        info[c][x][\"sp\"] = t_data.shape[0]\n",
    "        info[c][x][\"es_w\"] = 0\n",
    "        info[c][x][\"es_c\"] = 0\n",
    "        info[c][x][\"en_w\"] = len(map_dict[c][x][\"en_w\"])\n",
    "        info[c][x][\"en_c\"] = 0\n",
    "        durs[c].append(t_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr_train\n",
      "total hrs = 22.743\n",
      "min = 0.94, max = 23.85, mean = 9.28\n",
      "fr_dev\n",
      "total hrs = 2.128\n",
      "min = 4.78, max = 19.95, mean = 9.13\n",
      "fr_eval\n",
      "total hrs = 2.023\n",
      "min = 0.49, max = 15.34, mean = 8.87\n"
     ]
    }
   ],
   "source": [
    "for c in durs:\n",
    "    print(c)\n",
    "    print(\"total hrs = {0:.3f}\".format(sum(durs[c]) / 100. / 3600))\n",
    "    print(\"min = {0:.2f}, max = {1:.2f}, mean = {2:.2f}\".format(np.min(durs[c])/100, \n",
    "                                                                np.max(durs[c])/100, np.mean(durs[c])/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(info, open(\"./mfcc_13dim/info_gpfr.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gpfr_data, open(\"./gp/FR/gpfr_data.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_file = \"./gp/FR/fr_dev.clean.fr\"\n",
    "ref_out_file = \"./gp/FR/fr_dev.clean.wer.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in fr_sets:\n",
    "    with open(os.path.join(gpfr_out_path, \"{0:s}.ids\".format(c)), \"r\", encoding=\"utf-8\") as id_f, \\\n",
    "         open(os.path.join(gpfr_out_path, \"{0:s}.clean.fr\".format(c)), \"r\", encoding=\"utf-8\") as words_f, \\\n",
    "         open(os.path.join(gpfr_out_path, \"{0:s}.clean.wer.fr\".format(c)), \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for i, t in zip(id_f, words_f):\n",
    "            out_f.write(\"{0:s} ({1:s})\\n\".format(t.strip(), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_ids = {k: [] for k in fr_sets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in callhome_ids:\n",
    "    print(i)\n",
    "    callhome_ids[i] = sorted(list(callhome_map[i].keys()))\n",
    "    print(\"# utts = {0:d}\".format(len(ids)))\n",
    "    with open(os.path.join(callhome_out_path, \"{0:s}.ids\".format(i)), \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for j in callhome_ids[i]:\n",
    "            out_f.write(\"{0:s}\\n\".format(j))\n",
    "    \n",
    "    with open(os.path.join(callhome_out_path, \"{0:s}.en\".format(i)), \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for j in callhome_ids[i]:\n",
    "            out_text = \" \".join([w.decode() for w in callhome_map[i][j][\"en_w\"]])\n",
    "            out_text = get_out_str(out_text)\n",
    "            out_f.write(\"{0:s}\\n\".format(out_text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov = {}\n",
    "for i in callhome_ids:\n",
    "    oov[i] = []\n",
    "    print(i)\n",
    "    bpe_path = os.path.join(callhome_out_path, \"{0:s}.BPE_1000.en\".format(i))\n",
    "    id_path = os.path.join(callhome_out_path, \"{0:s}.ids\".format(i))\n",
    "    with open(bpe_path, \"rb\") as bpe_f, open(id_path, \"r\", encoding=\"utf-8\") as id_f:\n",
    "        for line_id, line_bpe in zip(id_f, bpe_f):\n",
    "            u = line_id.strip()\n",
    "            t = line_bpe.strip().split()\n",
    "            callhome_map[i][u][\"bpe_w\"] = t\n",
    "            for j in t:\n",
    "                if j not in callhome_vocab[\"bpe_w\"][\"w2i\"]:\n",
    "                    oov[i].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callhome_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(callhome_map, open(\"./mfcc_13dim/callhome_map.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callhome_info = pickle.load(open(\"./mfcc_13dim/callhome_info.dict\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callhome_info['callhome_devtest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durs = {}\n",
    "durs_by_utt = {}\n",
    "for i in callhome_ids:\n",
    "    oov[i] = []\n",
    "    print(i)\n",
    "    durs[i] = []\n",
    "    durs_by_utt[i] = {}\n",
    "    for x in tqdm(callhome_info[i], ncols=80):\n",
    "        t_data = np.load(\"./mfcc_13dim/{0:s}/{1:s}.npy\".format(i,x))\n",
    "        durs[i].append(t_data.shape[0])\n",
    "        durs_by_utt[i][x] = t_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in durs:\n",
    "    print(c)\n",
    "    print(\"total hrs = {0:.3f}\".format(sum(durs[c]) / 100. / 3600))\n",
    "    print(\"min = {0:.2f}, max = {1:.2f}, mean = {2:.2f}\".format(np.min(durs[c])/100, \n",
    "                                                                np.max(durs[c])/100, np.mean(durs[c])/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(durs[\"callhome_train\"], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in callhome_info[\"callhome_devtest\"]:\n",
    "    if callhome_info[\"callhome_devtest\"][u][\"sp\"] != durs_by_utt[\"callhome_devtest\"][u]:\n",
    "        print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bpe_text(text_path, ids):\n",
    "    all_words = []\n",
    "    utt2words = {}\n",
    "    with open(text_path, \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for u, line in tqdm(zip(ids, in_f)):\n",
    "            t = line.split()\n",
    "            utt2words[u] = t\n",
    "            all_words.extend(t)\n",
    "        # end for line\n",
    "    # end with\n",
    "    return utt2words, dict(Counter(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_train_bpe_text, mboshi_train_bpe_words = read_bpe_text(os.path.join(\"mboshi\", \n",
    "                                                              \"mboshi_train.BPE_1000.fr\"),\n",
    "                                                              train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_dev_bpe_text, mboshi_dev_bpe_words = read_bpe_text(os.path.join(\"mboshi\", \n",
    "                                                          \"mboshi_dev.BPE_1000.fr\"),\n",
    "                                                           dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_train_bpe_text[train_ids[4]], mboshi_train_dev_text[train_ids[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mboshi_train_bpe_words), len(mboshi_dev_bpe_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_text_sets = {\"mboshi_train\": mboshi_train_bpe_text, \n",
    "             \"mboshi_dev\": mboshi_dev_bpe_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in mboshi_train_dev_chars:\n",
    "    print(c, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_vocab(words):\n",
    "    out = {\"w2i\":{}, \"i2w\":{}, \"freq\":{}}\n",
    "    START_VOCAB = [PAD, GO, EOS, UNK]\n",
    "    for w in START_VOCAB:\n",
    "        out['w2i'][w] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][w] = 1\n",
    "    #for w in words_list['words']:\n",
    "    sorted_w = sorted(words.items(), reverse=True, key=lambda t: t[1])\n",
    "    for w in sorted_w:\n",
    "        encoded_word = w[0].encode()\n",
    "        out[\"w2i\"][encoded_word] = len(out[\"w2i\"])\n",
    "        out[\"freq\"][encoded_word] = w[1]\n",
    "\n",
    "    out[\"i2w\"] = {val:key for key, val in out[\"w2i\"].items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab[\"en_c\"] = create_new_vocab(mboshi_train_dev_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab[\"en_w\"] = create_new_vocab(mboshi_train_dev_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab[\"bpe_w\"] = create_new_vocab(mboshi_train_bpe_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_vocab[\"en_c\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fr_vocab, open(\"./mfcc_13dim/mboshi_train_vocab.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mboshi_map():\n",
    "    oov = {}\n",
    "    mboshi_map = {}\n",
    "    \n",
    "    for id_set in id_sets:\n",
    "        print(id_set)\n",
    "        mboshi_map[id_set] = {}\n",
    "        oov[id_set] = []\n",
    "        for i in id_sets[id_set]:\n",
    "            mboshi_map[id_set][i] = {}            \n",
    "            words = text_sets[id_set][i].split()\n",
    "            encoded_words = [w.encode() for w in words]\n",
    "            encoded_chars = [c.encode() for c in text_sets[id_set][i]]\n",
    "            \n",
    "            mboshi_map[id_set][i][\"en_w\"] = encoded_words\n",
    "            mboshi_map[id_set][i][\"en_c\"] = encoded_chars\n",
    "            \n",
    "            if id_set in bpe_text_sets:\n",
    "                bpe_tokens = [w.encode() for w in bpe_text_sets[id_set][i]]\n",
    "                \n",
    "            else:\n",
    "                bpe_tokens = []\n",
    "            mboshi_map[id_set][i][\"bpe_w\"] = bpe_tokens\n",
    "            \n",
    "            for c in bpe_tokens:\n",
    "                if c not in fr_vocab[\"bpe_w\"][\"w2i\"]:\n",
    "                    oov[id_set].append(c)\n",
    "    return mboshi_map, oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mboshi_map, oov = create_mboshi_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mboshi_map[\"mboshi_dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mboshi_map, open(\"./mfcc_13dim/mboshi_map.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = en_path = os.path.join(ainu_path, \"wav\")\n",
    "en_path = os.path.join(ainu_path, \"encl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [w.replace(\".wav\", \"\") for w in os.listdir(wav_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'0.000'.split(\".\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt2spk = {k: k.split(\".\",1)[0] for k in ids}\n",
    "spk2utt = {}\n",
    "\n",
    "for k in ids:\n",
    "    spk_key = k.split(\".\",1)[0]\n",
    "    if spk_key in spk2utt:\n",
    "        spk2utt[spk_key].append(k)\n",
    "    else:\n",
    "        spk2utt[spk_key] = []\n",
    "        \n",
    "print(\"# of utts = {0:d}\".format(len(ids)))\n",
    "print(\"# of narratives = {0:d}\".format(len(spk2utt)))\n",
    "for s in spk2utt:\n",
    "    print(s, len(spk2utt[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(ainu_path, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_file = os.path.join(out_path, \"ainu10.ids\")\n",
    "text_file = os.path.join(out_path, \"ainu10.clean.en\")\n",
    "bpe_file = os.path.join(out_path, \"ainu10.BPE_1000.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ids_file, \"w\", encoding=\"utf-8\") as ids_f, open(text_file, \"w\", encoding=\"utf-8\") as text_f:\n",
    "    for i in ids:\n",
    "        ids_f.write(\"{0:s}\\n\".format(i))\n",
    "        with open(os.path.join(en_path, \"{0:s}.en.cl\".format(i)), \"r\") as en_f:\n",
    "            lines = en_f.readlines()\n",
    "            text = clean_out_str(lines[0].strip())\n",
    "            if len(lines) > 1:\n",
    "                print(i, len(lines))\n",
    "            text_f.write(\"{0:s}\\n\".format(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MFCCs, and normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_path = '/afs/inf.ed.ac.uk/group/project/ast/work/corpora/mboshi-french-parallel-corpus/wavs'\n",
    "mfcc_path = '/afs/inf.ed.ac.uk/group/project/ast/work/corpora/mboshi-french-parallel-corpus/mfcc_raw'\n",
    "mfcc_std_path = '/afs/inf.ed.ac.uk/group/project/ast/work/corpora/mboshi-french-parallel-corpus/mfcc_std'\n",
    "mfcc_final_path = '/afs/inf.ed.ac.uk/group/project/ast/work/corpora/mboshi-french-parallel-corpus/mboshi_mfccs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_set in id_sets:\n",
    "    ids = id_sets[id_set]\n",
    "    for i in tqdm(ids, ncols=80):\n",
    "        wav_fname = os.path.join(wav_path, \"{0:s}.wav\".format(i))\n",
    "        mfcc_fname = os.path.join(mfcc_path, \"{0:s}.mfcc\".format(i))\n",
    "        mfcc_std_fname = os.path.join(mfcc_std_path, \"{0:s}.std.mfcc\".format(i))\n",
    "        mfcc_final_fname = os.path.join(mfcc_final_path, \"{0:s}\".format(i))\n",
    "\n",
    "        !$FEACALC -plp no -cep 13 -dom cep -deltaorder 0 -dither -frqaxis bark \\\n",
    "            -samplerate 16000 -win 25 -step 10 -ip MSWAVE -rasta false -compress true \\\n",
    "            -op swappedraw -o $mfcc_fname $wav_fname\n",
    "\n",
    "        !$STANDFEAT -D 13 -infile $mfcc_fname -outfile $mfcc_std_fname\n",
    "\n",
    "        out_mfcc = np.fromfile(mfcc_std_fname, dtype=np.float32)\n",
    "        out_mfcc = out_mfcc.reshape((-1,13))\n",
    "        print(out_mfcc.shape)\n",
    "        np.save(mfcc_final_fname, out_mfcc)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha = np.load(os.path.join(mfcc_final_path, \"abiayi_2015-09-08-11-18-39_samsung-SM-T530_mdw_elicit_Dico18_1.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = ha.reshape((-1,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha.shape, haha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in set(swbd1_ids) - {\"swbd1_train_nodev\"}:\n",
    "info = {}\n",
    "for c in id_sets:\n",
    "    print(c)\n",
    "    info[c] = {}\n",
    "    for x in tqdm(id_sets[c], ncols=80):\n",
    "        info[c][x] = {}\n",
    "        t_data = np.load(\"./mfcc_13dim/mboshi_mfccs/{0:s}.npy\".format(x))\n",
    "        info[c][x][\"sp\"] = t_data.shape[0]\n",
    "        info[c][x][\"es_w\"] = 0\n",
    "        info[c][x][\"es_c\"] = 0\n",
    "        info[c][x][\"en_w\"] = len(mboshi_map[c][x][\"en_w\"])\n",
    "        info[c][x][\"en_c\"] = len(mboshi_map[c][x][\"en_c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(info, open(\"./mfcc_13dim/info_mboshi.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durs = {}\n",
    "for c in id_sets:\n",
    "    print(c)\n",
    "    durs[c] = []\n",
    "    for x in tqdm(id_sets[c], ncols=80):\n",
    "        t_data = np.load(\"./mfcc_13dim/mboshi_mfccs/{0:s}.npy\".format(x))\n",
    "        durs[c].append(t_data.shape[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in durs:\n",
    "    print(c)\n",
    "    print(\"total hrs = {0:.3f}\".format(sum(durs[c]) / 100. / 3600))\n",
    "    print(\"min = {0:.2f}, max = {1:.2f}, mean = {2:.2f}\".format(np.min(durs[c])/100, \n",
    "                                                                np.max(durs[c])/100, np.mean(durs[c])/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"mboshi/mboshi_train.fr\"\n",
    "dev_text = \"mboshi/mboshi_test.fr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text_fname):\n",
    "    words = []\n",
    "    with open(text_fname, \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for line in in_f:\n",
    "            words.extend(line.strip().split())\n",
    "    return Counter(words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter = get_words(train_text)\n",
    "dev_counter = get_words(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lengths(text_fname):\n",
    "    lengths = []\n",
    "    with open(text_fname, \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for line in in_f:\n",
    "            lengths.append(len(line.strip().split()))\n",
    "    return np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths = get_lengths(train_text)\n",
    "dev_lengths = get_lengths(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_lengths), np.min(train_lengths), np.max(train_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dev_lengths), np.min(dev_lengths), np.max(dev_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 30\n",
    "N = len(dev_lengths)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_k_most_common(out_fname, K, N):\n",
    "    k_words = \" \".join([i[0] for i in train_counter.most_common(K)])\n",
    "    out_line = \"{0:s}\\n\".format(k_words)\n",
    "    with open(out_fname, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for n in range(N):\n",
    "            out_f.write(out_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_k_most_common(\"./mboshi/mboshi_test_dummy_top-{0:d}_words.fr\".format(K), K=K, N=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common sentences in Mboshi Train\n",
    "sort mboshi/mboshi_train.fr | uniq -c | sort -rn | head -n 12\n",
    "\n",
    "```\n",
    "6 les pêcheurs ont rapporté beaucoup de poisson\n",
    "6 les bananes sont pleines\n",
    "6 il est très bavard\n",
    "6 attends moi  j'arrive\n",
    "5 il n'aime pas être battu au jeu\n",
    "5 celui ci est mon champ  celui là est à mon oncle\n",
    "4 tu peux partir devant  je t'atteindrai en route\n",
    "4 si tu attends encore un peu  il va venir\n",
    "4 ses cheveux sont brillants\n",
    "4 on brûle les herbes\n",
    "4 on a augmenté la paie des trvailleurs\n",
    "4 l'éléphant barrit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common sentences in Fisher Train\n",
    "sort ../subword-nmt/fisher_train.en | uniq -c | sort -rn | head -n 20\n",
    "\n",
    "```\n",
    "   6804 yes\n",
    "   3106 aha\n",
    "   1975 mm\n",
    "   1237 hmm\n",
    "   1217 sure\n",
    "   1150 oh\n",
    "   1054 ah\n",
    "    930 mhm\n",
    "    790 yeah\n",
    "    726 yes yes\n",
    "    708 right\n",
    "    632 uh huh\n",
    "    434 hello\n",
    "    429 exactly\n",
    "    424 no\n",
    "    409 okay\n",
    "    403 uh uh\n",
    "    362 hm mm\n",
    "    358 oh yes\n",
    "    346 um\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare seq-to-seq dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = \"./mfcc_13dim/\"\n",
    "map_dict = pickle.load(open(\"../speech2text/mfcc_13dim/bpe_map.dict\", \"rb\"))\n",
    "vocab_dict = pickle.load(open(\"../speech2text/mfcc_13dim/bpe_train_vocab.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk2utt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ainu_map(train_spkrs, dev_spkrs, test_spkrs):\n",
    "    oov = {}\n",
    "    ainu_map = {}\n",
    "    train_ids_str = \"_\".join(map(str, train_spkrs))\n",
    "    dev_ids_str = \"_\".join(map(str, dev_spkrs))\n",
    "    test_ids_str = \"_\".join(map(str, test_spkrs))\n",
    "    \n",
    "    ainu_map_fname = \"ainu_train-{0:s}-dev-{1:s}-test-{2:s}_map.dict\".format(train_ids_str, \n",
    "                                                                             dev_ids_str, \n",
    "                                                                             test_ids_str)\n",
    "        \n",
    "    \n",
    "    with open(os.path.join(\"../subword-nmt/\", \"ainu10.BPE_1000.en\"), \"rb\") as text_f, \\\n",
    "         open(os.path.join(\"../subword-nmt/\", \"ainu10.ids\"), \"r\") as id_f, \\\n",
    "         open(os.path.join(\"../subword-nmt/\", \"ainu10.clean.en\"), \"rb\") as words_f:\n",
    "        for i, t, e in zip(id_f, text_f, words_f):\n",
    "            curr_spkr = utt2spk[i.strip()]\n",
    "            if curr_spkr in train_spkrs:\n",
    "                c = \"ainu_train-{0:s}\".format(train_ids_str)\n",
    "            elif curr_spkr in dev_spkrs:\n",
    "                c = \"ainu_dev-{0:s}\".format(dev_ids_str)\n",
    "            elif curr_spkr in test_spkrs:\n",
    "                c = \"ainu_test-{0:s}\".format(test_ids_str)\n",
    "            else:\n",
    "                print(i, curr_spkr)\n",
    "                print(\"Achtung!!\")\n",
    "            if c not in ainu_map:\n",
    "                ainu_map[c] = {}\n",
    "                oov[c] = []\n",
    "            ainu_map[c][i.strip()] = {}\n",
    "            ainu_map[c][i.strip()][\"bpe_w\"] = t.strip().split()\n",
    "            ainu_map[c][i.strip()][\"en_w\"] = e.strip().split()\n",
    "            ainu_map[c][i.strip()][\"en_c\"] = [tt.encode() for tt in e.strip().decode()]\n",
    "            for w in t.strip().split():\n",
    "                if w not in vocab_dict[\"bpe_w\"][\"w2i\"]:\n",
    "                    oov[c].append(w)\n",
    "                    \n",
    "            for w in t.strip().split():\n",
    "                if w not in vocab_dict[\"bpe_w\"][\"w2i\"]:\n",
    "                    oov[c].append(w)\n",
    "\n",
    "    print(ainu_map_fname)\n",
    "    return ainu_map, ainu_map_fname, oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spkrs=[str(i) for i in range(2,10)]\n",
    "dev_spkrs=['0']\n",
    "test_spkrs=['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainu_map, ainu_map_fname, oov = create_ainu_map(train_spkrs=train_spkrs, \n",
    "                                                dev_spkrs=dev_spkrs, test_spkrs=test_spkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([w.decode() for w in oov['ainu_train-2_3_4_5_6_7_8_9']])\n",
    "print([w.decode() for w in oov['ainu_dev-0']])\n",
    "print([w.decode() for w in oov['ainu_test-1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainu_map['ainu_train-2_3_4_5_6_7_8_9']['2.000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ainu_ref(train_spkrs, dev_spkrs, test_spkrs):\n",
    "    train_ids_str = \"_\".join(map(str, train_spkrs))\n",
    "    dev_ids_str = \"_\".join(map(str, dev_spkrs))\n",
    "    test_ids_str = \"_\".join(map(str, test_spkrs))\n",
    "        \n",
    "    ainu_ids = {}\n",
    "    ainu_text = {}\n",
    "    with open(os.path.join(\"../subword-nmt/\", \"ainu10.ids\"), \"r\") as id_f, \\\n",
    "         open(os.path.join(\"../subword-nmt/\", \"ainu10.clean.en\"), \"r\") as words_f:\n",
    "        for i, e in zip(id_f, words_f):\n",
    "            curr_spkr = utt2spk[i.strip()]\n",
    "            if curr_spkr in train_spkrs:\n",
    "                c = \"ainu_train-{0:s}\".format(train_ids_str)\n",
    "            elif curr_spkr in dev_spkrs:\n",
    "                c = \"ainu_dev-{0:s}\".format(dev_ids_str)\n",
    "            elif curr_spkr in test_spkrs:\n",
    "                c = \"ainu_test-{0:s}\".format(test_ids_str)\n",
    "            else:\n",
    "                print(i, curr_spkr)\n",
    "                print(\"Achtung!!\")\n",
    "            if c not in ainu_ids:\n",
    "                ainu_ids[c] = []\n",
    "                ainu_text[c] = []\n",
    "            \n",
    "            ainu_ids[c].append(i.strip())\n",
    "            ainu_text[c].append(e.strip())\n",
    "\n",
    "    print(list(ainu_ids.keys()))\n",
    "    return ainu_ids, ainu_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainu_ids, ainu_text = create_ainu_ref(train_spkrs=train_spkrs, dev_spkrs=dev_spkrs, test_spkrs=test_spkrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ainu_text:\n",
    "    with open(\"./mfcc_13dim/ainu/{0:s}.en\".format(c), \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for i in ainu_text[c]:\n",
    "            out_f.write(\"{0:s}\\n\".format(i))\n",
    "        # end for\n",
    "    # end with\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing ids to: {0:s}\".format(ainu_map_fname.replace(\"map\", \"ids\")))\n",
    "pickle.dump(ainu_ids, open(\"./mfcc_13dim/ainu/{0:s}\".format(ainu_map_fname.replace(\"map\", \"ids\")), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing map to: {0:s}\".format(ainu_map_fname))\n",
    "pickle.dump(ainu_map, open(os.path.join(cfg_path, ainu_map_fname), \"wb\"))\n",
    "print(\"writing vocab to: {0:s}\".format(ainu_map_fname.replace(\"_map\", \"_train_vocab\")))\n",
    "pickle.dump(vocab_dict, open(os.path.join(cfg_path, ainu_map_fname.replace(\"_map\", \"_train_vocab\")), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pickle.load(open(\"../speech2text/mfcc_13dim/info.dict\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ainu_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in set(swbd1_ids) - {\"swbd1_train_nodev\"}:\n",
    "info = {}\n",
    "for c in set(ainu_map.keys()):    \n",
    "    info[c] = {}\n",
    "    for x in tqdm(ainu_ids[c], ncols=80):\n",
    "        info[c][x] = {}\n",
    "        t_data = np.load(\"./mfcc_13dim/ainu_mfccs/{0:s}.npy\".format(x))\n",
    "        info[c][x][\"sp\"] = t_data.shape[0]\n",
    "        info[c][x][\"es_w\"] = 0\n",
    "        info[c][x][\"es_c\"] = 0\n",
    "        info[c][x][\"en_w\"] = len(ainu_map[c][x][\"en_w\"])\n",
    "        info[c][x][\"en_c\"] = len(\" \".join([w.decode() for w in ainu_map[c][x][\"en_w\"]]))\n",
    "    # end for        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"writing info to: {0:s}\".format(\"./mfcc_13dim/{0:s}\".format(ainu_map_fname.replace(\"map\", \"info\"))))\n",
    "pickle.dump(info, open(\"./mfcc_13dim/{0:s}\".format(ainu_map_fname.replace(\"map\", \"info\")), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array([len(ainu_map[\"ainu_train-2_3_4_5_6_7_8_9\"][x][\"bpe_w\"]) for x in info[\"ainu_train-2_3_4_5_6_7_8_9\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array([info[\"ainu_train-2_3_4_5_6_7_8_9\"][x][\"en_w\"] for x in info[\"ainu_train-2_3_4_5_6_7_8_9\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mfb(wav_fname, mfb_fname, mfb_std_fname, nfilt=40):\n",
    "    rate, sig = wav.read(wav_fname)\n",
    "    mfb_feat = logfbank(sig,rate, nfilt=nfilt)\n",
    "    d_mfb_feat = delta(mfb_feat, 2)\n",
    "    dd_mfb_feat = delta(d_mfb_feat, 2)\n",
    "    mfb_feat = np.concatenate((mfb_feat, d_mfb_feat), axis=1)\n",
    "    mfb_feat = np.concatenate((mfb_feat, dd_mfb_feat), axis=1)\n",
    "    try:\n",
    "        mfb_feat_std = (mfb_feat - np.mean(mfb_feat, axis=0)) / np.std(mfb_feat, axis=0)\n",
    "    except:\n",
    "        print(wav_fname)\n",
    "    # save mfb files\n",
    "#     np.save(open(mfb_fname, \"wb\"), mfb_feat)\n",
    "    np.save(open(mfb_std_fname, \"wb\"), mfb_feat_std.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mfcc(wav_fname, mfcc_fname, mfcc_std_fname):\n",
    "    rate, sig = wav.read(wav_fname)\n",
    "    mfcc_feat = mfcc(sig,rate)\n",
    "    d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "    dd_mfcc_feat = delta(d_mfcc_feat, 2)\n",
    "    mfcc_feat = np.concatenate((mfcc_feat, d_mfcc_feat), axis=1)\n",
    "    mfcc_feat = np.concatenate((mfcc_feat, dd_mfcc_feat), axis=1)\n",
    "    std_vals = np.std(mfcc_feat, axis=0)\n",
    "    mfcc_feat_std = (mfcc_feat - np.mean(mfcc_feat, axis=0)) / np.std(mfcc_feat, axis=0)\n",
    "    if not np.isfinite(std_vals).all() or not np.isfinite(std_vals).all():\n",
    "        print(\"HAAAALP\", wav_fname)\n",
    "    # save mfcc files\n",
    "#     np.save(open(mfcc_fname, \"wb\"), mfcc_feat)\n",
    "    np.save(open(mfcc_std_fname, \"wb\"), mfcc_feat_std.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_speech_features(mffc1mfb0=True):\n",
    "    with tqdm(total=len(align_dict)) as pbar:\n",
    "        for wav_fil in sorted(align_dict.keys()):\n",
    "            for j, uttr in enumerate(sorted(align_dict[wav_fil].keys())):\n",
    "                wav_fname = os.path.join(fa_vad_wavs_path, \"{0:s}_fa_vad.wav\".format(uttr))\n",
    "                if mffc1mfb0:\n",
    "                    mfcc_fname = os.path.join(fa_vad_mfcc_path, \"{0:s}_fa_vad.mfcc\".format(uttr))\n",
    "                    mfcc_std_fname = os.path.join(fa_vad_std_mfcc_path, \"{0:s}_fa_vad.std.mfcc\".format(uttr))\n",
    "                    create_mfcc(wav_fname, mfcc_fname, mfcc_std_fname)\n",
    "                else:\n",
    "                    mfb_fname = os.path.join(fa_vad_mfb_path, \"{0:s}_fa_vad.mfb\".format(uttr))\n",
    "                    mfb_std_fname = os.path.join(fa_vad_std_mfb_path, \"{0:s}_fa_vad.std.mfb\".format(uttr))\n",
    "                    create_mfb(wav_fname, mfb_fname, mfb_std_fname)\n",
    "                \n",
    "            # end for uttr\n",
    "            pbar.update(1)\n",
    "        # end for file\n",
    "    # end pbar\n",
    "\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_all_speech_features(mffc1mfb0=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, uttr in enumerate(sorted(align_dict['038'].keys())[20:]):\n",
    "    print(uttr, end=', ')\n",
    "    wav_fname = os.path.join(fa_vad_wavs_path, \"{0:s}_fa_vad.wav\".format(uttr))\n",
    "    mfcc_fname = os.path.join(fa_vad_mfcc_path, \"{0:s}_fa_vad.mfcc\".format(uttr))\n",
    "    mfcc_std_fname = os.path.join(fa_vad_std_mfcc_path, \"{0:s}_fa_vad.std.mfcc\".format(uttr))\n",
    "    create_mfcc(wav_fname, mfcc_fname, mfcc_std_fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../uttr_fa_vad_wavs/mfcc/ | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = np.load(\"../uttr_fa_vad_wavs/mfcc/001.002_fa_vad.mfcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha_mfb = np.load(\"../uttr_fa_vad_wavs/mfb/001.002_fa_vad.mfb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha.shape, haha_mfb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha_mfb[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(haha_mfb, 0)[0,0,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flipud(haha_mfb)[-1,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### etc code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavfile = os.path.join(uttr_wavs_path, \"001.002.wav\")\n",
    "(rate,sig) = wav.read(wavfile)\n",
    "mfcc_feat = mfcc(sig,rate)\n",
    "mfb = logfbank(sig, rate, nfilt=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfb.shape, mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rate,sig) = wav.read(wavfile)\n",
    "mfcc_feat = mfcc(sig,rate)\n",
    "d_mfcc_feat = delta(mfcc_feat, 2)\n",
    "dd_mfcc_feat = delta(d_mfcc_feat, 2)\n",
    "mfcc_feat = np.concatenate((mfcc_feat, d_mfcc_feat), axis=1)\n",
    "mfcc_feat = np.concatenate((mfcc_feat, dd_mfcc_feat), axis=1)\n",
    "mfcc_feat_std = (mfcc_feat - np.mean(mfcc_feat, axis=0)) / np.std(mfcc_feat, axis=0)\n",
    "mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mfcc(wavfile, fa_vad_mfcc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../uttr_fa_vad_wavs/mfcc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(wavfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat.shape, d_mfcc_feat.shape, dd_mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat_base[0,:], mfcc_feat[0, :13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_mfcc_feat[0,:], mfcc_feat[0, 13:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mfcc_feat, axis=0).shape, np.std(mfcc_feat, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat_std = (mfcc_feat - np.mean(mfcc_feat, axis=0)) / np.std(mfcc_feat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat_std[0, 13:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(os.path.join(uttr_wavs_path, \"001.002.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(os.path.join(fa_vad_wavs_path, \"001.002_fa_vad.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join([w.word for w in align_dict[\"110\"][\"110.005\"][\"es\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!soxi ../uttr_fa_vad_wavs/fa_vad_wavs/110.005_fa_vad.wav\n",
    "!soxi ../uttr_fa_vad_wavs/uttr_wavs/110.005.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MFCCs and Log Mel Filterbanks generated using Kaldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaldi_out_path = \"../uttr_fa_vad_wavs/kaldi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = np.load(\"../uttr_fa_vad_wavs/mfcc_std/001.002_fa_vad.std.mfcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha.shape, haha[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $kaldi_out_path/mfcc_cmvn_dd_vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaldi_test = np.load(\"../uttr_fa_vad_wavs/kaldi/mfcc_cmvn_dd_vad/test.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaldi_dev['001.002'].shape, kaldi_dev['001.002'][0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
